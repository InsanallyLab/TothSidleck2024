{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa5cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.0...\n",
      "Using DeepLabCut version: 2.3.0\n",
      "Using TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "#Run this code in Deeplabcut environment\n",
    "#https://www.nature.com/articles/s41596-019-0176-0#Sec14\n",
    "\n",
    "#Step1: Import modules\n",
    "try:\n",
    "    import deeplabcut\n",
    "    import tensorflow\n",
    "    import tkinter\n",
    "    from tkinter import filedialog\n",
    "\n",
    "    print(f'Using DeepLabCut version: {deeplabcut. __version__}')\n",
    "    print(f'Using TensorFlow version: {tensorflow. __version__}')   \n",
    "\n",
    "except:\n",
    "    print(\"Please run the notebook in in your local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3410b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3: Analyze the video which used for the kinematics analysis and create a labeled video\n",
    "config_path = 'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\config.yaml'\n",
    "video_file4 = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 9\\\\AE_231_3\\\\CogRig_AE_231_2023-02-14_3.avi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78055300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-500000 for model C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\\iteration-0\\Training_projectMar3-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_3\\CogRig_AE_231_2023-02-14_3.avi\n",
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_3\\CogRig_AE_231_2023-02-14_3.avi\n",
      "Duration of video [s]:  479.55 , recorded with  100.0 fps!\n",
      "Overall # of frames:  47955  found with (before cropping) frame dimensions:  640 480\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47955/47955 [16:56<00:00, 47.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_3...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "video4 = deeplabcut.analyze_videos(config_path, [video_file4],videotype='.avi',auto_track=True,save_as_csv=True)\n",
    "deeplabcut.create_labeled_video(config_path, [video_file4], save_frames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6f8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file5 = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 9\\\\AE_231_2\\\\CogRig_AE_231_2023-02-14_2.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90b5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-500000 for model C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\\iteration-0\\Training_projectMar3-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_2\\CogRig_AE_231_2023-02-14_2.avi\n",
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_2\\CogRig_AE_231_2023-02-14_2.avi\n",
      "Duration of video [s]:  504.09 , recorded with  100.0 fps!\n",
      "Overall # of frames:  50409  found with (before cropping) frame dimensions:  640 480\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50409/50409 [18:21<00:00, 45.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_2...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "video5 = deeplabcut.analyze_videos(config_path, [video_file5],videotype='.avi',auto_track=True,save_as_csv=True)\n",
    "deeplabcut.create_labeled_video(config_path, [video_file5], save_frames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c1dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c510bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f7916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca0df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaa852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66411c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee84f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5396ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2:Training\n",
    "#Create a new training project, then open yaml file crated and change the parameters\n",
    "#Extrace frames from all videos so we can create a training dataset\n",
    "#Label frames to manually mark the bodyparts in GUI\n",
    "#Check labels\n",
    "#Create a training dataset\n",
    "#Train the network\n",
    "#Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76088e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_directory= ['C:\\\\Users\\insan\\\\Desktop\\\\DLC_Training_Videos\\\\CogRig_AE_231_2023-02-10_1.avi',\n",
    "                  'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Training_Videos\\\\CogRig_AE_231_2023-02-14_2.avi',\n",
    "                  'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Training_Videos\\\\CogRig_AE_231_2023-02-14_1.avi',\n",
    "                  'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Training_Videos\\\\CogRig_TH_217_2023-02-13_3.avi',\n",
    "                  'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Training_Videos\\\\CogRig_TH_217_2023-02-23_3.avi',                             \n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424d8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\"\n",
      "Created \"C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\labeled-data\"\n",
      "Created \"C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\training-datasets\"\n",
      "Created \"C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\"\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "C:\\Users\\insan\\Desktop\\DLC_Training_Videos\\CogRig_AE_231_2023-02-10_1.avi moved to C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-10_1.avi\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "C:\\Users\\insan\\Desktop\\DLC_Training_Videos\\CogRig_AE_231_2023-02-14_2.avi moved to C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-14_2.avi\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "C:\\Users\\insan\\Desktop\\DLC_Training_Videos\\CogRig_AE_231_2023-02-14_1.avi moved to C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-14_1.avi\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "C:\\Users\\insan\\Desktop\\DLC_Training_Videos\\CogRig_TH_217_2023-02-13_3.avi moved to C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_TH_217_2023-02-13_3.avi\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "C:\\Users\\insan\\Desktop\\DLC_Training_Videos\\CogRig_TH_217_2023-02-23_3.avi moved to C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_TH_217_2023-02-23_3.avi\n",
      "C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-10_1.avi\n",
      "C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-14_2.avi\n",
      "C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_AE_231_2023-02-14_1.avi\n",
      "C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_TH_217_2023-02-13_3.avi\n",
      "C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\videos\\CogRig_TH_217_2023-02-23_3.avi\n",
      "Generated \"C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\config.yaml\"\n",
      "\n",
      "A new project with name Training_project-TH-2023-03-03 is created at C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "config_path = deeplabcut.create_new_project('Training_project', 'TH', video_directory, working_directory='C:\\\\Users\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti', copy_videos=False, multianimal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0392b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 1042.98  seconds.\n",
      "Extracting and downsampling... 104298  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104298it [02:34, 674.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 504.09  seconds.\n",
      "Extracting and downsampling... 50409  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50409it [01:14, 675.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 525.29  seconds.\n",
      "Extracting and downsampling... 52529  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52529it [01:17, 673.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 324.22  seconds.\n",
      "Extracting and downsampling... 32422  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32422it [00:47, 682.80it/s]\n",
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 196.38  seconds.\n",
      "Extracting and downsampling... 19638  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19638it [00:28, 687.19it/s]\n",
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=False, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eefcb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987c7c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea9ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffe3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'C:\\\\Users\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7de75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09993620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b501147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd867c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e43331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ab367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QStatusBar::insertPermanentWidget: Index out of range (0), appending widget\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3c43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by TH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.85it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.10it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  5.33it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc921d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([45, 33, 40, 26, 11,  2, 32, 43, 46, 30,  4, 10, 28, 22, 31, 49, 37,\n",
       "           7, 14, 27, 35, 50, 18, 52, 34, 15,  5, 29, 16, 53, 20, 48,  8, 13,\n",
       "          25, 17, 41, 54, 38,  1, 12, 42, 24,  6, 23, 36, 21, 19,  9, 39, 51,\n",
       "           3]),\n",
       "   array([ 0, 47, 44])))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path, augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680b6f40",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Documentation_data-Training_project_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading ImageNet-pretrained resnet_50\n",
      "Max_iters overwritten as 100000\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 10000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4]], 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Documentation_data-Training_project_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 5, 'pos_dist_thresh': 17, 'project_path': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0642 lr: 0.005\n",
      "iteration: 200 loss: 0.0229 lr: 0.005\n",
      "iteration: 300 loss: 0.0182 lr: 0.005\n",
      "iteration: 400 loss: 0.0161 lr: 0.005\n",
      "iteration: 500 loss: 0.0159 lr: 0.005\n",
      "iteration: 600 loss: 0.0138 lr: 0.005\n",
      "iteration: 700 loss: 0.0128 lr: 0.005\n",
      "iteration: 800 loss: 0.0118 lr: 0.005\n",
      "iteration: 900 loss: 0.0120 lr: 0.005\n",
      "iteration: 1000 loss: 0.0102 lr: 0.005\n",
      "iteration: 1100 loss: 0.0107 lr: 0.005\n",
      "iteration: 1200 loss: 0.0096 lr: 0.005\n",
      "iteration: 1300 loss: 0.0090 lr: 0.005\n",
      "iteration: 1400 loss: 0.0097 lr: 0.005\n",
      "iteration: 1500 loss: 0.0086 lr: 0.005\n",
      "iteration: 1600 loss: 0.0082 lr: 0.005\n",
      "iteration: 1700 loss: 0.0079 lr: 0.005\n",
      "iteration: 1800 loss: 0.0077 lr: 0.005\n",
      "iteration: 1900 loss: 0.0084 lr: 0.005\n",
      "iteration: 2000 loss: 0.0073 lr: 0.005\n",
      "iteration: 2100 loss: 0.0066 lr: 0.005\n",
      "iteration: 2200 loss: 0.0070 lr: 0.005\n",
      "iteration: 2300 loss: 0.0068 lr: 0.005\n",
      "iteration: 2400 loss: 0.0069 lr: 0.005\n",
      "iteration: 2500 loss: 0.0072 lr: 0.005\n",
      "iteration: 2600 loss: 0.0069 lr: 0.005\n",
      "iteration: 2700 loss: 0.0061 lr: 0.005\n",
      "iteration: 2800 loss: 0.0063 lr: 0.005\n",
      "iteration: 2900 loss: 0.0062 lr: 0.005\n",
      "iteration: 3000 loss: 0.0060 lr: 0.005\n",
      "iteration: 3100 loss: 0.0063 lr: 0.005\n",
      "iteration: 3200 loss: 0.0062 lr: 0.005\n",
      "iteration: 3300 loss: 0.0061 lr: 0.005\n",
      "iteration: 3400 loss: 0.0060 lr: 0.005\n",
      "iteration: 3500 loss: 0.0055 lr: 0.005\n",
      "iteration: 3600 loss: 0.0060 lr: 0.005\n",
      "iteration: 3700 loss: 0.0061 lr: 0.005\n",
      "iteration: 3800 loss: 0.0058 lr: 0.005\n",
      "iteration: 3900 loss: 0.0054 lr: 0.005\n",
      "iteration: 4000 loss: 0.0055 lr: 0.005\n",
      "iteration: 4100 loss: 0.0063 lr: 0.005\n",
      "iteration: 4200 loss: 0.0054 lr: 0.005\n",
      "iteration: 4300 loss: 0.0056 lr: 0.005\n",
      "iteration: 4400 loss: 0.0056 lr: 0.005\n",
      "iteration: 4500 loss: 0.0055 lr: 0.005\n",
      "iteration: 4600 loss: 0.0055 lr: 0.005\n",
      "iteration: 4700 loss: 0.0051 lr: 0.005\n",
      "iteration: 4800 loss: 0.0050 lr: 0.005\n",
      "iteration: 4900 loss: 0.0049 lr: 0.005\n",
      "iteration: 5000 loss: 0.0049 lr: 0.005\n",
      "iteration: 5100 loss: 0.0055 lr: 0.005\n",
      "iteration: 5200 loss: 0.0050 lr: 0.005\n",
      "iteration: 5300 loss: 0.0052 lr: 0.005\n",
      "iteration: 5400 loss: 0.0051 lr: 0.005\n",
      "iteration: 5500 loss: 0.0049 lr: 0.005\n",
      "iteration: 5600 loss: 0.0051 lr: 0.005\n",
      "iteration: 5700 loss: 0.0050 lr: 0.005\n",
      "iteration: 5800 loss: 0.0049 lr: 0.005\n",
      "iteration: 5900 loss: 0.0046 lr: 0.005\n",
      "iteration: 6000 loss: 0.0051 lr: 0.005\n",
      "iteration: 6100 loss: 0.0048 lr: 0.005\n",
      "iteration: 6200 loss: 0.0053 lr: 0.005\n",
      "iteration: 6300 loss: 0.0048 lr: 0.005\n",
      "iteration: 6400 loss: 0.0049 lr: 0.005\n",
      "iteration: 6500 loss: 0.0050 lr: 0.005\n",
      "iteration: 6600 loss: 0.0047 lr: 0.005\n",
      "iteration: 6700 loss: 0.0049 lr: 0.005\n",
      "iteration: 6800 loss: 0.0045 lr: 0.005\n",
      "iteration: 6900 loss: 0.0042 lr: 0.005\n",
      "iteration: 7000 loss: 0.0045 lr: 0.005\n",
      "iteration: 7100 loss: 0.0045 lr: 0.005\n",
      "iteration: 7200 loss: 0.0047 lr: 0.005\n",
      "iteration: 7300 loss: 0.0046 lr: 0.005\n",
      "iteration: 7400 loss: 0.0046 lr: 0.005\n",
      "iteration: 7500 loss: 0.0047 lr: 0.005\n",
      "iteration: 7600 loss: 0.0047 lr: 0.005\n",
      "iteration: 7700 loss: 0.0044 lr: 0.005\n",
      "iteration: 7800 loss: 0.0047 lr: 0.005\n",
      "iteration: 7900 loss: 0.0048 lr: 0.005\n",
      "iteration: 8000 loss: 0.0046 lr: 0.005\n",
      "iteration: 8100 loss: 0.0047 lr: 0.005\n",
      "iteration: 8200 loss: 0.0046 lr: 0.005\n",
      "iteration: 8300 loss: 0.0043 lr: 0.005\n",
      "iteration: 8400 loss: 0.0043 lr: 0.005\n",
      "iteration: 8500 loss: 0.0045 lr: 0.005\n",
      "iteration: 8600 loss: 0.0041 lr: 0.005\n",
      "iteration: 8700 loss: 0.0044 lr: 0.005\n",
      "iteration: 8800 loss: 0.0042 lr: 0.005\n",
      "iteration: 8900 loss: 0.0044 lr: 0.005\n",
      "iteration: 9000 loss: 0.0046 lr: 0.005\n",
      "iteration: 9100 loss: 0.0045 lr: 0.005\n",
      "iteration: 9200 loss: 0.0044 lr: 0.005\n",
      "iteration: 9300 loss: 0.0044 lr: 0.005\n",
      "iteration: 9400 loss: 0.0044 lr: 0.005\n",
      "iteration: 9500 loss: 0.0038 lr: 0.005\n",
      "iteration: 9600 loss: 0.0043 lr: 0.005\n",
      "iteration: 9700 loss: 0.0044 lr: 0.005\n",
      "iteration: 9800 loss: 0.0046 lr: 0.005\n",
      "iteration: 9900 loss: 0.0045 lr: 0.005\n",
      "iteration: 10000 loss: 0.0039 lr: 0.005\n",
      "iteration: 10100 loss: 0.0066 lr: 0.02\n",
      "iteration: 10200 loss: 0.0058 lr: 0.02\n",
      "iteration: 10300 loss: 0.0063 lr: 0.02\n",
      "iteration: 10400 loss: 0.0055 lr: 0.02\n",
      "iteration: 10500 loss: 0.0060 lr: 0.02\n",
      "iteration: 10600 loss: 0.0059 lr: 0.02\n",
      "iteration: 10700 loss: 0.0056 lr: 0.02\n",
      "iteration: 10800 loss: 0.0054 lr: 0.02\n",
      "iteration: 10900 loss: 0.0048 lr: 0.02\n",
      "iteration: 11000 loss: 0.0054 lr: 0.02\n",
      "iteration: 11100 loss: 0.0049 lr: 0.02\n",
      "iteration: 11200 loss: 0.0052 lr: 0.02\n",
      "iteration: 11300 loss: 0.0051 lr: 0.02\n",
      "iteration: 11400 loss: 0.0057 lr: 0.02\n",
      "iteration: 11500 loss: 0.0047 lr: 0.02\n",
      "iteration: 11600 loss: 0.0054 lr: 0.02\n",
      "iteration: 11700 loss: 0.0049 lr: 0.02\n",
      "iteration: 11800 loss: 0.0047 lr: 0.02\n",
      "iteration: 11900 loss: 0.0049 lr: 0.02\n",
      "iteration: 12000 loss: 0.0047 lr: 0.02\n",
      "iteration: 12100 loss: 0.0048 lr: 0.02\n",
      "iteration: 12200 loss: 0.0048 lr: 0.02\n",
      "iteration: 12300 loss: 0.0048 lr: 0.02\n",
      "iteration: 12400 loss: 0.0046 lr: 0.02\n",
      "iteration: 12500 loss: 0.0044 lr: 0.02\n",
      "iteration: 12600 loss: 0.0047 lr: 0.02\n",
      "iteration: 12700 loss: 0.0045 lr: 0.02\n",
      "iteration: 12800 loss: 0.0040 lr: 0.02\n",
      "iteration: 12900 loss: 0.0049 lr: 0.02\n",
      "iteration: 13000 loss: 0.0046 lr: 0.02\n",
      "iteration: 13100 loss: 0.0043 lr: 0.02\n",
      "iteration: 13200 loss: 0.0043 lr: 0.02\n",
      "iteration: 13300 loss: 0.0044 lr: 0.02\n",
      "iteration: 13400 loss: 0.0042 lr: 0.02\n",
      "iteration: 13500 loss: 0.0043 lr: 0.02\n",
      "iteration: 13600 loss: 0.0043 lr: 0.02\n",
      "iteration: 13700 loss: 0.0043 lr: 0.02\n",
      "iteration: 13800 loss: 0.0043 lr: 0.02\n",
      "iteration: 13900 loss: 0.0042 lr: 0.02\n",
      "iteration: 14000 loss: 0.0043 lr: 0.02\n",
      "iteration: 14100 loss: 0.0043 lr: 0.02\n",
      "iteration: 14200 loss: 0.0041 lr: 0.02\n",
      "iteration: 14300 loss: 0.0041 lr: 0.02\n",
      "iteration: 14400 loss: 0.0040 lr: 0.02\n",
      "iteration: 14500 loss: 0.0042 lr: 0.02\n",
      "iteration: 14600 loss: 0.0042 lr: 0.02\n",
      "iteration: 14700 loss: 0.0038 lr: 0.02\n",
      "iteration: 14800 loss: 0.0038 lr: 0.02\n",
      "iteration: 14900 loss: 0.0042 lr: 0.02\n",
      "iteration: 15000 loss: 0.0040 lr: 0.02\n",
      "iteration: 15100 loss: 0.0042 lr: 0.02\n",
      "iteration: 15200 loss: 0.0038 lr: 0.02\n",
      "iteration: 15300 loss: 0.0037 lr: 0.02\n",
      "iteration: 15400 loss: 0.0038 lr: 0.02\n",
      "iteration: 15500 loss: 0.0038 lr: 0.02\n",
      "iteration: 15600 loss: 0.0037 lr: 0.02\n",
      "iteration: 15700 loss: 0.0036 lr: 0.02\n",
      "iteration: 15800 loss: 0.0040 lr: 0.02\n",
      "iteration: 15900 loss: 0.0039 lr: 0.02\n",
      "iteration: 16000 loss: 0.0039 lr: 0.02\n",
      "iteration: 16100 loss: 0.0039 lr: 0.02\n",
      "iteration: 16200 loss: 0.0037 lr: 0.02\n",
      "iteration: 16300 loss: 0.0039 lr: 0.02\n",
      "iteration: 16400 loss: 0.0037 lr: 0.02\n",
      "iteration: 16500 loss: 0.0037 lr: 0.02\n",
      "iteration: 16600 loss: 0.0037 lr: 0.02\n",
      "iteration: 16700 loss: 0.0041 lr: 0.02\n",
      "iteration: 16800 loss: 0.0035 lr: 0.02\n",
      "iteration: 16900 loss: 0.0036 lr: 0.02\n",
      "iteration: 17000 loss: 0.0038 lr: 0.02\n",
      "iteration: 17100 loss: 0.0038 lr: 0.02\n",
      "iteration: 17200 loss: 0.0037 lr: 0.02\n",
      "iteration: 17300 loss: 0.0037 lr: 0.02\n",
      "iteration: 17400 loss: 0.0035 lr: 0.02\n",
      "iteration: 17500 loss: 0.0038 lr: 0.02\n",
      "iteration: 17600 loss: 0.0039 lr: 0.02\n",
      "iteration: 17700 loss: 0.0037 lr: 0.02\n",
      "iteration: 17800 loss: 0.0035 lr: 0.02\n",
      "iteration: 17900 loss: 0.0036 lr: 0.02\n",
      "iteration: 18000 loss: 0.0035 lr: 0.02\n",
      "iteration: 18100 loss: 0.0038 lr: 0.02\n",
      "iteration: 18200 loss: 0.0036 lr: 0.02\n",
      "iteration: 18300 loss: 0.0036 lr: 0.02\n",
      "iteration: 18400 loss: 0.0035 lr: 0.02\n",
      "iteration: 18500 loss: 0.0036 lr: 0.02\n",
      "iteration: 18600 loss: 0.0037 lr: 0.02\n",
      "iteration: 18700 loss: 0.0036 lr: 0.02\n",
      "iteration: 18800 loss: 0.0033 lr: 0.02\n",
      "iteration: 18900 loss: 0.0038 lr: 0.02\n",
      "iteration: 19000 loss: 0.0035 lr: 0.02\n",
      "iteration: 19100 loss: 0.0032 lr: 0.02\n",
      "iteration: 19200 loss: 0.0036 lr: 0.02\n",
      "iteration: 19300 loss: 0.0035 lr: 0.02\n",
      "iteration: 19400 loss: 0.0031 lr: 0.02\n",
      "iteration: 19500 loss: 0.0034 lr: 0.02\n",
      "iteration: 19600 loss: 0.0034 lr: 0.02\n",
      "iteration: 19700 loss: 0.0037 lr: 0.02\n",
      "iteration: 19800 loss: 0.0037 lr: 0.02\n",
      "iteration: 19900 loss: 0.0033 lr: 0.02\n",
      "iteration: 20000 loss: 0.0033 lr: 0.02\n",
      "iteration: 20100 loss: 0.0034 lr: 0.02\n",
      "iteration: 20200 loss: 0.0034 lr: 0.02\n",
      "iteration: 20300 loss: 0.0038 lr: 0.02\n",
      "iteration: 20400 loss: 0.0037 lr: 0.02\n",
      "iteration: 20500 loss: 0.0035 lr: 0.02\n",
      "iteration: 20600 loss: 0.0032 lr: 0.02\n",
      "iteration: 20700 loss: 0.0036 lr: 0.02\n",
      "iteration: 20800 loss: 0.0036 lr: 0.02\n",
      "iteration: 20900 loss: 0.0036 lr: 0.02\n",
      "iteration: 21000 loss: 0.0033 lr: 0.02\n",
      "iteration: 21100 loss: 0.0034 lr: 0.02\n",
      "iteration: 21200 loss: 0.0037 lr: 0.02\n",
      "iteration: 21300 loss: 0.0035 lr: 0.02\n",
      "iteration: 21400 loss: 0.0034 lr: 0.02\n",
      "iteration: 21500 loss: 0.0033 lr: 0.02\n",
      "iteration: 21600 loss: 0.0035 lr: 0.02\n",
      "iteration: 21700 loss: 0.0034 lr: 0.02\n",
      "iteration: 21800 loss: 0.0035 lr: 0.02\n",
      "iteration: 21900 loss: 0.0033 lr: 0.02\n",
      "iteration: 22000 loss: 0.0034 lr: 0.02\n",
      "iteration: 22100 loss: 0.0033 lr: 0.02\n",
      "iteration: 22200 loss: 0.0038 lr: 0.02\n",
      "iteration: 22300 loss: 0.0035 lr: 0.02\n",
      "iteration: 22400 loss: 0.0035 lr: 0.02\n",
      "iteration: 22500 loss: 0.0033 lr: 0.02\n",
      "iteration: 22600 loss: 0.0031 lr: 0.02\n",
      "iteration: 22700 loss: 0.0033 lr: 0.02\n",
      "iteration: 22800 loss: 0.0031 lr: 0.02\n",
      "iteration: 22900 loss: 0.0034 lr: 0.02\n",
      "iteration: 23000 loss: 0.0032 lr: 0.02\n",
      "iteration: 23100 loss: 0.0034 lr: 0.02\n",
      "iteration: 23200 loss: 0.0032 lr: 0.02\n",
      "iteration: 23300 loss: 0.0030 lr: 0.02\n",
      "iteration: 23400 loss: 0.0033 lr: 0.02\n",
      "iteration: 23500 loss: 0.0034 lr: 0.02\n",
      "iteration: 23600 loss: 0.0033 lr: 0.02\n",
      "iteration: 23700 loss: 0.0033 lr: 0.02\n",
      "iteration: 23800 loss: 0.0033 lr: 0.02\n",
      "iteration: 23900 loss: 0.0033 lr: 0.02\n",
      "iteration: 24000 loss: 0.0033 lr: 0.02\n",
      "iteration: 24100 loss: 0.0034 lr: 0.02\n",
      "iteration: 24200 loss: 0.0032 lr: 0.02\n",
      "iteration: 24300 loss: 0.0032 lr: 0.02\n",
      "iteration: 24400 loss: 0.0034 lr: 0.02\n",
      "iteration: 24500 loss: 0.0033 lr: 0.02\n",
      "iteration: 24600 loss: 0.0033 lr: 0.02\n",
      "iteration: 24700 loss: 0.0032 lr: 0.02\n",
      "iteration: 24800 loss: 0.0031 lr: 0.02\n",
      "iteration: 24900 loss: 0.0032 lr: 0.02\n",
      "iteration: 25000 loss: 0.0033 lr: 0.02\n",
      "iteration: 25100 loss: 0.0033 lr: 0.02\n",
      "iteration: 25200 loss: 0.0032 lr: 0.02\n",
      "iteration: 25300 loss: 0.0033 lr: 0.02\n",
      "iteration: 25400 loss: 0.0031 lr: 0.02\n",
      "iteration: 25500 loss: 0.0033 lr: 0.02\n",
      "iteration: 25600 loss: 0.0031 lr: 0.02\n",
      "iteration: 25700 loss: 0.0035 lr: 0.02\n",
      "iteration: 25800 loss: 0.0033 lr: 0.02\n",
      "iteration: 25900 loss: 0.0032 lr: 0.02\n",
      "iteration: 26000 loss: 0.0030 lr: 0.02\n",
      "iteration: 26100 loss: 0.0030 lr: 0.02\n",
      "iteration: 26200 loss: 0.0032 lr: 0.02\n",
      "iteration: 26300 loss: 0.0030 lr: 0.02\n",
      "iteration: 26400 loss: 0.0031 lr: 0.02\n",
      "iteration: 26500 loss: 0.0030 lr: 0.02\n",
      "iteration: 26600 loss: 0.0031 lr: 0.02\n",
      "iteration: 26700 loss: 0.0033 lr: 0.02\n",
      "iteration: 26800 loss: 0.0030 lr: 0.02\n",
      "iteration: 26900 loss: 0.0031 lr: 0.02\n",
      "iteration: 27000 loss: 0.0030 lr: 0.02\n",
      "iteration: 27100 loss: 0.0029 lr: 0.02\n",
      "iteration: 27200 loss: 0.0032 lr: 0.02\n",
      "iteration: 27300 loss: 0.0031 lr: 0.02\n",
      "iteration: 27400 loss: 0.0032 lr: 0.02\n",
      "iteration: 27500 loss: 0.0030 lr: 0.02\n",
      "iteration: 27600 loss: 0.0031 lr: 0.02\n",
      "iteration: 27700 loss: 0.0029 lr: 0.02\n",
      "iteration: 27800 loss: 0.0030 lr: 0.02\n",
      "iteration: 27900 loss: 0.0027 lr: 0.02\n",
      "iteration: 28000 loss: 0.0030 lr: 0.02\n",
      "iteration: 28100 loss: 0.0031 lr: 0.02\n",
      "iteration: 28200 loss: 0.0032 lr: 0.02\n",
      "iteration: 28300 loss: 0.0032 lr: 0.02\n",
      "iteration: 28400 loss: 0.0030 lr: 0.02\n",
      "iteration: 28500 loss: 0.0028 lr: 0.02\n",
      "iteration: 28600 loss: 0.0030 lr: 0.02\n",
      "iteration: 28700 loss: 0.0029 lr: 0.02\n",
      "iteration: 28800 loss: 0.0028 lr: 0.02\n",
      "iteration: 28900 loss: 0.0028 lr: 0.02\n",
      "iteration: 29000 loss: 0.0029 lr: 0.02\n",
      "iteration: 29100 loss: 0.0031 lr: 0.02\n",
      "iteration: 29200 loss: 0.0033 lr: 0.02\n",
      "iteration: 29300 loss: 0.0029 lr: 0.02\n",
      "iteration: 29400 loss: 0.0029 lr: 0.02\n",
      "iteration: 29500 loss: 0.0029 lr: 0.02\n",
      "iteration: 29600 loss: 0.0028 lr: 0.02\n",
      "iteration: 29700 loss: 0.0029 lr: 0.02\n",
      "iteration: 29800 loss: 0.0030 lr: 0.02\n",
      "iteration: 29900 loss: 0.0028 lr: 0.02\n",
      "iteration: 30000 loss: 0.0030 lr: 0.02\n",
      "iteration: 30100 loss: 0.0031 lr: 0.02\n",
      "iteration: 30200 loss: 0.0028 lr: 0.02\n",
      "iteration: 30300 loss: 0.0028 lr: 0.02\n",
      "iteration: 30400 loss: 0.0032 lr: 0.02\n",
      "iteration: 30500 loss: 0.0027 lr: 0.02\n",
      "iteration: 30600 loss: 0.0031 lr: 0.02\n",
      "iteration: 30700 loss: 0.0029 lr: 0.02\n",
      "iteration: 30800 loss: 0.0027 lr: 0.02\n",
      "iteration: 30900 loss: 0.0029 lr: 0.02\n",
      "iteration: 31000 loss: 0.0028 lr: 0.02\n",
      "iteration: 31100 loss: 0.0030 lr: 0.02\n",
      "iteration: 31200 loss: 0.0028 lr: 0.02\n",
      "iteration: 31300 loss: 0.0028 lr: 0.02\n",
      "iteration: 31400 loss: 0.0027 lr: 0.02\n",
      "iteration: 31500 loss: 0.0029 lr: 0.02\n",
      "iteration: 31600 loss: 0.0026 lr: 0.02\n",
      "iteration: 31700 loss: 0.0030 lr: 0.02\n",
      "iteration: 31800 loss: 0.0029 lr: 0.02\n",
      "iteration: 31900 loss: 0.0031 lr: 0.02\n",
      "iteration: 32000 loss: 0.0027 lr: 0.02\n",
      "iteration: 32100 loss: 0.0031 lr: 0.02\n",
      "iteration: 32200 loss: 0.0028 lr: 0.02\n",
      "iteration: 32300 loss: 0.0028 lr: 0.02\n",
      "iteration: 32400 loss: 0.0028 lr: 0.02\n",
      "iteration: 32500 loss: 0.0031 lr: 0.02\n",
      "iteration: 32600 loss: 0.0029 lr: 0.02\n",
      "iteration: 32700 loss: 0.0029 lr: 0.02\n",
      "iteration: 32800 loss: 0.0031 lr: 0.02\n",
      "iteration: 32900 loss: 0.0028 lr: 0.02\n",
      "iteration: 33000 loss: 0.0028 lr: 0.02\n",
      "iteration: 33100 loss: 0.0028 lr: 0.02\n",
      "iteration: 33200 loss: 0.0028 lr: 0.02\n",
      "iteration: 33300 loss: 0.0029 lr: 0.02\n",
      "iteration: 33400 loss: 0.0028 lr: 0.02\n",
      "iteration: 33500 loss: 0.0029 lr: 0.02\n",
      "iteration: 33600 loss: 0.0031 lr: 0.02\n",
      "iteration: 33700 loss: 0.0029 lr: 0.02\n",
      "iteration: 33800 loss: 0.0030 lr: 0.02\n",
      "iteration: 33900 loss: 0.0030 lr: 0.02\n",
      "iteration: 34000 loss: 0.0029 lr: 0.02\n",
      "iteration: 34100 loss: 0.0027 lr: 0.02\n",
      "iteration: 34200 loss: 0.0027 lr: 0.02\n",
      "iteration: 34300 loss: 0.0030 lr: 0.02\n",
      "iteration: 34400 loss: 0.0030 lr: 0.02\n",
      "iteration: 34500 loss: 0.0031 lr: 0.02\n",
      "iteration: 34600 loss: 0.0027 lr: 0.02\n",
      "iteration: 34700 loss: 0.0028 lr: 0.02\n",
      "iteration: 34800 loss: 0.0029 lr: 0.02\n",
      "iteration: 34900 loss: 0.0028 lr: 0.02\n",
      "iteration: 35000 loss: 0.0027 lr: 0.02\n",
      "iteration: 35100 loss: 0.0027 lr: 0.02\n",
      "iteration: 35200 loss: 0.0027 lr: 0.02\n",
      "iteration: 35300 loss: 0.0030 lr: 0.02\n",
      "iteration: 35400 loss: 0.0028 lr: 0.02\n",
      "iteration: 35500 loss: 0.0029 lr: 0.02\n",
      "iteration: 35600 loss: 0.0026 lr: 0.02\n",
      "iteration: 35700 loss: 0.0027 lr: 0.02\n",
      "iteration: 35800 loss: 0.0028 lr: 0.02\n",
      "iteration: 35900 loss: 0.0026 lr: 0.02\n",
      "iteration: 36000 loss: 0.0027 lr: 0.02\n",
      "iteration: 36100 loss: 0.0026 lr: 0.02\n",
      "iteration: 36200 loss: 0.0027 lr: 0.02\n",
      "iteration: 36300 loss: 0.0030 lr: 0.02\n",
      "iteration: 36400 loss: 0.0028 lr: 0.02\n",
      "iteration: 36500 loss: 0.0029 lr: 0.02\n",
      "iteration: 36600 loss: 0.0029 lr: 0.02\n",
      "iteration: 36700 loss: 0.0027 lr: 0.02\n",
      "iteration: 36800 loss: 0.0028 lr: 0.02\n",
      "iteration: 36900 loss: 0.0029 lr: 0.02\n",
      "iteration: 37000 loss: 0.0028 lr: 0.02\n",
      "iteration: 37100 loss: 0.0028 lr: 0.02\n",
      "iteration: 37200 loss: 0.0027 lr: 0.02\n",
      "iteration: 37300 loss: 0.0028 lr: 0.02\n",
      "iteration: 37400 loss: 0.0027 lr: 0.02\n",
      "iteration: 37500 loss: 0.0026 lr: 0.02\n",
      "iteration: 37600 loss: 0.0027 lr: 0.02\n",
      "iteration: 37700 loss: 0.0027 lr: 0.02\n",
      "iteration: 37800 loss: 0.0029 lr: 0.02\n",
      "iteration: 37900 loss: 0.0026 lr: 0.02\n",
      "iteration: 38000 loss: 0.0027 lr: 0.02\n",
      "iteration: 38100 loss: 0.0027 lr: 0.02\n",
      "iteration: 38200 loss: 0.0028 lr: 0.02\n",
      "iteration: 38300 loss: 0.0028 lr: 0.02\n",
      "iteration: 38400 loss: 0.0026 lr: 0.02\n",
      "iteration: 38500 loss: 0.0026 lr: 0.02\n",
      "iteration: 38600 loss: 0.0027 lr: 0.02\n",
      "iteration: 38700 loss: 0.0026 lr: 0.02\n",
      "iteration: 38800 loss: 0.0026 lr: 0.02\n",
      "iteration: 38900 loss: 0.0029 lr: 0.02\n",
      "iteration: 39000 loss: 0.0029 lr: 0.02\n",
      "iteration: 39100 loss: 0.0028 lr: 0.02\n",
      "iteration: 39200 loss: 0.0026 lr: 0.02\n",
      "iteration: 39300 loss: 0.0026 lr: 0.02\n",
      "iteration: 39400 loss: 0.0026 lr: 0.02\n",
      "iteration: 39500 loss: 0.0027 lr: 0.02\n",
      "iteration: 39600 loss: 0.0028 lr: 0.02\n",
      "iteration: 39700 loss: 0.0027 lr: 0.02\n",
      "iteration: 39800 loss: 0.0026 lr: 0.02\n",
      "iteration: 39900 loss: 0.0028 lr: 0.02\n",
      "iteration: 40000 loss: 0.0026 lr: 0.02\n",
      "iteration: 40100 loss: 0.0029 lr: 0.02\n",
      "iteration: 40200 loss: 0.0027 lr: 0.02\n",
      "iteration: 40300 loss: 0.0026 lr: 0.02\n",
      "iteration: 40400 loss: 0.0029 lr: 0.02\n",
      "iteration: 40500 loss: 0.0027 lr: 0.02\n",
      "iteration: 40600 loss: 0.0027 lr: 0.02\n",
      "iteration: 40700 loss: 0.0027 lr: 0.02\n",
      "iteration: 40800 loss: 0.0029 lr: 0.02\n",
      "iteration: 40900 loss: 0.0027 lr: 0.02\n",
      "iteration: 41000 loss: 0.0028 lr: 0.02\n",
      "iteration: 41100 loss: 0.0025 lr: 0.02\n",
      "iteration: 41200 loss: 0.0027 lr: 0.02\n",
      "iteration: 41300 loss: 0.0026 lr: 0.02\n",
      "iteration: 41400 loss: 0.0027 lr: 0.02\n",
      "iteration: 41500 loss: 0.0029 lr: 0.02\n",
      "iteration: 41600 loss: 0.0027 lr: 0.02\n",
      "iteration: 41700 loss: 0.0026 lr: 0.02\n",
      "iteration: 41800 loss: 0.0028 lr: 0.02\n",
      "iteration: 41900 loss: 0.0026 lr: 0.02\n",
      "iteration: 42000 loss: 0.0025 lr: 0.02\n",
      "iteration: 42100 loss: 0.0028 lr: 0.02\n",
      "iteration: 42200 loss: 0.0026 lr: 0.02\n",
      "iteration: 42300 loss: 0.0026 lr: 0.02\n",
      "iteration: 42400 loss: 0.0025 lr: 0.02\n",
      "iteration: 42500 loss: 0.0029 lr: 0.02\n",
      "iteration: 42600 loss: 0.0026 lr: 0.02\n",
      "iteration: 42700 loss: 0.0026 lr: 0.02\n",
      "iteration: 42800 loss: 0.0028 lr: 0.02\n",
      "iteration: 42900 loss: 0.0027 lr: 0.02\n",
      "iteration: 43000 loss: 0.0025 lr: 0.02\n",
      "iteration: 43100 loss: 0.0028 lr: 0.02\n",
      "iteration: 43200 loss: 0.0025 lr: 0.02\n",
      "iteration: 43300 loss: 0.0026 lr: 0.02\n",
      "iteration: 43400 loss: 0.0025 lr: 0.02\n",
      "iteration: 43500 loss: 0.0026 lr: 0.02\n",
      "iteration: 43600 loss: 0.0024 lr: 0.02\n",
      "iteration: 43700 loss: 0.0027 lr: 0.02\n",
      "iteration: 43800 loss: 0.0026 lr: 0.02\n",
      "iteration: 43900 loss: 0.0024 lr: 0.02\n",
      "iteration: 44000 loss: 0.0025 lr: 0.02\n",
      "iteration: 44100 loss: 0.0024 lr: 0.02\n",
      "iteration: 44200 loss: 0.0025 lr: 0.02\n",
      "iteration: 44300 loss: 0.0026 lr: 0.02\n",
      "iteration: 44400 loss: 0.0028 lr: 0.02\n",
      "iteration: 44500 loss: 0.0024 lr: 0.02\n",
      "iteration: 44600 loss: 0.0026 lr: 0.02\n",
      "iteration: 44700 loss: 0.0027 lr: 0.02\n",
      "iteration: 44800 loss: 0.0027 lr: 0.02\n",
      "iteration: 44900 loss: 0.0027 lr: 0.02\n",
      "iteration: 45000 loss: 0.0023 lr: 0.02\n",
      "iteration: 45100 loss: 0.0026 lr: 0.02\n",
      "iteration: 45200 loss: 0.0028 lr: 0.02\n",
      "iteration: 45300 loss: 0.0027 lr: 0.02\n",
      "iteration: 45400 loss: 0.0027 lr: 0.02\n",
      "iteration: 45500 loss: 0.0024 lr: 0.02\n",
      "iteration: 45600 loss: 0.0026 lr: 0.02\n",
      "iteration: 45700 loss: 0.0026 lr: 0.02\n",
      "iteration: 45800 loss: 0.0026 lr: 0.02\n",
      "iteration: 45900 loss: 0.0026 lr: 0.02\n",
      "iteration: 46000 loss: 0.0024 lr: 0.02\n",
      "iteration: 46100 loss: 0.0026 lr: 0.02\n",
      "iteration: 46200 loss: 0.0023 lr: 0.02\n",
      "iteration: 46300 loss: 0.0025 lr: 0.02\n",
      "iteration: 46400 loss: 0.0026 lr: 0.02\n",
      "iteration: 46500 loss: 0.0025 lr: 0.02\n",
      "iteration: 46600 loss: 0.0026 lr: 0.02\n",
      "iteration: 46700 loss: 0.0026 lr: 0.02\n",
      "iteration: 46800 loss: 0.0024 lr: 0.02\n",
      "iteration: 46900 loss: 0.0023 lr: 0.02\n",
      "iteration: 47000 loss: 0.0025 lr: 0.02\n",
      "iteration: 47100 loss: 0.0027 lr: 0.02\n",
      "iteration: 47200 loss: 0.0026 lr: 0.02\n",
      "iteration: 47300 loss: 0.0026 lr: 0.02\n",
      "iteration: 47400 loss: 0.0025 lr: 0.02\n",
      "iteration: 47500 loss: 0.0022 lr: 0.02\n",
      "iteration: 47600 loss: 0.0027 lr: 0.02\n",
      "iteration: 47700 loss: 0.0026 lr: 0.02\n",
      "iteration: 47800 loss: 0.0024 lr: 0.02\n",
      "iteration: 47900 loss: 0.0025 lr: 0.02\n",
      "iteration: 48000 loss: 0.0024 lr: 0.02\n",
      "iteration: 48100 loss: 0.0025 lr: 0.02\n",
      "iteration: 48200 loss: 0.0028 lr: 0.02\n",
      "iteration: 48300 loss: 0.0026 lr: 0.02\n",
      "iteration: 48400 loss: 0.0025 lr: 0.02\n",
      "iteration: 48500 loss: 0.0026 lr: 0.02\n",
      "iteration: 48600 loss: 0.0026 lr: 0.02\n",
      "iteration: 48700 loss: 0.0025 lr: 0.02\n",
      "iteration: 48800 loss: 0.0024 lr: 0.02\n",
      "iteration: 48900 loss: 0.0027 lr: 0.02\n",
      "iteration: 49000 loss: 0.0023 lr: 0.02\n",
      "iteration: 49100 loss: 0.0026 lr: 0.02\n",
      "iteration: 49200 loss: 0.0025 lr: 0.02\n",
      "iteration: 49300 loss: 0.0025 lr: 0.02\n",
      "iteration: 49400 loss: 0.0025 lr: 0.02\n",
      "iteration: 49500 loss: 0.0026 lr: 0.02\n",
      "iteration: 49600 loss: 0.0029 lr: 0.02\n",
      "iteration: 49700 loss: 0.0025 lr: 0.02\n",
      "iteration: 49800 loss: 0.0026 lr: 0.02\n",
      "iteration: 49900 loss: 0.0025 lr: 0.02\n",
      "iteration: 50000 loss: 0.0025 lr: 0.02\n",
      "iteration: 50100 loss: 0.0023 lr: 0.02\n",
      "iteration: 50200 loss: 0.0026 lr: 0.02\n",
      "iteration: 50300 loss: 0.0025 lr: 0.02\n",
      "iteration: 50400 loss: 0.0023 lr: 0.02\n",
      "iteration: 50500 loss: 0.0024 lr: 0.02\n",
      "iteration: 50600 loss: 0.0025 lr: 0.02\n",
      "iteration: 50700 loss: 0.0026 lr: 0.02\n",
      "iteration: 50800 loss: 0.0024 lr: 0.02\n",
      "iteration: 50900 loss: 0.0023 lr: 0.02\n",
      "iteration: 51000 loss: 0.0025 lr: 0.02\n",
      "iteration: 51100 loss: 0.0024 lr: 0.02\n",
      "iteration: 51200 loss: 0.0027 lr: 0.02\n",
      "iteration: 51300 loss: 0.0024 lr: 0.02\n",
      "iteration: 51400 loss: 0.0026 lr: 0.02\n",
      "iteration: 51500 loss: 0.0025 lr: 0.02\n",
      "iteration: 51600 loss: 0.0023 lr: 0.02\n",
      "iteration: 51700 loss: 0.0025 lr: 0.02\n",
      "iteration: 51800 loss: 0.0023 lr: 0.02\n",
      "iteration: 51900 loss: 0.0025 lr: 0.02\n",
      "iteration: 52000 loss: 0.0025 lr: 0.02\n",
      "iteration: 52100 loss: 0.0024 lr: 0.02\n",
      "iteration: 52200 loss: 0.0024 lr: 0.02\n",
      "iteration: 52300 loss: 0.0025 lr: 0.02\n",
      "iteration: 52400 loss: 0.0023 lr: 0.02\n",
      "iteration: 52500 loss: 0.0026 lr: 0.02\n",
      "iteration: 52600 loss: 0.0023 lr: 0.02\n",
      "iteration: 52700 loss: 0.0025 lr: 0.02\n",
      "iteration: 52800 loss: 0.0026 lr: 0.02\n",
      "iteration: 52900 loss: 0.0026 lr: 0.02\n",
      "iteration: 53000 loss: 0.0024 lr: 0.02\n",
      "iteration: 53100 loss: 0.0024 lr: 0.02\n",
      "iteration: 53200 loss: 0.0024 lr: 0.02\n",
      "iteration: 53300 loss: 0.0025 lr: 0.02\n",
      "iteration: 53400 loss: 0.0023 lr: 0.02\n",
      "iteration: 53500 loss: 0.0024 lr: 0.02\n",
      "iteration: 53600 loss: 0.0025 lr: 0.02\n",
      "iteration: 53700 loss: 0.0026 lr: 0.02\n",
      "iteration: 53800 loss: 0.0025 lr: 0.02\n",
      "iteration: 53900 loss: 0.0023 lr: 0.02\n",
      "iteration: 54000 loss: 0.0024 lr: 0.02\n",
      "iteration: 54100 loss: 0.0026 lr: 0.02\n",
      "iteration: 54200 loss: 0.0024 lr: 0.02\n",
      "iteration: 54300 loss: 0.0022 lr: 0.02\n",
      "iteration: 54400 loss: 0.0024 lr: 0.02\n",
      "iteration: 54500 loss: 0.0024 lr: 0.02\n",
      "iteration: 54600 loss: 0.0023 lr: 0.02\n",
      "iteration: 54700 loss: 0.0024 lr: 0.02\n",
      "iteration: 54800 loss: 0.0022 lr: 0.02\n",
      "iteration: 54900 loss: 0.0024 lr: 0.02\n",
      "iteration: 55000 loss: 0.0023 lr: 0.02\n",
      "iteration: 55100 loss: 0.0025 lr: 0.02\n",
      "iteration: 55200 loss: 0.0023 lr: 0.02\n",
      "iteration: 55300 loss: 0.0024 lr: 0.02\n",
      "iteration: 55400 loss: 0.0022 lr: 0.02\n",
      "iteration: 55500 loss: 0.0023 lr: 0.02\n",
      "iteration: 55600 loss: 0.0025 lr: 0.02\n",
      "iteration: 55700 loss: 0.0025 lr: 0.02\n",
      "iteration: 55800 loss: 0.0021 lr: 0.02\n",
      "iteration: 55900 loss: 0.0024 lr: 0.02\n",
      "iteration: 56000 loss: 0.0024 lr: 0.02\n",
      "iteration: 56100 loss: 0.0024 lr: 0.02\n",
      "iteration: 56200 loss: 0.0024 lr: 0.02\n",
      "iteration: 56300 loss: 0.0022 lr: 0.02\n",
      "iteration: 56400 loss: 0.0023 lr: 0.02\n",
      "iteration: 56500 loss: 0.0022 lr: 0.02\n",
      "iteration: 56600 loss: 0.0024 lr: 0.02\n",
      "iteration: 56700 loss: 0.0023 lr: 0.02\n",
      "iteration: 56800 loss: 0.0024 lr: 0.02\n",
      "iteration: 56900 loss: 0.0024 lr: 0.02\n",
      "iteration: 57000 loss: 0.0024 lr: 0.02\n",
      "iteration: 57100 loss: 0.0024 lr: 0.02\n",
      "iteration: 57200 loss: 0.0025 lr: 0.02\n",
      "iteration: 57300 loss: 0.0023 lr: 0.02\n",
      "iteration: 57400 loss: 0.0022 lr: 0.02\n",
      "iteration: 57500 loss: 0.0024 lr: 0.02\n",
      "iteration: 57600 loss: 0.0022 lr: 0.02\n",
      "iteration: 57700 loss: 0.0023 lr: 0.02\n",
      "iteration: 57800 loss: 0.0021 lr: 0.02\n",
      "iteration: 57900 loss: 0.0023 lr: 0.02\n",
      "iteration: 58000 loss: 0.0023 lr: 0.02\n",
      "iteration: 58100 loss: 0.0024 lr: 0.02\n",
      "iteration: 58200 loss: 0.0022 lr: 0.02\n",
      "iteration: 58300 loss: 0.0024 lr: 0.02\n",
      "iteration: 58400 loss: 0.0023 lr: 0.02\n",
      "iteration: 58500 loss: 0.0023 lr: 0.02\n",
      "iteration: 58600 loss: 0.0025 lr: 0.02\n",
      "iteration: 58700 loss: 0.0023 lr: 0.02\n",
      "iteration: 58800 loss: 0.0024 lr: 0.02\n",
      "iteration: 58900 loss: 0.0024 lr: 0.02\n",
      "iteration: 59000 loss: 0.0023 lr: 0.02\n",
      "iteration: 59100 loss: 0.0023 lr: 0.02\n",
      "iteration: 59200 loss: 0.0026 lr: 0.02\n",
      "iteration: 59300 loss: 0.0021 lr: 0.02\n",
      "iteration: 59400 loss: 0.0023 lr: 0.02\n",
      "iteration: 59500 loss: 0.0023 lr: 0.02\n",
      "iteration: 59600 loss: 0.0024 lr: 0.02\n",
      "iteration: 59700 loss: 0.0022 lr: 0.02\n",
      "iteration: 59800 loss: 0.0023 lr: 0.02\n",
      "iteration: 59900 loss: 0.0023 lr: 0.02\n",
      "iteration: 60000 loss: 0.0025 lr: 0.02\n",
      "iteration: 60100 loss: 0.0023 lr: 0.02\n",
      "iteration: 60200 loss: 0.0023 lr: 0.02\n",
      "iteration: 60300 loss: 0.0023 lr: 0.02\n",
      "iteration: 60400 loss: 0.0022 lr: 0.02\n",
      "iteration: 60500 loss: 0.0022 lr: 0.02\n",
      "iteration: 60600 loss: 0.0023 lr: 0.02\n",
      "iteration: 60700 loss: 0.0023 lr: 0.02\n",
      "iteration: 60800 loss: 0.0024 lr: 0.02\n",
      "iteration: 60900 loss: 0.0021 lr: 0.02\n",
      "iteration: 61000 loss: 0.0023 lr: 0.02\n",
      "iteration: 61100 loss: 0.0022 lr: 0.02\n",
      "iteration: 61200 loss: 0.0023 lr: 0.02\n",
      "iteration: 61300 loss: 0.0024 lr: 0.02\n",
      "iteration: 61400 loss: 0.0022 lr: 0.02\n",
      "iteration: 61500 loss: 0.0024 lr: 0.02\n",
      "iteration: 61600 loss: 0.0024 lr: 0.02\n",
      "iteration: 61700 loss: 0.0022 lr: 0.02\n",
      "iteration: 61800 loss: 0.0025 lr: 0.02\n",
      "iteration: 61900 loss: 0.0023 lr: 0.02\n",
      "iteration: 62000 loss: 0.0023 lr: 0.02\n",
      "iteration: 62100 loss: 0.0022 lr: 0.02\n",
      "iteration: 62200 loss: 0.0023 lr: 0.02\n",
      "iteration: 62300 loss: 0.0022 lr: 0.02\n",
      "iteration: 62400 loss: 0.0023 lr: 0.02\n",
      "iteration: 62500 loss: 0.0022 lr: 0.02\n",
      "iteration: 62600 loss: 0.0023 lr: 0.02\n",
      "iteration: 62700 loss: 0.0021 lr: 0.02\n",
      "iteration: 62800 loss: 0.0023 lr: 0.02\n",
      "iteration: 62900 loss: 0.0023 lr: 0.02\n",
      "iteration: 63000 loss: 0.0023 lr: 0.02\n",
      "iteration: 63100 loss: 0.0023 lr: 0.02\n",
      "iteration: 63200 loss: 0.0021 lr: 0.02\n",
      "iteration: 63300 loss: 0.0024 lr: 0.02\n",
      "iteration: 63400 loss: 0.0021 lr: 0.02\n",
      "iteration: 63500 loss: 0.0021 lr: 0.02\n",
      "iteration: 63600 loss: 0.0024 lr: 0.02\n",
      "iteration: 63700 loss: 0.0021 lr: 0.02\n",
      "iteration: 63800 loss: 0.0022 lr: 0.02\n",
      "iteration: 63900 loss: 0.0024 lr: 0.02\n",
      "iteration: 64000 loss: 0.0022 lr: 0.02\n",
      "iteration: 64100 loss: 0.0022 lr: 0.02\n",
      "iteration: 64200 loss: 0.0024 lr: 0.02\n",
      "iteration: 64300 loss: 0.0022 lr: 0.02\n",
      "iteration: 64400 loss: 0.0023 lr: 0.02\n",
      "iteration: 64500 loss: 0.0022 lr: 0.02\n",
      "iteration: 64600 loss: 0.0022 lr: 0.02\n",
      "iteration: 64700 loss: 0.0022 lr: 0.02\n",
      "iteration: 64800 loss: 0.0023 lr: 0.02\n",
      "iteration: 64900 loss: 0.0022 lr: 0.02\n",
      "iteration: 65000 loss: 0.0021 lr: 0.02\n",
      "iteration: 65100 loss: 0.0021 lr: 0.02\n",
      "iteration: 65200 loss: 0.0022 lr: 0.02\n",
      "iteration: 65300 loss: 0.0021 lr: 0.02\n",
      "iteration: 65400 loss: 0.0023 lr: 0.02\n",
      "iteration: 65500 loss: 0.0023 lr: 0.02\n",
      "iteration: 65600 loss: 0.0023 lr: 0.02\n",
      "iteration: 65700 loss: 0.0022 lr: 0.02\n",
      "iteration: 65800 loss: 0.0022 lr: 0.02\n",
      "iteration: 65900 loss: 0.0021 lr: 0.02\n",
      "iteration: 66000 loss: 0.0021 lr: 0.02\n",
      "iteration: 66100 loss: 0.0024 lr: 0.02\n",
      "iteration: 66200 loss: 0.0023 lr: 0.02\n",
      "iteration: 66300 loss: 0.0021 lr: 0.02\n",
      "iteration: 66400 loss: 0.0022 lr: 0.02\n",
      "iteration: 66500 loss: 0.0021 lr: 0.02\n",
      "iteration: 66600 loss: 0.0023 lr: 0.02\n",
      "iteration: 66700 loss: 0.0022 lr: 0.02\n",
      "iteration: 66800 loss: 0.0022 lr: 0.02\n",
      "iteration: 66900 loss: 0.0020 lr: 0.02\n",
      "iteration: 67000 loss: 0.0021 lr: 0.02\n",
      "iteration: 67100 loss: 0.0022 lr: 0.02\n",
      "iteration: 67200 loss: 0.0022 lr: 0.02\n",
      "iteration: 67300 loss: 0.0023 lr: 0.02\n",
      "iteration: 67400 loss: 0.0021 lr: 0.02\n",
      "iteration: 67500 loss: 0.0023 lr: 0.02\n",
      "iteration: 67600 loss: 0.0020 lr: 0.02\n",
      "iteration: 67700 loss: 0.0021 lr: 0.02\n",
      "iteration: 67800 loss: 0.0022 lr: 0.02\n",
      "iteration: 67900 loss: 0.0022 lr: 0.02\n",
      "iteration: 68000 loss: 0.0021 lr: 0.02\n",
      "iteration: 68100 loss: 0.0021 lr: 0.02\n",
      "iteration: 68200 loss: 0.0022 lr: 0.02\n",
      "iteration: 68300 loss: 0.0021 lr: 0.02\n",
      "iteration: 68400 loss: 0.0022 lr: 0.02\n",
      "iteration: 68500 loss: 0.0022 lr: 0.02\n",
      "iteration: 68600 loss: 0.0020 lr: 0.02\n",
      "iteration: 68700 loss: 0.0022 lr: 0.02\n",
      "iteration: 68800 loss: 0.0021 lr: 0.02\n",
      "iteration: 68900 loss: 0.0020 lr: 0.02\n",
      "iteration: 69000 loss: 0.0022 lr: 0.02\n",
      "iteration: 69100 loss: 0.0020 lr: 0.02\n",
      "iteration: 69200 loss: 0.0021 lr: 0.02\n",
      "iteration: 69300 loss: 0.0021 lr: 0.02\n",
      "iteration: 69400 loss: 0.0022 lr: 0.02\n",
      "iteration: 69500 loss: 0.0023 lr: 0.02\n",
      "iteration: 69600 loss: 0.0022 lr: 0.02\n",
      "iteration: 69700 loss: 0.0021 lr: 0.02\n",
      "iteration: 69800 loss: 0.0024 lr: 0.02\n",
      "iteration: 69900 loss: 0.0021 lr: 0.02\n",
      "iteration: 70000 loss: 0.0020 lr: 0.02\n",
      "iteration: 70100 loss: 0.0022 lr: 0.02\n",
      "iteration: 70200 loss: 0.0020 lr: 0.02\n",
      "iteration: 70300 loss: 0.0021 lr: 0.02\n",
      "iteration: 70400 loss: 0.0022 lr: 0.02\n",
      "iteration: 70500 loss: 0.0021 lr: 0.02\n",
      "iteration: 70600 loss: 0.0022 lr: 0.02\n",
      "iteration: 70700 loss: 0.0022 lr: 0.02\n",
      "iteration: 70800 loss: 0.0023 lr: 0.02\n",
      "iteration: 70900 loss: 0.0022 lr: 0.02\n",
      "iteration: 71000 loss: 0.0022 lr: 0.02\n",
      "iteration: 71100 loss: 0.0020 lr: 0.02\n",
      "iteration: 71200 loss: 0.0020 lr: 0.02\n",
      "iteration: 71300 loss: 0.0021 lr: 0.02\n",
      "iteration: 71400 loss: 0.0022 lr: 0.02\n",
      "iteration: 71500 loss: 0.0022 lr: 0.02\n",
      "iteration: 71600 loss: 0.0022 lr: 0.02\n",
      "iteration: 71700 loss: 0.0021 lr: 0.02\n",
      "iteration: 71800 loss: 0.0022 lr: 0.02\n",
      "iteration: 71900 loss: 0.0022 lr: 0.02\n",
      "iteration: 72000 loss: 0.0022 lr: 0.02\n",
      "iteration: 72100 loss: 0.0021 lr: 0.02\n",
      "iteration: 72200 loss: 0.0023 lr: 0.02\n",
      "iteration: 72300 loss: 0.0020 lr: 0.02\n",
      "iteration: 72400 loss: 0.0021 lr: 0.02\n",
      "iteration: 72500 loss: 0.0023 lr: 0.02\n",
      "iteration: 72600 loss: 0.0021 lr: 0.02\n",
      "iteration: 72700 loss: 0.0021 lr: 0.02\n",
      "iteration: 72800 loss: 0.0020 lr: 0.02\n",
      "iteration: 72900 loss: 0.0018 lr: 0.02\n",
      "iteration: 73000 loss: 0.0022 lr: 0.02\n",
      "iteration: 73100 loss: 0.0020 lr: 0.02\n",
      "iteration: 73200 loss: 0.0024 lr: 0.02\n",
      "iteration: 73300 loss: 0.0022 lr: 0.02\n",
      "iteration: 73400 loss: 0.0021 lr: 0.02\n",
      "iteration: 73500 loss: 0.0023 lr: 0.02\n",
      "iteration: 73600 loss: 0.0022 lr: 0.02\n",
      "iteration: 73700 loss: 0.0020 lr: 0.02\n",
      "iteration: 73800 loss: 0.0021 lr: 0.02\n",
      "iteration: 73900 loss: 0.0020 lr: 0.02\n",
      "iteration: 74000 loss: 0.0021 lr: 0.02\n",
      "iteration: 74100 loss: 0.0020 lr: 0.02\n",
      "iteration: 74200 loss: 0.0020 lr: 0.02\n",
      "iteration: 74300 loss: 0.0020 lr: 0.02\n",
      "iteration: 74400 loss: 0.0017 lr: 0.02\n",
      "iteration: 74500 loss: 0.0020 lr: 0.02\n",
      "iteration: 74600 loss: 0.0023 lr: 0.02\n",
      "iteration: 74700 loss: 0.0019 lr: 0.02\n",
      "iteration: 74800 loss: 0.0020 lr: 0.02\n",
      "iteration: 74900 loss: 0.0023 lr: 0.02\n",
      "iteration: 75000 loss: 0.0021 lr: 0.02\n",
      "iteration: 75100 loss: 0.0021 lr: 0.02\n",
      "iteration: 75200 loss: 0.0022 lr: 0.02\n",
      "iteration: 75300 loss: 0.0020 lr: 0.02\n",
      "iteration: 75400 loss: 0.0022 lr: 0.02\n",
      "iteration: 75500 loss: 0.0022 lr: 0.02\n",
      "iteration: 75600 loss: 0.0020 lr: 0.02\n",
      "iteration: 75700 loss: 0.0020 lr: 0.02\n",
      "iteration: 75800 loss: 0.0019 lr: 0.02\n",
      "iteration: 75900 loss: 0.0022 lr: 0.02\n",
      "iteration: 76000 loss: 0.0021 lr: 0.02\n",
      "iteration: 76100 loss: 0.0020 lr: 0.02\n",
      "iteration: 76200 loss: 0.0020 lr: 0.02\n",
      "iteration: 76300 loss: 0.0020 lr: 0.02\n",
      "iteration: 76400 loss: 0.0022 lr: 0.02\n",
      "iteration: 76500 loss: 0.0020 lr: 0.02\n",
      "iteration: 76600 loss: 0.0021 lr: 0.02\n",
      "iteration: 76700 loss: 0.0022 lr: 0.02\n",
      "iteration: 76800 loss: 0.0021 lr: 0.02\n",
      "iteration: 76900 loss: 0.0021 lr: 0.02\n",
      "iteration: 77000 loss: 0.0020 lr: 0.02\n",
      "iteration: 77100 loss: 0.0021 lr: 0.02\n",
      "iteration: 77200 loss: 0.0020 lr: 0.02\n",
      "iteration: 77300 loss: 0.0021 lr: 0.02\n",
      "iteration: 77400 loss: 0.0021 lr: 0.02\n",
      "iteration: 77500 loss: 0.0019 lr: 0.02\n",
      "iteration: 77600 loss: 0.0020 lr: 0.02\n",
      "iteration: 77700 loss: 0.0018 lr: 0.02\n",
      "iteration: 77800 loss: 0.0022 lr: 0.02\n",
      "iteration: 77900 loss: 0.0022 lr: 0.02\n",
      "iteration: 78000 loss: 0.0021 lr: 0.02\n",
      "iteration: 78100 loss: 0.0022 lr: 0.02\n",
      "iteration: 78200 loss: 0.0019 lr: 0.02\n",
      "iteration: 78300 loss: 0.0019 lr: 0.02\n",
      "iteration: 78400 loss: 0.0023 lr: 0.02\n",
      "iteration: 78500 loss: 0.0020 lr: 0.02\n",
      "iteration: 78600 loss: 0.0018 lr: 0.02\n",
      "iteration: 78700 loss: 0.0021 lr: 0.02\n",
      "iteration: 78800 loss: 0.0022 lr: 0.02\n",
      "iteration: 78900 loss: 0.0020 lr: 0.02\n",
      "iteration: 79000 loss: 0.0019 lr: 0.02\n",
      "iteration: 79100 loss: 0.0019 lr: 0.02\n",
      "iteration: 79200 loss: 0.0020 lr: 0.02\n",
      "iteration: 79300 loss: 0.0019 lr: 0.02\n",
      "iteration: 79400 loss: 0.0021 lr: 0.02\n",
      "iteration: 79500 loss: 0.0021 lr: 0.02\n",
      "iteration: 79600 loss: 0.0020 lr: 0.02\n",
      "iteration: 79700 loss: 0.0021 lr: 0.02\n",
      "iteration: 79800 loss: 0.0019 lr: 0.02\n",
      "iteration: 79900 loss: 0.0020 lr: 0.02\n",
      "iteration: 80000 loss: 0.0020 lr: 0.02\n",
      "iteration: 80100 loss: 0.0020 lr: 0.02\n",
      "iteration: 80200 loss: 0.0020 lr: 0.02\n",
      "iteration: 80300 loss: 0.0018 lr: 0.02\n",
      "iteration: 80400 loss: 0.0021 lr: 0.02\n",
      "iteration: 80500 loss: 0.0021 lr: 0.02\n",
      "iteration: 80600 loss: 0.0018 lr: 0.02\n",
      "iteration: 80700 loss: 0.0020 lr: 0.02\n",
      "iteration: 80800 loss: 0.0018 lr: 0.02\n",
      "iteration: 80900 loss: 0.0019 lr: 0.02\n",
      "iteration: 81000 loss: 0.0019 lr: 0.02\n",
      "iteration: 81100 loss: 0.0019 lr: 0.02\n",
      "iteration: 81200 loss: 0.0020 lr: 0.02\n",
      "iteration: 81300 loss: 0.0020 lr: 0.02\n",
      "iteration: 81400 loss: 0.0019 lr: 0.02\n",
      "iteration: 81500 loss: 0.0023 lr: 0.02\n",
      "iteration: 81600 loss: 0.0021 lr: 0.02\n",
      "iteration: 81700 loss: 0.0019 lr: 0.02\n",
      "iteration: 81800 loss: 0.0019 lr: 0.02\n",
      "iteration: 81900 loss: 0.0019 lr: 0.02\n",
      "iteration: 82000 loss: 0.0019 lr: 0.02\n",
      "iteration: 82100 loss: 0.0021 lr: 0.02\n",
      "iteration: 82200 loss: 0.0019 lr: 0.02\n",
      "iteration: 82300 loss: 0.0018 lr: 0.02\n",
      "iteration: 82400 loss: 0.0020 lr: 0.02\n",
      "iteration: 82500 loss: 0.0020 lr: 0.02\n",
      "iteration: 82600 loss: 0.0020 lr: 0.02\n",
      "iteration: 82700 loss: 0.0019 lr: 0.02\n",
      "iteration: 82800 loss: 0.0018 lr: 0.02\n",
      "iteration: 82900 loss: 0.0021 lr: 0.02\n",
      "iteration: 83000 loss: 0.0019 lr: 0.02\n",
      "iteration: 83100 loss: 0.0020 lr: 0.02\n",
      "iteration: 83200 loss: 0.0018 lr: 0.02\n",
      "iteration: 83300 loss: 0.0020 lr: 0.02\n",
      "iteration: 83400 loss: 0.0019 lr: 0.02\n",
      "iteration: 83500 loss: 0.0019 lr: 0.02\n",
      "iteration: 83600 loss: 0.0019 lr: 0.02\n",
      "iteration: 83700 loss: 0.0020 lr: 0.02\n",
      "iteration: 83800 loss: 0.0020 lr: 0.02\n",
      "iteration: 83900 loss: 0.0021 lr: 0.02\n",
      "iteration: 84000 loss: 0.0019 lr: 0.02\n",
      "iteration: 84100 loss: 0.0018 lr: 0.02\n",
      "iteration: 84200 loss: 0.0018 lr: 0.02\n",
      "iteration: 84300 loss: 0.0019 lr: 0.02\n",
      "iteration: 84400 loss: 0.0021 lr: 0.02\n",
      "iteration: 84500 loss: 0.0019 lr: 0.02\n",
      "iteration: 84600 loss: 0.0019 lr: 0.02\n",
      "iteration: 84700 loss: 0.0019 lr: 0.02\n",
      "iteration: 84800 loss: 0.0019 lr: 0.02\n",
      "iteration: 84900 loss: 0.0019 lr: 0.02\n",
      "iteration: 85000 loss: 0.0020 lr: 0.02\n",
      "iteration: 85100 loss: 0.0020 lr: 0.02\n",
      "iteration: 85200 loss: 0.0019 lr: 0.02\n",
      "iteration: 85300 loss: 0.0021 lr: 0.02\n",
      "iteration: 85400 loss: 0.0021 lr: 0.02\n",
      "iteration: 85500 loss: 0.0019 lr: 0.02\n",
      "iteration: 85600 loss: 0.0018 lr: 0.02\n",
      "iteration: 85700 loss: 0.0019 lr: 0.02\n",
      "iteration: 85800 loss: 0.0017 lr: 0.02\n",
      "iteration: 85900 loss: 0.0020 lr: 0.02\n",
      "iteration: 86000 loss: 0.0021 lr: 0.02\n",
      "iteration: 86100 loss: 0.0019 lr: 0.02\n",
      "iteration: 86200 loss: 0.0018 lr: 0.02\n",
      "iteration: 86300 loss: 0.0019 lr: 0.02\n",
      "iteration: 86400 loss: 0.0019 lr: 0.02\n",
      "iteration: 86500 loss: 0.0020 lr: 0.02\n",
      "iteration: 86600 loss: 0.0019 lr: 0.02\n",
      "iteration: 86700 loss: 0.0019 lr: 0.02\n",
      "iteration: 86800 loss: 0.0020 lr: 0.02\n",
      "iteration: 86900 loss: 0.0020 lr: 0.02\n",
      "iteration: 87000 loss: 0.0021 lr: 0.02\n",
      "iteration: 87100 loss: 0.0019 lr: 0.02\n",
      "iteration: 87200 loss: 0.0019 lr: 0.02\n",
      "iteration: 87300 loss: 0.0020 lr: 0.02\n",
      "iteration: 87400 loss: 0.0020 lr: 0.02\n",
      "iteration: 87500 loss: 0.0018 lr: 0.02\n",
      "iteration: 87600 loss: 0.0019 lr: 0.02\n",
      "iteration: 87700 loss: 0.0020 lr: 0.02\n",
      "iteration: 87800 loss: 0.0019 lr: 0.02\n",
      "iteration: 87900 loss: 0.0021 lr: 0.02\n",
      "iteration: 88000 loss: 0.0019 lr: 0.02\n",
      "iteration: 88100 loss: 0.0020 lr: 0.02\n",
      "iteration: 88200 loss: 0.0019 lr: 0.02\n",
      "iteration: 88300 loss: 0.0019 lr: 0.02\n",
      "iteration: 88400 loss: 0.0019 lr: 0.02\n",
      "iteration: 88500 loss: 0.0019 lr: 0.02\n",
      "iteration: 88600 loss: 0.0019 lr: 0.02\n",
      "iteration: 88700 loss: 0.0018 lr: 0.02\n",
      "iteration: 88800 loss: 0.0017 lr: 0.02\n",
      "iteration: 88900 loss: 0.0018 lr: 0.02\n",
      "iteration: 89000 loss: 0.0020 lr: 0.02\n",
      "iteration: 89100 loss: 0.0019 lr: 0.02\n",
      "iteration: 89200 loss: 0.0020 lr: 0.02\n",
      "iteration: 89300 loss: 0.0018 lr: 0.02\n",
      "iteration: 89400 loss: 0.0018 lr: 0.02\n",
      "iteration: 89500 loss: 0.0021 lr: 0.02\n",
      "iteration: 89600 loss: 0.0020 lr: 0.02\n",
      "iteration: 89700 loss: 0.0021 lr: 0.02\n",
      "iteration: 89800 loss: 0.0019 lr: 0.02\n",
      "iteration: 89900 loss: 0.0018 lr: 0.02\n",
      "iteration: 90000 loss: 0.0019 lr: 0.02\n",
      "iteration: 90100 loss: 0.0018 lr: 0.02\n",
      "iteration: 90200 loss: 0.0017 lr: 0.02\n",
      "iteration: 90300 loss: 0.0020 lr: 0.02\n",
      "iteration: 90400 loss: 0.0019 lr: 0.02\n",
      "iteration: 90500 loss: 0.0020 lr: 0.02\n",
      "iteration: 90600 loss: 0.0019 lr: 0.02\n",
      "iteration: 90700 loss: 0.0018 lr: 0.02\n",
      "iteration: 90800 loss: 0.0020 lr: 0.02\n",
      "iteration: 90900 loss: 0.0019 lr: 0.02\n",
      "iteration: 91000 loss: 0.0017 lr: 0.02\n",
      "iteration: 91100 loss: 0.0019 lr: 0.02\n",
      "iteration: 91200 loss: 0.0019 lr: 0.02\n",
      "iteration: 91300 loss: 0.0018 lr: 0.02\n",
      "iteration: 91400 loss: 0.0018 lr: 0.02\n",
      "iteration: 91500 loss: 0.0018 lr: 0.02\n",
      "iteration: 91600 loss: 0.0021 lr: 0.02\n",
      "iteration: 91700 loss: 0.0021 lr: 0.02\n",
      "iteration: 91800 loss: 0.0017 lr: 0.02\n",
      "iteration: 91900 loss: 0.0018 lr: 0.02\n",
      "iteration: 92000 loss: 0.0018 lr: 0.02\n",
      "iteration: 92100 loss: 0.0018 lr: 0.02\n",
      "iteration: 92200 loss: 0.0018 lr: 0.02\n",
      "iteration: 92300 loss: 0.0017 lr: 0.02\n",
      "iteration: 92400 loss: 0.0020 lr: 0.02\n",
      "iteration: 92500 loss: 0.0019 lr: 0.02\n",
      "iteration: 92600 loss: 0.0018 lr: 0.02\n",
      "iteration: 92700 loss: 0.0018 lr: 0.02\n",
      "iteration: 92800 loss: 0.0017 lr: 0.02\n",
      "iteration: 92900 loss: 0.0019 lr: 0.02\n",
      "iteration: 93000 loss: 0.0021 lr: 0.02\n",
      "iteration: 93100 loss: 0.0019 lr: 0.02\n",
      "iteration: 93200 loss: 0.0019 lr: 0.02\n",
      "iteration: 93300 loss: 0.0018 lr: 0.02\n",
      "iteration: 93400 loss: 0.0018 lr: 0.02\n",
      "iteration: 93500 loss: 0.0019 lr: 0.02\n",
      "iteration: 93600 loss: 0.0018 lr: 0.02\n",
      "iteration: 93700 loss: 0.0017 lr: 0.02\n",
      "iteration: 93800 loss: 0.0018 lr: 0.02\n",
      "iteration: 93900 loss: 0.0019 lr: 0.02\n",
      "iteration: 94000 loss: 0.0018 lr: 0.02\n",
      "iteration: 94100 loss: 0.0019 lr: 0.02\n",
      "iteration: 94200 loss: 0.0017 lr: 0.02\n",
      "iteration: 94300 loss: 0.0017 lr: 0.02\n",
      "iteration: 94400 loss: 0.0018 lr: 0.02\n",
      "iteration: 94500 loss: 0.0017 lr: 0.02\n",
      "iteration: 94600 loss: 0.0016 lr: 0.02\n",
      "iteration: 94700 loss: 0.0019 lr: 0.02\n",
      "iteration: 94800 loss: 0.0019 lr: 0.02\n",
      "iteration: 94900 loss: 0.0020 lr: 0.02\n",
      "iteration: 95000 loss: 0.0017 lr: 0.02\n",
      "iteration: 95100 loss: 0.0016 lr: 0.02\n",
      "iteration: 95200 loss: 0.0017 lr: 0.02\n",
      "iteration: 95300 loss: 0.0018 lr: 0.02\n",
      "iteration: 95400 loss: 0.0020 lr: 0.02\n",
      "iteration: 95500 loss: 0.0017 lr: 0.02\n",
      "iteration: 95600 loss: 0.0018 lr: 0.02\n",
      "iteration: 95700 loss: 0.0017 lr: 0.02\n",
      "iteration: 95800 loss: 0.0017 lr: 0.02\n",
      "iteration: 95900 loss: 0.0017 lr: 0.02\n",
      "iteration: 96000 loss: 0.0019 lr: 0.02\n",
      "iteration: 96100 loss: 0.0017 lr: 0.02\n",
      "iteration: 96200 loss: 0.0020 lr: 0.02\n",
      "iteration: 96300 loss: 0.0020 lr: 0.02\n",
      "iteration: 96400 loss: 0.0017 lr: 0.02\n",
      "iteration: 96500 loss: 0.0018 lr: 0.02\n",
      "iteration: 96600 loss: 0.0018 lr: 0.02\n",
      "iteration: 96700 loss: 0.0018 lr: 0.02\n",
      "iteration: 96800 loss: 0.0017 lr: 0.02\n",
      "iteration: 96900 loss: 0.0018 lr: 0.02\n",
      "iteration: 97000 loss: 0.0018 lr: 0.02\n",
      "iteration: 97100 loss: 0.0019 lr: 0.02\n",
      "iteration: 97200 loss: 0.0018 lr: 0.02\n",
      "iteration: 97300 loss: 0.0019 lr: 0.02\n",
      "iteration: 97400 loss: 0.0018 lr: 0.02\n",
      "iteration: 97500 loss: 0.0017 lr: 0.02\n",
      "iteration: 97600 loss: 0.0018 lr: 0.02\n",
      "iteration: 97700 loss: 0.0017 lr: 0.02\n",
      "iteration: 97800 loss: 0.0017 lr: 0.02\n",
      "iteration: 97900 loss: 0.0018 lr: 0.02\n",
      "iteration: 98000 loss: 0.0018 lr: 0.02\n",
      "iteration: 98100 loss: 0.0019 lr: 0.02\n",
      "iteration: 98200 loss: 0.0018 lr: 0.02\n",
      "iteration: 98300 loss: 0.0018 lr: 0.02\n",
      "iteration: 98400 loss: 0.0016 lr: 0.02\n",
      "iteration: 98500 loss: 0.0017 lr: 0.02\n",
      "iteration: 98600 loss: 0.0017 lr: 0.02\n",
      "iteration: 98700 loss: 0.0017 lr: 0.02\n",
      "iteration: 98800 loss: 0.0016 lr: 0.02\n",
      "iteration: 98900 loss: 0.0017 lr: 0.02\n",
      "iteration: 99000 loss: 0.0019 lr: 0.02\n",
      "iteration: 99100 loss: 0.0018 lr: 0.02\n",
      "iteration: 99200 loss: 0.0018 lr: 0.02\n",
      "iteration: 99300 loss: 0.0017 lr: 0.02\n",
      "iteration: 99400 loss: 0.0018 lr: 0.02\n",
      "iteration: 99500 loss: 0.0018 lr: 0.02\n",
      "iteration: 99600 loss: 0.0015 lr: 0.02\n",
      "iteration: 99700 loss: 0.0018 lr: 0.02\n",
      "iteration: 99800 loss: 0.0017 lr: 0.02\n",
      "iteration: 99900 loss: 0.0017 lr: 0.02\n",
      "iteration: 100000 loss: 0.0018 lr: 0.02\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1378, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _run_fn\n",
      "    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1454, in _call_tf_sessionrun\n",
      "    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 85, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1191, in _run\n",
      "    results = self._do_run(handle, final_targets, final_fetches,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1371, in _do_run\n",
      "    return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1397, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:\n",
      "\n",
      "Detected at node 'fifo_queue_enqueue' defined at (most recent call last):\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "      app.start()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "      self.io_loop.start()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "      await result\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\insan\\AppData\\Local\\Temp\\ipykernel_11176\\135815682.py\", line 1, in <module>\n",
      "      deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=100000, allow_growth=True)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n",
      "      train(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 171, in train\n",
      "      batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 71, in setup_preloading\n",
      "      enqueue_op = q.enqueue(placeholders_list)\n",
      "Node: 'fifo_queue_enqueue'\n",
      "Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_enqueue':\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\insan\\AppData\\Local\\Temp\\ipykernel_11176\\135815682.py\", line 1, in <module>\n",
      "    deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=100000, allow_growth=True)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n",
      "    train(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 171, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 71, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 346, in enqueue\n",
      "    return gen_data_flow_ops.queue_enqueue_v2(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 4062, in queue_enqueue_v2\n",
      "    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n",
      "    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n",
      "    ret = Operation(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=100000, allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ad2586",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_Training_projectMar3shuffle1_100000  with # of training iterations: 100000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:02, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-100000\n",
      "Results for 100000  training iterations: 95 1 train error: 2.9 pixels. Test error: 4.73  pixels.\n",
      "With pcutoff of 0.6  train error: 2.9 pixels. Test error: 4.73 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:12<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(config_path,plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a120da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 7\\\\Cut\\\\AE_231_Cut.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccfa130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f697e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-100000 for model C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\\iteration-0\\Training_projectMar3-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\Cut\\AE_231_Cut.mp4\n",
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\Cut\\AE_231_Cut.mp4\n",
      "Duration of video [s]:  24.13 , recorded with  30.0 fps!\n",
      "Overall # of frames:  724  found with (before cropping) frame dimensions:  1920 1080\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [02:15<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\Cut...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video1 = deeplabcut.analyze_videos(config_path, [video_file],videotype='.mp4',auto_track=True,save_as_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fd3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(config_path, [video_file], save_frames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6464ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file2 = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 7\\\\AE_231_3\\\\AE_231_3_Test.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfbe912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-100000 for model C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\\iteration-0\\Training_projectMar3-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\AE_231_3\\AE_231_3_Test.mp4\n",
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\AE_231_3\\AE_231_3_Test.mp4\n",
      "Duration of video [s]:  30.5 , recorded with  30.0 fps!\n",
      "Overall # of frames:  915  found with (before cropping) frame dimensions:  1920 1080\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [02:23<00:00,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 7\\AE_231_3...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video2 = deeplabcut.analyze_videos(config_path, [video_file2],videotype='.mp4',auto_track=True,save_as_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5960169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(config_path, [video_file2], save_frames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace3209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f42c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best training method: with 500K iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192f178e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Documentation_data-Training_project_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ImageNet-pretrained resnet_50\n",
      "Max_iters overwritten as 500000\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 10000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4]], 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Documentation_data-Training_project_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 5, 'pos_dist_thresh': 17, 'project_path': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 100 loss: 0.0649 lr: 0.005\n",
      "iteration: 200 loss: 0.0249 lr: 0.005\n",
      "iteration: 300 loss: 0.0205 lr: 0.005\n",
      "iteration: 400 loss: 0.0184 lr: 0.005\n",
      "iteration: 500 loss: 0.0161 lr: 0.005\n",
      "iteration: 600 loss: 0.0142 lr: 0.005\n",
      "iteration: 700 loss: 0.0134 lr: 0.005\n",
      "iteration: 800 loss: 0.0136 lr: 0.005\n",
      "iteration: 900 loss: 0.0120 lr: 0.005\n",
      "iteration: 1000 loss: 0.0112 lr: 0.005\n",
      "iteration: 1100 loss: 0.0102 lr: 0.005\n",
      "iteration: 1200 loss: 0.0099 lr: 0.005\n",
      "iteration: 1300 loss: 0.0095 lr: 0.005\n",
      "iteration: 1400 loss: 0.0092 lr: 0.005\n",
      "iteration: 1500 loss: 0.0088 lr: 0.005\n",
      "iteration: 1600 loss: 0.0093 lr: 0.005\n",
      "iteration: 1700 loss: 0.0084 lr: 0.005\n",
      "iteration: 1800 loss: 0.0079 lr: 0.005\n",
      "iteration: 1900 loss: 0.0076 lr: 0.005\n",
      "iteration: 2000 loss: 0.0078 lr: 0.005\n",
      "iteration: 2100 loss: 0.0074 lr: 0.005\n",
      "iteration: 2200 loss: 0.0072 lr: 0.005\n",
      "iteration: 2300 loss: 0.0071 lr: 0.005\n",
      "iteration: 2400 loss: 0.0066 lr: 0.005\n",
      "iteration: 2500 loss: 0.0065 lr: 0.005\n",
      "iteration: 2600 loss: 0.0073 lr: 0.005\n",
      "iteration: 2700 loss: 0.0065 lr: 0.005\n",
      "iteration: 2800 loss: 0.0065 lr: 0.005\n",
      "iteration: 2900 loss: 0.0064 lr: 0.005\n",
      "iteration: 3000 loss: 0.0060 lr: 0.005\n",
      "iteration: 3100 loss: 0.0065 lr: 0.005\n",
      "iteration: 3200 loss: 0.0060 lr: 0.005\n",
      "iteration: 3300 loss: 0.0064 lr: 0.005\n",
      "iteration: 3400 loss: 0.0061 lr: 0.005\n",
      "iteration: 3500 loss: 0.0056 lr: 0.005\n",
      "iteration: 3600 loss: 0.0060 lr: 0.005\n",
      "iteration: 3700 loss: 0.0057 lr: 0.005\n",
      "iteration: 3800 loss: 0.0059 lr: 0.005\n",
      "iteration: 3900 loss: 0.0058 lr: 0.005\n",
      "iteration: 4000 loss: 0.0054 lr: 0.005\n",
      "iteration: 4100 loss: 0.0060 lr: 0.005\n",
      "iteration: 4200 loss: 0.0054 lr: 0.005\n",
      "iteration: 4300 loss: 0.0054 lr: 0.005\n",
      "iteration: 4400 loss: 0.0055 lr: 0.005\n",
      "iteration: 4500 loss: 0.0051 lr: 0.005\n",
      "iteration: 4600 loss: 0.0057 lr: 0.005\n",
      "iteration: 4700 loss: 0.0053 lr: 0.005\n",
      "iteration: 4800 loss: 0.0051 lr: 0.005\n",
      "iteration: 4900 loss: 0.0049 lr: 0.005\n",
      "iteration: 5000 loss: 0.0053 lr: 0.005\n",
      "iteration: 5100 loss: 0.0055 lr: 0.005\n",
      "iteration: 5200 loss: 0.0046 lr: 0.005\n",
      "iteration: 5300 loss: 0.0048 lr: 0.005\n",
      "iteration: 5400 loss: 0.0052 lr: 0.005\n",
      "iteration: 5500 loss: 0.0050 lr: 0.005\n",
      "iteration: 5600 loss: 0.0050 lr: 0.005\n",
      "iteration: 5700 loss: 0.0051 lr: 0.005\n",
      "iteration: 5800 loss: 0.0049 lr: 0.005\n",
      "iteration: 5900 loss: 0.0044 lr: 0.005\n",
      "iteration: 6000 loss: 0.0047 lr: 0.005\n",
      "iteration: 6100 loss: 0.0053 lr: 0.005\n",
      "iteration: 6200 loss: 0.0046 lr: 0.005\n",
      "iteration: 6300 loss: 0.0052 lr: 0.005\n",
      "iteration: 6400 loss: 0.0047 lr: 0.005\n",
      "iteration: 6500 loss: 0.0047 lr: 0.005\n",
      "iteration: 6600 loss: 0.0048 lr: 0.005\n",
      "iteration: 6700 loss: 0.0044 lr: 0.005\n",
      "iteration: 6800 loss: 0.0048 lr: 0.005\n",
      "iteration: 6900 loss: 0.0048 lr: 0.005\n",
      "iteration: 7000 loss: 0.0048 lr: 0.005\n",
      "iteration: 7100 loss: 0.0045 lr: 0.005\n",
      "iteration: 7200 loss: 0.0048 lr: 0.005\n",
      "iteration: 7300 loss: 0.0048 lr: 0.005\n",
      "iteration: 7400 loss: 0.0047 lr: 0.005\n",
      "iteration: 7500 loss: 0.0046 lr: 0.005\n",
      "iteration: 7600 loss: 0.0043 lr: 0.005\n",
      "iteration: 7700 loss: 0.0043 lr: 0.005\n",
      "iteration: 7800 loss: 0.0041 lr: 0.005\n",
      "iteration: 7900 loss: 0.0044 lr: 0.005\n",
      "iteration: 8000 loss: 0.0041 lr: 0.005\n",
      "iteration: 8100 loss: 0.0045 lr: 0.005\n",
      "iteration: 8200 loss: 0.0040 lr: 0.005\n",
      "iteration: 8300 loss: 0.0045 lr: 0.005\n",
      "iteration: 8400 loss: 0.0042 lr: 0.005\n",
      "iteration: 8500 loss: 0.0044 lr: 0.005\n",
      "iteration: 8600 loss: 0.0046 lr: 0.005\n",
      "iteration: 8700 loss: 0.0039 lr: 0.005\n",
      "iteration: 8800 loss: 0.0044 lr: 0.005\n",
      "iteration: 8900 loss: 0.0040 lr: 0.005\n",
      "iteration: 9000 loss: 0.0041 lr: 0.005\n",
      "iteration: 9100 loss: 0.0043 lr: 0.005\n",
      "iteration: 9200 loss: 0.0046 lr: 0.005\n",
      "iteration: 9300 loss: 0.0041 lr: 0.005\n",
      "iteration: 9400 loss: 0.0047 lr: 0.005\n",
      "iteration: 9500 loss: 0.0041 lr: 0.005\n",
      "iteration: 9600 loss: 0.0044 lr: 0.005\n",
      "iteration: 9700 loss: 0.0048 lr: 0.005\n",
      "iteration: 9800 loss: 0.0043 lr: 0.005\n",
      "iteration: 9900 loss: 0.0042 lr: 0.005\n",
      "iteration: 10000 loss: 0.0042 lr: 0.005\n",
      "iteration: 10100 loss: 0.0061 lr: 0.02\n",
      "iteration: 10200 loss: 0.0067 lr: 0.02\n",
      "iteration: 10300 loss: 0.0064 lr: 0.02\n",
      "iteration: 10400 loss: 0.0059 lr: 0.02\n",
      "iteration: 10500 loss: 0.0059 lr: 0.02\n",
      "iteration: 10600 loss: 0.0054 lr: 0.02\n",
      "iteration: 10700 loss: 0.0054 lr: 0.02\n",
      "iteration: 10800 loss: 0.0058 lr: 0.02\n",
      "iteration: 10900 loss: 0.0054 lr: 0.02\n",
      "iteration: 11000 loss: 0.0053 lr: 0.02\n",
      "iteration: 11100 loss: 0.0056 lr: 0.02\n",
      "iteration: 11200 loss: 0.0056 lr: 0.02\n",
      "iteration: 11300 loss: 0.0051 lr: 0.02\n",
      "iteration: 11400 loss: 0.0051 lr: 0.02\n",
      "iteration: 11500 loss: 0.0049 lr: 0.02\n",
      "iteration: 11600 loss: 0.0045 lr: 0.02\n",
      "iteration: 11700 loss: 0.0049 lr: 0.02\n",
      "iteration: 11800 loss: 0.0047 lr: 0.02\n",
      "iteration: 11900 loss: 0.0050 lr: 0.02\n",
      "iteration: 12000 loss: 0.0050 lr: 0.02\n",
      "iteration: 12100 loss: 0.0051 lr: 0.02\n",
      "iteration: 12200 loss: 0.0049 lr: 0.02\n",
      "iteration: 12300 loss: 0.0046 lr: 0.02\n",
      "iteration: 12400 loss: 0.0046 lr: 0.02\n",
      "iteration: 12500 loss: 0.0043 lr: 0.02\n",
      "iteration: 12600 loss: 0.0047 lr: 0.02\n",
      "iteration: 12700 loss: 0.0045 lr: 0.02\n",
      "iteration: 12800 loss: 0.0042 lr: 0.02\n",
      "iteration: 12900 loss: 0.0046 lr: 0.02\n",
      "iteration: 13000 loss: 0.0044 lr: 0.02\n",
      "iteration: 13100 loss: 0.0047 lr: 0.02\n",
      "iteration: 13200 loss: 0.0048 lr: 0.02\n",
      "iteration: 13300 loss: 0.0045 lr: 0.02\n",
      "iteration: 13400 loss: 0.0045 lr: 0.02\n",
      "iteration: 13500 loss: 0.0044 lr: 0.02\n",
      "iteration: 13600 loss: 0.0041 lr: 0.02\n",
      "iteration: 13700 loss: 0.0043 lr: 0.02\n",
      "iteration: 13800 loss: 0.0046 lr: 0.02\n",
      "iteration: 13900 loss: 0.0040 lr: 0.02\n",
      "iteration: 14000 loss: 0.0041 lr: 0.02\n",
      "iteration: 14100 loss: 0.0040 lr: 0.02\n",
      "iteration: 14200 loss: 0.0040 lr: 0.02\n",
      "iteration: 14300 loss: 0.0040 lr: 0.02\n",
      "iteration: 14400 loss: 0.0041 lr: 0.02\n",
      "iteration: 14500 loss: 0.0041 lr: 0.02\n",
      "iteration: 14600 loss: 0.0041 lr: 0.02\n",
      "iteration: 14700 loss: 0.0040 lr: 0.02\n",
      "iteration: 14800 loss: 0.0039 lr: 0.02\n",
      "iteration: 14900 loss: 0.0041 lr: 0.02\n",
      "iteration: 15000 loss: 0.0039 lr: 0.02\n",
      "iteration: 15100 loss: 0.0042 lr: 0.02\n",
      "iteration: 15200 loss: 0.0038 lr: 0.02\n",
      "iteration: 15300 loss: 0.0039 lr: 0.02\n",
      "iteration: 15400 loss: 0.0043 lr: 0.02\n",
      "iteration: 15500 loss: 0.0041 lr: 0.02\n",
      "iteration: 15600 loss: 0.0039 lr: 0.02\n",
      "iteration: 15700 loss: 0.0041 lr: 0.02\n",
      "iteration: 15800 loss: 0.0038 lr: 0.02\n",
      "iteration: 15900 loss: 0.0041 lr: 0.02\n",
      "iteration: 16000 loss: 0.0039 lr: 0.02\n",
      "iteration: 16100 loss: 0.0039 lr: 0.02\n",
      "iteration: 16200 loss: 0.0039 lr: 0.02\n",
      "iteration: 16300 loss: 0.0039 lr: 0.02\n",
      "iteration: 16400 loss: 0.0038 lr: 0.02\n",
      "iteration: 16500 loss: 0.0037 lr: 0.02\n",
      "iteration: 16600 loss: 0.0039 lr: 0.02\n",
      "iteration: 16700 loss: 0.0037 lr: 0.02\n",
      "iteration: 16800 loss: 0.0038 lr: 0.02\n",
      "iteration: 16900 loss: 0.0037 lr: 0.02\n",
      "iteration: 17000 loss: 0.0038 lr: 0.02\n",
      "iteration: 17100 loss: 0.0040 lr: 0.02\n",
      "iteration: 17200 loss: 0.0039 lr: 0.02\n",
      "iteration: 17300 loss: 0.0039 lr: 0.02\n",
      "iteration: 17400 loss: 0.0041 lr: 0.02\n",
      "iteration: 17500 loss: 0.0039 lr: 0.02\n",
      "iteration: 17600 loss: 0.0038 lr: 0.02\n",
      "iteration: 17700 loss: 0.0037 lr: 0.02\n",
      "iteration: 17800 loss: 0.0036 lr: 0.02\n",
      "iteration: 17900 loss: 0.0035 lr: 0.02\n",
      "iteration: 18000 loss: 0.0035 lr: 0.02\n",
      "iteration: 18100 loss: 0.0038 lr: 0.02\n",
      "iteration: 18200 loss: 0.0037 lr: 0.02\n",
      "iteration: 18300 loss: 0.0039 lr: 0.02\n",
      "iteration: 18400 loss: 0.0038 lr: 0.02\n",
      "iteration: 18500 loss: 0.0036 lr: 0.02\n",
      "iteration: 18600 loss: 0.0035 lr: 0.02\n",
      "iteration: 18700 loss: 0.0038 lr: 0.02\n",
      "iteration: 18800 loss: 0.0036 lr: 0.02\n",
      "iteration: 18900 loss: 0.0033 lr: 0.02\n",
      "iteration: 19000 loss: 0.0038 lr: 0.02\n",
      "iteration: 19100 loss: 0.0034 lr: 0.02\n",
      "iteration: 19200 loss: 0.0039 lr: 0.02\n",
      "iteration: 19300 loss: 0.0035 lr: 0.02\n",
      "iteration: 19400 loss: 0.0039 lr: 0.02\n",
      "iteration: 19500 loss: 0.0034 lr: 0.02\n",
      "iteration: 19600 loss: 0.0037 lr: 0.02\n",
      "iteration: 19700 loss: 0.0033 lr: 0.02\n",
      "iteration: 19800 loss: 0.0037 lr: 0.02\n",
      "iteration: 19900 loss: 0.0035 lr: 0.02\n",
      "iteration: 20000 loss: 0.0033 lr: 0.02\n",
      "iteration: 20100 loss: 0.0034 lr: 0.02\n",
      "iteration: 20200 loss: 0.0035 lr: 0.02\n",
      "iteration: 20300 loss: 0.0035 lr: 0.02\n",
      "iteration: 20400 loss: 0.0035 lr: 0.02\n",
      "iteration: 20500 loss: 0.0035 lr: 0.02\n",
      "iteration: 20600 loss: 0.0033 lr: 0.02\n",
      "iteration: 20700 loss: 0.0033 lr: 0.02\n",
      "iteration: 20800 loss: 0.0035 lr: 0.02\n",
      "iteration: 20900 loss: 0.0032 lr: 0.02\n",
      "iteration: 21000 loss: 0.0035 lr: 0.02\n",
      "iteration: 21100 loss: 0.0034 lr: 0.02\n",
      "iteration: 21200 loss: 0.0033 lr: 0.02\n",
      "iteration: 21300 loss: 0.0036 lr: 0.02\n",
      "iteration: 21400 loss: 0.0032 lr: 0.02\n",
      "iteration: 21500 loss: 0.0034 lr: 0.02\n",
      "iteration: 21600 loss: 0.0035 lr: 0.02\n",
      "iteration: 21700 loss: 0.0034 lr: 0.02\n",
      "iteration: 21800 loss: 0.0034 lr: 0.02\n",
      "iteration: 21900 loss: 0.0033 lr: 0.02\n",
      "iteration: 22000 loss: 0.0034 lr: 0.02\n",
      "iteration: 22100 loss: 0.0032 lr: 0.02\n",
      "iteration: 22200 loss: 0.0032 lr: 0.02\n",
      "iteration: 22300 loss: 0.0032 lr: 0.02\n",
      "iteration: 22400 loss: 0.0033 lr: 0.02\n",
      "iteration: 22500 loss: 0.0032 lr: 0.02\n",
      "iteration: 22600 loss: 0.0034 lr: 0.02\n",
      "iteration: 22700 loss: 0.0033 lr: 0.02\n",
      "iteration: 22800 loss: 0.0033 lr: 0.02\n",
      "iteration: 22900 loss: 0.0036 lr: 0.02\n",
      "iteration: 23000 loss: 0.0035 lr: 0.02\n",
      "iteration: 23100 loss: 0.0032 lr: 0.02\n",
      "iteration: 23200 loss: 0.0034 lr: 0.02\n",
      "iteration: 23300 loss: 0.0032 lr: 0.02\n",
      "iteration: 23400 loss: 0.0032 lr: 0.02\n",
      "iteration: 23500 loss: 0.0033 lr: 0.02\n",
      "iteration: 23600 loss: 0.0032 lr: 0.02\n",
      "iteration: 23700 loss: 0.0031 lr: 0.02\n",
      "iteration: 23800 loss: 0.0031 lr: 0.02\n",
      "iteration: 23900 loss: 0.0033 lr: 0.02\n",
      "iteration: 24000 loss: 0.0034 lr: 0.02\n",
      "iteration: 24100 loss: 0.0033 lr: 0.02\n",
      "iteration: 24200 loss: 0.0033 lr: 0.02\n",
      "iteration: 24300 loss: 0.0034 lr: 0.02\n",
      "iteration: 24400 loss: 0.0032 lr: 0.02\n",
      "iteration: 24500 loss: 0.0033 lr: 0.02\n",
      "iteration: 24600 loss: 0.0035 lr: 0.02\n",
      "iteration: 24700 loss: 0.0032 lr: 0.02\n",
      "iteration: 24800 loss: 0.0034 lr: 0.02\n",
      "iteration: 24900 loss: 0.0036 lr: 0.02\n",
      "iteration: 25000 loss: 0.0033 lr: 0.02\n",
      "iteration: 25100 loss: 0.0031 lr: 0.02\n",
      "iteration: 25200 loss: 0.0032 lr: 0.02\n",
      "iteration: 25300 loss: 0.0036 lr: 0.02\n",
      "iteration: 25400 loss: 0.0032 lr: 0.02\n",
      "iteration: 25500 loss: 0.0031 lr: 0.02\n",
      "iteration: 25600 loss: 0.0034 lr: 0.02\n",
      "iteration: 25700 loss: 0.0032 lr: 0.02\n",
      "iteration: 25800 loss: 0.0033 lr: 0.02\n",
      "iteration: 25900 loss: 0.0032 lr: 0.02\n",
      "iteration: 26000 loss: 0.0030 lr: 0.02\n",
      "iteration: 26100 loss: 0.0032 lr: 0.02\n",
      "iteration: 26200 loss: 0.0035 lr: 0.02\n",
      "iteration: 26300 loss: 0.0031 lr: 0.02\n",
      "iteration: 26400 loss: 0.0031 lr: 0.02\n",
      "iteration: 26500 loss: 0.0033 lr: 0.02\n",
      "iteration: 26600 loss: 0.0030 lr: 0.02\n",
      "iteration: 26700 loss: 0.0032 lr: 0.02\n",
      "iteration: 26800 loss: 0.0030 lr: 0.02\n",
      "iteration: 26900 loss: 0.0034 lr: 0.02\n",
      "iteration: 27000 loss: 0.0032 lr: 0.02\n",
      "iteration: 27100 loss: 0.0031 lr: 0.02\n",
      "iteration: 27200 loss: 0.0031 lr: 0.02\n",
      "iteration: 27300 loss: 0.0031 lr: 0.02\n",
      "iteration: 27400 loss: 0.0028 lr: 0.02\n",
      "iteration: 27500 loss: 0.0033 lr: 0.02\n",
      "iteration: 27600 loss: 0.0030 lr: 0.02\n",
      "iteration: 27700 loss: 0.0031 lr: 0.02\n",
      "iteration: 27800 loss: 0.0031 lr: 0.02\n",
      "iteration: 27900 loss: 0.0033 lr: 0.02\n",
      "iteration: 28000 loss: 0.0032 lr: 0.02\n",
      "iteration: 28100 loss: 0.0029 lr: 0.02\n",
      "iteration: 28200 loss: 0.0030 lr: 0.02\n",
      "iteration: 28300 loss: 0.0029 lr: 0.02\n",
      "iteration: 28400 loss: 0.0034 lr: 0.02\n",
      "iteration: 28500 loss: 0.0033 lr: 0.02\n",
      "iteration: 28600 loss: 0.0030 lr: 0.02\n",
      "iteration: 28700 loss: 0.0032 lr: 0.02\n",
      "iteration: 28800 loss: 0.0030 lr: 0.02\n",
      "iteration: 28900 loss: 0.0032 lr: 0.02\n",
      "iteration: 29000 loss: 0.0032 lr: 0.02\n",
      "iteration: 29100 loss: 0.0032 lr: 0.02\n",
      "iteration: 29200 loss: 0.0026 lr: 0.02\n",
      "iteration: 29300 loss: 0.0031 lr: 0.02\n",
      "iteration: 29400 loss: 0.0031 lr: 0.02\n",
      "iteration: 29500 loss: 0.0030 lr: 0.02\n",
      "iteration: 29600 loss: 0.0032 lr: 0.02\n",
      "iteration: 29700 loss: 0.0031 lr: 0.02\n",
      "iteration: 29800 loss: 0.0030 lr: 0.02\n",
      "iteration: 29900 loss: 0.0029 lr: 0.02\n",
      "iteration: 30000 loss: 0.0032 lr: 0.02\n",
      "iteration: 30100 loss: 0.0028 lr: 0.02\n",
      "iteration: 30200 loss: 0.0032 lr: 0.02\n",
      "iteration: 30300 loss: 0.0032 lr: 0.02\n",
      "iteration: 30400 loss: 0.0032 lr: 0.02\n",
      "iteration: 30500 loss: 0.0029 lr: 0.02\n",
      "iteration: 30600 loss: 0.0031 lr: 0.02\n",
      "iteration: 30700 loss: 0.0031 lr: 0.02\n",
      "iteration: 30800 loss: 0.0029 lr: 0.02\n",
      "iteration: 30900 loss: 0.0031 lr: 0.02\n",
      "iteration: 31000 loss: 0.0029 lr: 0.02\n",
      "iteration: 31100 loss: 0.0031 lr: 0.02\n",
      "iteration: 31200 loss: 0.0029 lr: 0.02\n",
      "iteration: 31300 loss: 0.0034 lr: 0.02\n",
      "iteration: 31400 loss: 0.0035 lr: 0.02\n",
      "iteration: 31500 loss: 0.0033 lr: 0.02\n",
      "iteration: 31600 loss: 0.0030 lr: 0.02\n",
      "iteration: 31700 loss: 0.0028 lr: 0.02\n",
      "iteration: 31800 loss: 0.0029 lr: 0.02\n",
      "iteration: 31900 loss: 0.0028 lr: 0.02\n",
      "iteration: 32000 loss: 0.0027 lr: 0.02\n",
      "iteration: 32100 loss: 0.0028 lr: 0.02\n",
      "iteration: 32200 loss: 0.0030 lr: 0.02\n",
      "iteration: 32300 loss: 0.0028 lr: 0.02\n",
      "iteration: 32400 loss: 0.0029 lr: 0.02\n",
      "iteration: 32500 loss: 0.0031 lr: 0.02\n",
      "iteration: 32600 loss: 0.0027 lr: 0.02\n",
      "iteration: 32700 loss: 0.0028 lr: 0.02\n",
      "iteration: 32800 loss: 0.0028 lr: 0.02\n",
      "iteration: 32900 loss: 0.0029 lr: 0.02\n",
      "iteration: 33000 loss: 0.0029 lr: 0.02\n",
      "iteration: 33100 loss: 0.0027 lr: 0.02\n",
      "iteration: 33200 loss: 0.0031 lr: 0.02\n",
      "iteration: 33300 loss: 0.0031 lr: 0.02\n",
      "iteration: 33400 loss: 0.0027 lr: 0.02\n",
      "iteration: 33500 loss: 0.0028 lr: 0.02\n",
      "iteration: 33600 loss: 0.0028 lr: 0.02\n",
      "iteration: 33700 loss: 0.0028 lr: 0.02\n",
      "iteration: 33800 loss: 0.0029 lr: 0.02\n",
      "iteration: 33900 loss: 0.0028 lr: 0.02\n",
      "iteration: 34000 loss: 0.0027 lr: 0.02\n",
      "iteration: 34100 loss: 0.0029 lr: 0.02\n",
      "iteration: 34200 loss: 0.0030 lr: 0.02\n",
      "iteration: 34300 loss: 0.0031 lr: 0.02\n",
      "iteration: 34400 loss: 0.0029 lr: 0.02\n",
      "iteration: 34500 loss: 0.0027 lr: 0.02\n",
      "iteration: 34600 loss: 0.0030 lr: 0.02\n",
      "iteration: 34700 loss: 0.0028 lr: 0.02\n",
      "iteration: 34800 loss: 0.0030 lr: 0.02\n",
      "iteration: 34900 loss: 0.0028 lr: 0.02\n",
      "iteration: 35000 loss: 0.0029 lr: 0.02\n",
      "iteration: 35100 loss: 0.0027 lr: 0.02\n",
      "iteration: 35200 loss: 0.0028 lr: 0.02\n",
      "iteration: 35300 loss: 0.0030 lr: 0.02\n",
      "iteration: 35400 loss: 0.0027 lr: 0.02\n",
      "iteration: 35500 loss: 0.0030 lr: 0.02\n",
      "iteration: 35600 loss: 0.0028 lr: 0.02\n",
      "iteration: 35700 loss: 0.0032 lr: 0.02\n",
      "iteration: 35800 loss: 0.0028 lr: 0.02\n",
      "iteration: 35900 loss: 0.0030 lr: 0.02\n",
      "iteration: 36000 loss: 0.0029 lr: 0.02\n",
      "iteration: 36100 loss: 0.0030 lr: 0.02\n",
      "iteration: 36200 loss: 0.0029 lr: 0.02\n",
      "iteration: 36300 loss: 0.0029 lr: 0.02\n",
      "iteration: 36400 loss: 0.0027 lr: 0.02\n",
      "iteration: 36500 loss: 0.0028 lr: 0.02\n",
      "iteration: 36600 loss: 0.0026 lr: 0.02\n",
      "iteration: 36700 loss: 0.0028 lr: 0.02\n",
      "iteration: 36800 loss: 0.0028 lr: 0.02\n",
      "iteration: 36900 loss: 0.0030 lr: 0.02\n",
      "iteration: 37000 loss: 0.0026 lr: 0.02\n",
      "iteration: 37100 loss: 0.0029 lr: 0.02\n",
      "iteration: 37200 loss: 0.0028 lr: 0.02\n",
      "iteration: 37300 loss: 0.0026 lr: 0.02\n",
      "iteration: 37400 loss: 0.0027 lr: 0.02\n",
      "iteration: 37500 loss: 0.0028 lr: 0.02\n",
      "iteration: 37600 loss: 0.0029 lr: 0.02\n",
      "iteration: 37700 loss: 0.0025 lr: 0.02\n",
      "iteration: 37800 loss: 0.0026 lr: 0.02\n",
      "iteration: 37900 loss: 0.0028 lr: 0.02\n",
      "iteration: 38000 loss: 0.0027 lr: 0.02\n",
      "iteration: 38100 loss: 0.0026 lr: 0.02\n",
      "iteration: 38200 loss: 0.0029 lr: 0.02\n",
      "iteration: 38300 loss: 0.0026 lr: 0.02\n",
      "iteration: 38400 loss: 0.0029 lr: 0.02\n",
      "iteration: 38500 loss: 0.0026 lr: 0.02\n",
      "iteration: 38600 loss: 0.0025 lr: 0.02\n",
      "iteration: 38700 loss: 0.0027 lr: 0.02\n",
      "iteration: 38800 loss: 0.0028 lr: 0.02\n",
      "iteration: 38900 loss: 0.0026 lr: 0.02\n",
      "iteration: 39000 loss: 0.0028 lr: 0.02\n",
      "iteration: 39100 loss: 0.0026 lr: 0.02\n",
      "iteration: 39200 loss: 0.0025 lr: 0.02\n",
      "iteration: 39300 loss: 0.0027 lr: 0.02\n",
      "iteration: 39400 loss: 0.0028 lr: 0.02\n",
      "iteration: 39500 loss: 0.0025 lr: 0.02\n",
      "iteration: 39600 loss: 0.0025 lr: 0.02\n",
      "iteration: 39700 loss: 0.0027 lr: 0.02\n",
      "iteration: 39800 loss: 0.0025 lr: 0.02\n",
      "iteration: 39900 loss: 0.0027 lr: 0.02\n",
      "iteration: 40000 loss: 0.0027 lr: 0.02\n",
      "iteration: 40100 loss: 0.0028 lr: 0.02\n",
      "iteration: 40200 loss: 0.0026 lr: 0.02\n",
      "iteration: 40300 loss: 0.0027 lr: 0.02\n",
      "iteration: 40400 loss: 0.0028 lr: 0.02\n",
      "iteration: 40500 loss: 0.0026 lr: 0.02\n",
      "iteration: 40600 loss: 0.0026 lr: 0.02\n",
      "iteration: 40700 loss: 0.0028 lr: 0.02\n",
      "iteration: 40800 loss: 0.0027 lr: 0.02\n",
      "iteration: 40900 loss: 0.0028 lr: 0.02\n",
      "iteration: 41000 loss: 0.0028 lr: 0.02\n",
      "iteration: 41100 loss: 0.0026 lr: 0.02\n",
      "iteration: 41200 loss: 0.0027 lr: 0.02\n",
      "iteration: 41300 loss: 0.0028 lr: 0.02\n",
      "iteration: 41400 loss: 0.0027 lr: 0.02\n",
      "iteration: 41500 loss: 0.0028 lr: 0.02\n",
      "iteration: 41600 loss: 0.0026 lr: 0.02\n",
      "iteration: 41700 loss: 0.0026 lr: 0.02\n",
      "iteration: 41800 loss: 0.0026 lr: 0.02\n",
      "iteration: 41900 loss: 0.0025 lr: 0.02\n",
      "iteration: 42000 loss: 0.0026 lr: 0.02\n",
      "iteration: 42100 loss: 0.0026 lr: 0.02\n",
      "iteration: 42200 loss: 0.0027 lr: 0.02\n",
      "iteration: 42300 loss: 0.0026 lr: 0.02\n",
      "iteration: 42400 loss: 0.0024 lr: 0.02\n",
      "iteration: 42500 loss: 0.0026 lr: 0.02\n",
      "iteration: 42600 loss: 0.0025 lr: 0.02\n",
      "iteration: 42700 loss: 0.0024 lr: 0.02\n",
      "iteration: 42800 loss: 0.0029 lr: 0.02\n",
      "iteration: 42900 loss: 0.0025 lr: 0.02\n",
      "iteration: 43000 loss: 0.0025 lr: 0.02\n",
      "iteration: 43100 loss: 0.0027 lr: 0.02\n",
      "iteration: 43200 loss: 0.0025 lr: 0.02\n",
      "iteration: 43300 loss: 0.0025 lr: 0.02\n",
      "iteration: 43400 loss: 0.0025 lr: 0.02\n",
      "iteration: 43500 loss: 0.0025 lr: 0.02\n",
      "iteration: 43600 loss: 0.0028 lr: 0.02\n",
      "iteration: 43700 loss: 0.0027 lr: 0.02\n",
      "iteration: 43800 loss: 0.0025 lr: 0.02\n",
      "iteration: 43900 loss: 0.0026 lr: 0.02\n",
      "iteration: 44000 loss: 0.0027 lr: 0.02\n",
      "iteration: 44100 loss: 0.0026 lr: 0.02\n",
      "iteration: 44200 loss: 0.0027 lr: 0.02\n",
      "iteration: 44300 loss: 0.0026 lr: 0.02\n",
      "iteration: 44400 loss: 0.0023 lr: 0.02\n",
      "iteration: 44500 loss: 0.0026 lr: 0.02\n",
      "iteration: 44600 loss: 0.0025 lr: 0.02\n",
      "iteration: 44700 loss: 0.0028 lr: 0.02\n",
      "iteration: 44800 loss: 0.0027 lr: 0.02\n",
      "iteration: 44900 loss: 0.0027 lr: 0.02\n",
      "iteration: 45000 loss: 0.0024 lr: 0.02\n",
      "iteration: 45100 loss: 0.0025 lr: 0.02\n",
      "iteration: 45200 loss: 0.0023 lr: 0.02\n",
      "iteration: 45300 loss: 0.0025 lr: 0.02\n",
      "iteration: 45400 loss: 0.0026 lr: 0.02\n",
      "iteration: 45500 loss: 0.0027 lr: 0.02\n",
      "iteration: 45600 loss: 0.0024 lr: 0.02\n",
      "iteration: 45700 loss: 0.0026 lr: 0.02\n",
      "iteration: 45800 loss: 0.0023 lr: 0.02\n",
      "iteration: 45900 loss: 0.0028 lr: 0.02\n",
      "iteration: 46000 loss: 0.0025 lr: 0.02\n",
      "iteration: 46100 loss: 0.0025 lr: 0.02\n",
      "iteration: 46200 loss: 0.0024 lr: 0.02\n",
      "iteration: 46300 loss: 0.0027 lr: 0.02\n",
      "iteration: 46400 loss: 0.0023 lr: 0.02\n",
      "iteration: 46500 loss: 0.0025 lr: 0.02\n",
      "iteration: 46600 loss: 0.0025 lr: 0.02\n",
      "iteration: 46700 loss: 0.0026 lr: 0.02\n",
      "iteration: 46800 loss: 0.0022 lr: 0.02\n",
      "iteration: 46900 loss: 0.0025 lr: 0.02\n",
      "iteration: 47000 loss: 0.0023 lr: 0.02\n",
      "iteration: 47100 loss: 0.0026 lr: 0.02\n",
      "iteration: 47200 loss: 0.0024 lr: 0.02\n",
      "iteration: 47300 loss: 0.0025 lr: 0.02\n",
      "iteration: 47400 loss: 0.0025 lr: 0.02\n",
      "iteration: 47500 loss: 0.0025 lr: 0.02\n",
      "iteration: 47600 loss: 0.0025 lr: 0.02\n",
      "iteration: 47700 loss: 0.0024 lr: 0.02\n",
      "iteration: 47800 loss: 0.0024 lr: 0.02\n",
      "iteration: 47900 loss: 0.0024 lr: 0.02\n",
      "iteration: 48000 loss: 0.0024 lr: 0.02\n",
      "iteration: 48100 loss: 0.0024 lr: 0.02\n",
      "iteration: 48200 loss: 0.0022 lr: 0.02\n",
      "iteration: 48300 loss: 0.0025 lr: 0.02\n",
      "iteration: 48400 loss: 0.0024 lr: 0.02\n",
      "iteration: 48500 loss: 0.0024 lr: 0.02\n",
      "iteration: 48600 loss: 0.0022 lr: 0.02\n",
      "iteration: 48700 loss: 0.0025 lr: 0.02\n",
      "iteration: 48800 loss: 0.0023 lr: 0.02\n",
      "iteration: 48900 loss: 0.0024 lr: 0.02\n",
      "iteration: 49000 loss: 0.0026 lr: 0.02\n",
      "iteration: 49100 loss: 0.0026 lr: 0.02\n",
      "iteration: 49200 loss: 0.0025 lr: 0.02\n",
      "iteration: 49300 loss: 0.0023 lr: 0.02\n",
      "iteration: 49400 loss: 0.0024 lr: 0.02\n",
      "iteration: 49500 loss: 0.0026 lr: 0.02\n",
      "iteration: 49600 loss: 0.0025 lr: 0.02\n",
      "iteration: 49700 loss: 0.0023 lr: 0.02\n",
      "iteration: 49800 loss: 0.0024 lr: 0.02\n",
      "iteration: 49900 loss: 0.0023 lr: 0.02\n",
      "iteration: 50000 loss: 0.0026 lr: 0.02\n",
      "iteration: 50100 loss: 0.0022 lr: 0.02\n",
      "iteration: 50200 loss: 0.0023 lr: 0.02\n",
      "iteration: 50300 loss: 0.0026 lr: 0.02\n",
      "iteration: 50400 loss: 0.0026 lr: 0.02\n",
      "iteration: 50500 loss: 0.0023 lr: 0.02\n",
      "iteration: 50600 loss: 0.0024 lr: 0.02\n",
      "iteration: 50700 loss: 0.0023 lr: 0.02\n",
      "iteration: 50800 loss: 0.0024 lr: 0.02\n",
      "iteration: 50900 loss: 0.0022 lr: 0.02\n",
      "iteration: 51000 loss: 0.0024 lr: 0.02\n",
      "iteration: 51100 loss: 0.0023 lr: 0.02\n",
      "iteration: 51200 loss: 0.0024 lr: 0.02\n",
      "iteration: 51300 loss: 0.0024 lr: 0.02\n",
      "iteration: 51400 loss: 0.0025 lr: 0.02\n",
      "iteration: 51500 loss: 0.0022 lr: 0.02\n",
      "iteration: 51600 loss: 0.0023 lr: 0.02\n",
      "iteration: 51700 loss: 0.0023 lr: 0.02\n",
      "iteration: 51800 loss: 0.0024 lr: 0.02\n",
      "iteration: 51900 loss: 0.0023 lr: 0.02\n",
      "iteration: 52000 loss: 0.0024 lr: 0.02\n",
      "iteration: 52100 loss: 0.0023 lr: 0.02\n",
      "iteration: 52200 loss: 0.0023 lr: 0.02\n",
      "iteration: 52300 loss: 0.0024 lr: 0.02\n",
      "iteration: 52400 loss: 0.0025 lr: 0.02\n",
      "iteration: 52500 loss: 0.0023 lr: 0.02\n",
      "iteration: 52600 loss: 0.0025 lr: 0.02\n",
      "iteration: 52700 loss: 0.0026 lr: 0.02\n",
      "iteration: 52800 loss: 0.0023 lr: 0.02\n",
      "iteration: 52900 loss: 0.0023 lr: 0.02\n",
      "iteration: 53000 loss: 0.0025 lr: 0.02\n",
      "iteration: 53100 loss: 0.0023 lr: 0.02\n",
      "iteration: 53200 loss: 0.0022 lr: 0.02\n",
      "iteration: 53300 loss: 0.0023 lr: 0.02\n",
      "iteration: 53400 loss: 0.0022 lr: 0.02\n",
      "iteration: 53500 loss: 0.0024 lr: 0.02\n",
      "iteration: 53600 loss: 0.0024 lr: 0.02\n",
      "iteration: 53700 loss: 0.0024 lr: 0.02\n",
      "iteration: 53800 loss: 0.0024 lr: 0.02\n",
      "iteration: 53900 loss: 0.0021 lr: 0.02\n",
      "iteration: 54000 loss: 0.0023 lr: 0.02\n",
      "iteration: 54100 loss: 0.0024 lr: 0.02\n",
      "iteration: 54200 loss: 0.0023 lr: 0.02\n",
      "iteration: 54300 loss: 0.0022 lr: 0.02\n",
      "iteration: 54400 loss: 0.0021 lr: 0.02\n",
      "iteration: 54500 loss: 0.0024 lr: 0.02\n",
      "iteration: 54600 loss: 0.0023 lr: 0.02\n",
      "iteration: 54700 loss: 0.0022 lr: 0.02\n",
      "iteration: 54800 loss: 0.0022 lr: 0.02\n",
      "iteration: 54900 loss: 0.0026 lr: 0.02\n",
      "iteration: 55000 loss: 0.0023 lr: 0.02\n",
      "iteration: 55100 loss: 0.0022 lr: 0.02\n",
      "iteration: 55200 loss: 0.0023 lr: 0.02\n",
      "iteration: 55300 loss: 0.0024 lr: 0.02\n",
      "iteration: 55400 loss: 0.0022 lr: 0.02\n",
      "iteration: 55500 loss: 0.0022 lr: 0.02\n",
      "iteration: 55600 loss: 0.0023 lr: 0.02\n",
      "iteration: 55700 loss: 0.0022 lr: 0.02\n",
      "iteration: 55800 loss: 0.0021 lr: 0.02\n",
      "iteration: 55900 loss: 0.0021 lr: 0.02\n",
      "iteration: 56000 loss: 0.0023 lr: 0.02\n",
      "iteration: 56100 loss: 0.0022 lr: 0.02\n",
      "iteration: 56200 loss: 0.0024 lr: 0.02\n",
      "iteration: 56300 loss: 0.0023 lr: 0.02\n",
      "iteration: 56400 loss: 0.0024 lr: 0.02\n",
      "iteration: 56500 loss: 0.0021 lr: 0.02\n",
      "iteration: 56600 loss: 0.0023 lr: 0.02\n",
      "iteration: 56700 loss: 0.0022 lr: 0.02\n",
      "iteration: 56800 loss: 0.0023 lr: 0.02\n",
      "iteration: 56900 loss: 0.0023 lr: 0.02\n",
      "iteration: 57000 loss: 0.0023 lr: 0.02\n",
      "iteration: 57100 loss: 0.0021 lr: 0.02\n",
      "iteration: 57200 loss: 0.0021 lr: 0.02\n",
      "iteration: 57300 loss: 0.0022 lr: 0.02\n",
      "iteration: 57400 loss: 0.0023 lr: 0.02\n",
      "iteration: 57500 loss: 0.0022 lr: 0.02\n",
      "iteration: 57600 loss: 0.0020 lr: 0.02\n",
      "iteration: 57700 loss: 0.0023 lr: 0.02\n",
      "iteration: 57800 loss: 0.0021 lr: 0.02\n",
      "iteration: 57900 loss: 0.0021 lr: 0.02\n",
      "iteration: 58000 loss: 0.0020 lr: 0.02\n",
      "iteration: 58100 loss: 0.0021 lr: 0.02\n",
      "iteration: 58200 loss: 0.0024 lr: 0.02\n",
      "iteration: 58300 loss: 0.0022 lr: 0.02\n",
      "iteration: 58400 loss: 0.0023 lr: 0.02\n",
      "iteration: 58500 loss: 0.0023 lr: 0.02\n",
      "iteration: 58600 loss: 0.0025 lr: 0.02\n",
      "iteration: 58700 loss: 0.0022 lr: 0.02\n",
      "iteration: 58800 loss: 0.0022 lr: 0.02\n",
      "iteration: 58900 loss: 0.0023 lr: 0.02\n",
      "iteration: 59000 loss: 0.0020 lr: 0.02\n",
      "iteration: 59100 loss: 0.0023 lr: 0.02\n",
      "iteration: 59200 loss: 0.0022 lr: 0.02\n",
      "iteration: 59300 loss: 0.0021 lr: 0.02\n",
      "iteration: 59400 loss: 0.0021 lr: 0.02\n",
      "iteration: 59500 loss: 0.0021 lr: 0.02\n",
      "iteration: 59600 loss: 0.0021 lr: 0.02\n",
      "iteration: 59700 loss: 0.0023 lr: 0.02\n",
      "iteration: 59800 loss: 0.0025 lr: 0.02\n",
      "iteration: 59900 loss: 0.0023 lr: 0.02\n",
      "iteration: 60000 loss: 0.0021 lr: 0.02\n",
      "iteration: 60100 loss: 0.0021 lr: 0.02\n",
      "iteration: 60200 loss: 0.0021 lr: 0.02\n",
      "iteration: 60300 loss: 0.0021 lr: 0.02\n",
      "iteration: 60400 loss: 0.0022 lr: 0.02\n",
      "iteration: 60500 loss: 0.0022 lr: 0.02\n",
      "iteration: 60600 loss: 0.0020 lr: 0.02\n",
      "iteration: 60700 loss: 0.0021 lr: 0.02\n",
      "iteration: 60800 loss: 0.0020 lr: 0.02\n",
      "iteration: 60900 loss: 0.0020 lr: 0.02\n",
      "iteration: 61000 loss: 0.0021 lr: 0.02\n",
      "iteration: 61100 loss: 0.0020 lr: 0.02\n",
      "iteration: 61200 loss: 0.0022 lr: 0.02\n",
      "iteration: 61300 loss: 0.0023 lr: 0.02\n",
      "iteration: 61400 loss: 0.0020 lr: 0.02\n",
      "iteration: 61500 loss: 0.0021 lr: 0.02\n",
      "iteration: 61600 loss: 0.0023 lr: 0.02\n",
      "iteration: 61700 loss: 0.0022 lr: 0.02\n",
      "iteration: 61800 loss: 0.0021 lr: 0.02\n",
      "iteration: 61900 loss: 0.0019 lr: 0.02\n",
      "iteration: 62000 loss: 0.0021 lr: 0.02\n",
      "iteration: 62100 loss: 0.0021 lr: 0.02\n",
      "iteration: 62200 loss: 0.0020 lr: 0.02\n",
      "iteration: 62300 loss: 0.0020 lr: 0.02\n",
      "iteration: 62400 loss: 0.0021 lr: 0.02\n",
      "iteration: 62500 loss: 0.0020 lr: 0.02\n",
      "iteration: 62600 loss: 0.0022 lr: 0.02\n",
      "iteration: 62700 loss: 0.0023 lr: 0.02\n",
      "iteration: 62800 loss: 0.0022 lr: 0.02\n",
      "iteration: 62900 loss: 0.0021 lr: 0.02\n",
      "iteration: 63000 loss: 0.0020 lr: 0.02\n",
      "iteration: 63100 loss: 0.0019 lr: 0.02\n",
      "iteration: 63200 loss: 0.0024 lr: 0.02\n",
      "iteration: 63300 loss: 0.0021 lr: 0.02\n",
      "iteration: 63400 loss: 0.0021 lr: 0.02\n",
      "iteration: 63500 loss: 0.0020 lr: 0.02\n",
      "iteration: 63600 loss: 0.0024 lr: 0.02\n",
      "iteration: 63700 loss: 0.0022 lr: 0.02\n",
      "iteration: 63800 loss: 0.0023 lr: 0.02\n",
      "iteration: 63900 loss: 0.0022 lr: 0.02\n",
      "iteration: 64000 loss: 0.0021 lr: 0.02\n",
      "iteration: 64100 loss: 0.0021 lr: 0.02\n",
      "iteration: 64200 loss: 0.0020 lr: 0.02\n",
      "iteration: 64300 loss: 0.0020 lr: 0.02\n",
      "iteration: 64400 loss: 0.0020 lr: 0.02\n",
      "iteration: 64500 loss: 0.0021 lr: 0.02\n",
      "iteration: 64600 loss: 0.0022 lr: 0.02\n",
      "iteration: 64700 loss: 0.0023 lr: 0.02\n",
      "iteration: 64800 loss: 0.0021 lr: 0.02\n",
      "iteration: 64900 loss: 0.0021 lr: 0.02\n",
      "iteration: 65000 loss: 0.0021 lr: 0.02\n",
      "iteration: 65100 loss: 0.0019 lr: 0.02\n",
      "iteration: 65200 loss: 0.0020 lr: 0.02\n",
      "iteration: 65300 loss: 0.0021 lr: 0.02\n",
      "iteration: 65400 loss: 0.0020 lr: 0.02\n",
      "iteration: 65500 loss: 0.0018 lr: 0.02\n",
      "iteration: 65600 loss: 0.0022 lr: 0.02\n",
      "iteration: 65700 loss: 0.0019 lr: 0.02\n",
      "iteration: 65800 loss: 0.0021 lr: 0.02\n",
      "iteration: 65900 loss: 0.0022 lr: 0.02\n",
      "iteration: 66000 loss: 0.0020 lr: 0.02\n",
      "iteration: 66100 loss: 0.0020 lr: 0.02\n",
      "iteration: 66200 loss: 0.0022 lr: 0.02\n",
      "iteration: 66300 loss: 0.0020 lr: 0.02\n",
      "iteration: 66400 loss: 0.0022 lr: 0.02\n",
      "iteration: 66500 loss: 0.0021 lr: 0.02\n",
      "iteration: 66600 loss: 0.0020 lr: 0.02\n",
      "iteration: 66700 loss: 0.0021 lr: 0.02\n",
      "iteration: 66800 loss: 0.0020 lr: 0.02\n",
      "iteration: 66900 loss: 0.0020 lr: 0.02\n",
      "iteration: 67000 loss: 0.0022 lr: 0.02\n",
      "iteration: 67100 loss: 0.0019 lr: 0.02\n",
      "iteration: 67200 loss: 0.0022 lr: 0.02\n",
      "iteration: 67300 loss: 0.0019 lr: 0.02\n",
      "iteration: 67400 loss: 0.0022 lr: 0.02\n",
      "iteration: 67500 loss: 0.0019 lr: 0.02\n",
      "iteration: 67600 loss: 0.0019 lr: 0.02\n",
      "iteration: 67700 loss: 0.0020 lr: 0.02\n",
      "iteration: 67800 loss: 0.0019 lr: 0.02\n",
      "iteration: 67900 loss: 0.0019 lr: 0.02\n",
      "iteration: 68000 loss: 0.0020 lr: 0.02\n",
      "iteration: 68100 loss: 0.0019 lr: 0.02\n",
      "iteration: 68200 loss: 0.0022 lr: 0.02\n",
      "iteration: 68300 loss: 0.0020 lr: 0.02\n",
      "iteration: 68400 loss: 0.0020 lr: 0.02\n",
      "iteration: 68500 loss: 0.0021 lr: 0.02\n",
      "iteration: 68600 loss: 0.0021 lr: 0.02\n",
      "iteration: 68700 loss: 0.0018 lr: 0.02\n",
      "iteration: 68800 loss: 0.0020 lr: 0.02\n",
      "iteration: 68900 loss: 0.0019 lr: 0.02\n",
      "iteration: 69000 loss: 0.0021 lr: 0.02\n",
      "iteration: 69100 loss: 0.0020 lr: 0.02\n",
      "iteration: 69200 loss: 0.0020 lr: 0.02\n",
      "iteration: 69300 loss: 0.0021 lr: 0.02\n",
      "iteration: 69400 loss: 0.0020 lr: 0.02\n",
      "iteration: 69500 loss: 0.0020 lr: 0.02\n",
      "iteration: 69600 loss: 0.0019 lr: 0.02\n",
      "iteration: 69700 loss: 0.0019 lr: 0.02\n",
      "iteration: 69800 loss: 0.0018 lr: 0.02\n",
      "iteration: 69900 loss: 0.0022 lr: 0.02\n",
      "iteration: 70000 loss: 0.0020 lr: 0.02\n",
      "iteration: 70100 loss: 0.0022 lr: 0.02\n",
      "iteration: 70200 loss: 0.0021 lr: 0.02\n",
      "iteration: 70300 loss: 0.0022 lr: 0.02\n",
      "iteration: 70400 loss: 0.0019 lr: 0.02\n",
      "iteration: 70500 loss: 0.0020 lr: 0.02\n",
      "iteration: 70600 loss: 0.0021 lr: 0.02\n",
      "iteration: 70700 loss: 0.0019 lr: 0.02\n",
      "iteration: 70800 loss: 0.0020 lr: 0.02\n",
      "iteration: 70900 loss: 0.0018 lr: 0.02\n",
      "iteration: 71000 loss: 0.0020 lr: 0.02\n",
      "iteration: 71100 loss: 0.0020 lr: 0.02\n",
      "iteration: 71200 loss: 0.0019 lr: 0.02\n",
      "iteration: 71300 loss: 0.0020 lr: 0.02\n",
      "iteration: 71400 loss: 0.0020 lr: 0.02\n",
      "iteration: 71500 loss: 0.0020 lr: 0.02\n",
      "iteration: 71600 loss: 0.0019 lr: 0.02\n",
      "iteration: 71700 loss: 0.0020 lr: 0.02\n",
      "iteration: 71800 loss: 0.0019 lr: 0.02\n",
      "iteration: 71900 loss: 0.0019 lr: 0.02\n",
      "iteration: 72000 loss: 0.0020 lr: 0.02\n",
      "iteration: 72100 loss: 0.0018 lr: 0.02\n",
      "iteration: 72200 loss: 0.0020 lr: 0.02\n",
      "iteration: 72300 loss: 0.0020 lr: 0.02\n",
      "iteration: 72400 loss: 0.0019 lr: 0.02\n",
      "iteration: 72500 loss: 0.0021 lr: 0.02\n",
      "iteration: 72600 loss: 0.0021 lr: 0.02\n",
      "iteration: 72700 loss: 0.0019 lr: 0.02\n",
      "iteration: 72800 loss: 0.0020 lr: 0.02\n",
      "iteration: 72900 loss: 0.0019 lr: 0.02\n",
      "iteration: 73000 loss: 0.0021 lr: 0.02\n",
      "iteration: 73100 loss: 0.0021 lr: 0.02\n",
      "iteration: 73200 loss: 0.0018 lr: 0.02\n",
      "iteration: 73300 loss: 0.0019 lr: 0.02\n",
      "iteration: 73400 loss: 0.0018 lr: 0.02\n",
      "iteration: 73500 loss: 0.0020 lr: 0.02\n",
      "iteration: 73600 loss: 0.0018 lr: 0.02\n",
      "iteration: 73700 loss: 0.0018 lr: 0.02\n",
      "iteration: 73800 loss: 0.0020 lr: 0.02\n",
      "iteration: 73900 loss: 0.0020 lr: 0.02\n",
      "iteration: 74000 loss: 0.0020 lr: 0.02\n",
      "iteration: 74100 loss: 0.0019 lr: 0.02\n",
      "iteration: 74200 loss: 0.0019 lr: 0.02\n",
      "iteration: 74300 loss: 0.0018 lr: 0.02\n",
      "iteration: 74400 loss: 0.0019 lr: 0.02\n",
      "iteration: 74500 loss: 0.0019 lr: 0.02\n",
      "iteration: 74600 loss: 0.0019 lr: 0.02\n",
      "iteration: 74700 loss: 0.0022 lr: 0.02\n",
      "iteration: 74800 loss: 0.0019 lr: 0.02\n",
      "iteration: 74900 loss: 0.0019 lr: 0.02\n",
      "iteration: 75000 loss: 0.0020 lr: 0.02\n",
      "iteration: 75100 loss: 0.0017 lr: 0.02\n",
      "iteration: 75200 loss: 0.0018 lr: 0.02\n",
      "iteration: 75300 loss: 0.0019 lr: 0.02\n",
      "iteration: 75400 loss: 0.0019 lr: 0.02\n",
      "iteration: 75500 loss: 0.0019 lr: 0.02\n",
      "iteration: 75600 loss: 0.0018 lr: 0.02\n",
      "iteration: 75700 loss: 0.0020 lr: 0.02\n",
      "iteration: 75800 loss: 0.0020 lr: 0.02\n",
      "iteration: 75900 loss: 0.0021 lr: 0.02\n",
      "iteration: 76000 loss: 0.0019 lr: 0.02\n",
      "iteration: 76100 loss: 0.0019 lr: 0.02\n",
      "iteration: 76200 loss: 0.0019 lr: 0.02\n",
      "iteration: 76300 loss: 0.0019 lr: 0.02\n",
      "iteration: 76400 loss: 0.0019 lr: 0.02\n",
      "iteration: 76500 loss: 0.0019 lr: 0.02\n",
      "iteration: 76600 loss: 0.0018 lr: 0.02\n",
      "iteration: 76700 loss: 0.0019 lr: 0.02\n",
      "iteration: 76800 loss: 0.0020 lr: 0.02\n",
      "iteration: 76900 loss: 0.0019 lr: 0.02\n",
      "iteration: 77000 loss: 0.0017 lr: 0.02\n",
      "iteration: 77100 loss: 0.0019 lr: 0.02\n",
      "iteration: 77200 loss: 0.0021 lr: 0.02\n",
      "iteration: 77300 loss: 0.0020 lr: 0.02\n",
      "iteration: 77400 loss: 0.0019 lr: 0.02\n",
      "iteration: 77500 loss: 0.0021 lr: 0.02\n",
      "iteration: 77600 loss: 0.0020 lr: 0.02\n",
      "iteration: 77700 loss: 0.0021 lr: 0.02\n",
      "iteration: 77800 loss: 0.0021 lr: 0.02\n",
      "iteration: 77900 loss: 0.0020 lr: 0.02\n",
      "iteration: 78000 loss: 0.0019 lr: 0.02\n",
      "iteration: 78100 loss: 0.0020 lr: 0.02\n",
      "iteration: 78200 loss: 0.0019 lr: 0.02\n",
      "iteration: 78300 loss: 0.0022 lr: 0.02\n",
      "iteration: 78400 loss: 0.0020 lr: 0.02\n",
      "iteration: 78500 loss: 0.0020 lr: 0.02\n",
      "iteration: 78600 loss: 0.0017 lr: 0.02\n",
      "iteration: 78700 loss: 0.0021 lr: 0.02\n",
      "iteration: 78800 loss: 0.0018 lr: 0.02\n",
      "iteration: 78900 loss: 0.0018 lr: 0.02\n",
      "iteration: 79000 loss: 0.0019 lr: 0.02\n",
      "iteration: 79100 loss: 0.0019 lr: 0.02\n",
      "iteration: 79200 loss: 0.0019 lr: 0.02\n",
      "iteration: 79300 loss: 0.0019 lr: 0.02\n",
      "iteration: 79400 loss: 0.0020 lr: 0.02\n",
      "iteration: 79500 loss: 0.0020 lr: 0.02\n",
      "iteration: 79600 loss: 0.0018 lr: 0.02\n",
      "iteration: 79700 loss: 0.0019 lr: 0.02\n",
      "iteration: 79800 loss: 0.0020 lr: 0.02\n",
      "iteration: 79900 loss: 0.0022 lr: 0.02\n",
      "iteration: 80000 loss: 0.0018 lr: 0.02\n",
      "iteration: 80100 loss: 0.0018 lr: 0.02\n",
      "iteration: 80200 loss: 0.0018 lr: 0.02\n",
      "iteration: 80300 loss: 0.0017 lr: 0.02\n",
      "iteration: 80400 loss: 0.0019 lr: 0.02\n",
      "iteration: 80500 loss: 0.0018 lr: 0.02\n",
      "iteration: 80600 loss: 0.0020 lr: 0.02\n",
      "iteration: 80700 loss: 0.0018 lr: 0.02\n",
      "iteration: 80800 loss: 0.0019 lr: 0.02\n",
      "iteration: 80900 loss: 0.0019 lr: 0.02\n",
      "iteration: 81000 loss: 0.0018 lr: 0.02\n",
      "iteration: 81100 loss: 0.0019 lr: 0.02\n",
      "iteration: 81200 loss: 0.0018 lr: 0.02\n",
      "iteration: 81300 loss: 0.0020 lr: 0.02\n",
      "iteration: 81400 loss: 0.0019 lr: 0.02\n",
      "iteration: 81500 loss: 0.0019 lr: 0.02\n",
      "iteration: 81600 loss: 0.0018 lr: 0.02\n",
      "iteration: 81700 loss: 0.0018 lr: 0.02\n",
      "iteration: 81800 loss: 0.0017 lr: 0.02\n",
      "iteration: 81900 loss: 0.0020 lr: 0.02\n",
      "iteration: 82000 loss: 0.0017 lr: 0.02\n",
      "iteration: 82100 loss: 0.0018 lr: 0.02\n",
      "iteration: 82200 loss: 0.0017 lr: 0.02\n",
      "iteration: 82300 loss: 0.0020 lr: 0.02\n",
      "iteration: 82400 loss: 0.0017 lr: 0.02\n",
      "iteration: 82500 loss: 0.0018 lr: 0.02\n",
      "iteration: 82600 loss: 0.0019 lr: 0.02\n",
      "iteration: 82700 loss: 0.0019 lr: 0.02\n",
      "iteration: 82800 loss: 0.0020 lr: 0.02\n",
      "iteration: 82900 loss: 0.0019 lr: 0.02\n",
      "iteration: 83000 loss: 0.0019 lr: 0.02\n",
      "iteration: 83100 loss: 0.0019 lr: 0.02\n",
      "iteration: 83200 loss: 0.0019 lr: 0.02\n",
      "iteration: 83300 loss: 0.0018 lr: 0.02\n",
      "iteration: 83400 loss: 0.0018 lr: 0.02\n",
      "iteration: 83500 loss: 0.0017 lr: 0.02\n",
      "iteration: 83600 loss: 0.0019 lr: 0.02\n",
      "iteration: 83700 loss: 0.0019 lr: 0.02\n",
      "iteration: 83800 loss: 0.0020 lr: 0.02\n",
      "iteration: 83900 loss: 0.0018 lr: 0.02\n",
      "iteration: 84000 loss: 0.0019 lr: 0.02\n",
      "iteration: 84100 loss: 0.0018 lr: 0.02\n",
      "iteration: 84200 loss: 0.0016 lr: 0.02\n",
      "iteration: 84300 loss: 0.0017 lr: 0.02\n",
      "iteration: 84400 loss: 0.0015 lr: 0.02\n",
      "iteration: 84500 loss: 0.0017 lr: 0.02\n",
      "iteration: 84600 loss: 0.0018 lr: 0.02\n",
      "iteration: 84700 loss: 0.0019 lr: 0.02\n",
      "iteration: 84800 loss: 0.0019 lr: 0.02\n",
      "iteration: 84900 loss: 0.0019 lr: 0.02\n",
      "iteration: 85000 loss: 0.0017 lr: 0.02\n",
      "iteration: 85100 loss: 0.0019 lr: 0.02\n",
      "iteration: 85200 loss: 0.0018 lr: 0.02\n",
      "iteration: 85300 loss: 0.0017 lr: 0.02\n",
      "iteration: 85400 loss: 0.0018 lr: 0.02\n",
      "iteration: 85500 loss: 0.0019 lr: 0.02\n",
      "iteration: 85600 loss: 0.0018 lr: 0.02\n",
      "iteration: 85700 loss: 0.0018 lr: 0.02\n",
      "iteration: 85800 loss: 0.0019 lr: 0.02\n",
      "iteration: 85900 loss: 0.0018 lr: 0.02\n",
      "iteration: 86000 loss: 0.0019 lr: 0.02\n",
      "iteration: 86100 loss: 0.0016 lr: 0.02\n",
      "iteration: 86200 loss: 0.0019 lr: 0.02\n",
      "iteration: 86300 loss: 0.0018 lr: 0.02\n",
      "iteration: 86400 loss: 0.0018 lr: 0.02\n",
      "iteration: 86500 loss: 0.0019 lr: 0.02\n",
      "iteration: 86600 loss: 0.0019 lr: 0.02\n",
      "iteration: 86700 loss: 0.0019 lr: 0.02\n",
      "iteration: 86800 loss: 0.0019 lr: 0.02\n",
      "iteration: 86900 loss: 0.0018 lr: 0.02\n",
      "iteration: 87000 loss: 0.0019 lr: 0.02\n",
      "iteration: 87100 loss: 0.0019 lr: 0.02\n",
      "iteration: 87200 loss: 0.0020 lr: 0.02\n",
      "iteration: 87300 loss: 0.0019 lr: 0.02\n",
      "iteration: 87400 loss: 0.0019 lr: 0.02\n",
      "iteration: 87500 loss: 0.0019 lr: 0.02\n",
      "iteration: 87600 loss: 0.0017 lr: 0.02\n",
      "iteration: 87700 loss: 0.0017 lr: 0.02\n",
      "iteration: 87800 loss: 0.0018 lr: 0.02\n",
      "iteration: 87900 loss: 0.0018 lr: 0.02\n",
      "iteration: 88000 loss: 0.0018 lr: 0.02\n",
      "iteration: 88100 loss: 0.0018 lr: 0.02\n",
      "iteration: 88200 loss: 0.0019 lr: 0.02\n",
      "iteration: 88300 loss: 0.0019 lr: 0.02\n",
      "iteration: 88400 loss: 0.0018 lr: 0.02\n",
      "iteration: 88500 loss: 0.0018 lr: 0.02\n",
      "iteration: 88600 loss: 0.0019 lr: 0.02\n",
      "iteration: 88700 loss: 0.0016 lr: 0.02\n",
      "iteration: 88800 loss: 0.0018 lr: 0.02\n",
      "iteration: 88900 loss: 0.0018 lr: 0.02\n",
      "iteration: 89000 loss: 0.0017 lr: 0.02\n",
      "iteration: 89100 loss: 0.0017 lr: 0.02\n",
      "iteration: 89200 loss: 0.0019 lr: 0.02\n",
      "iteration: 89300 loss: 0.0017 lr: 0.02\n",
      "iteration: 89400 loss: 0.0020 lr: 0.02\n",
      "iteration: 89500 loss: 0.0017 lr: 0.02\n",
      "iteration: 89600 loss: 0.0020 lr: 0.02\n",
      "iteration: 89700 loss: 0.0018 lr: 0.02\n",
      "iteration: 89800 loss: 0.0016 lr: 0.02\n",
      "iteration: 89900 loss: 0.0019 lr: 0.02\n",
      "iteration: 90000 loss: 0.0018 lr: 0.02\n",
      "iteration: 90100 loss: 0.0019 lr: 0.02\n",
      "iteration: 90200 loss: 0.0018 lr: 0.02\n",
      "iteration: 90300 loss: 0.0018 lr: 0.02\n",
      "iteration: 90400 loss: 0.0018 lr: 0.02\n",
      "iteration: 90500 loss: 0.0018 lr: 0.02\n",
      "iteration: 90600 loss: 0.0019 lr: 0.02\n",
      "iteration: 90700 loss: 0.0019 lr: 0.02\n",
      "iteration: 90800 loss: 0.0018 lr: 0.02\n",
      "iteration: 90900 loss: 0.0017 lr: 0.02\n",
      "iteration: 91000 loss: 0.0017 lr: 0.02\n",
      "iteration: 91100 loss: 0.0018 lr: 0.02\n",
      "iteration: 91200 loss: 0.0017 lr: 0.02\n",
      "iteration: 91300 loss: 0.0018 lr: 0.02\n",
      "iteration: 91400 loss: 0.0019 lr: 0.02\n",
      "iteration: 91500 loss: 0.0017 lr: 0.02\n",
      "iteration: 91600 loss: 0.0019 lr: 0.02\n",
      "iteration: 91700 loss: 0.0018 lr: 0.02\n",
      "iteration: 91800 loss: 0.0018 lr: 0.02\n",
      "iteration: 91900 loss: 0.0017 lr: 0.02\n",
      "iteration: 92000 loss: 0.0019 lr: 0.02\n",
      "iteration: 92100 loss: 0.0018 lr: 0.02\n",
      "iteration: 92200 loss: 0.0018 lr: 0.02\n",
      "iteration: 92300 loss: 0.0017 lr: 0.02\n",
      "iteration: 92400 loss: 0.0019 lr: 0.02\n",
      "iteration: 92500 loss: 0.0018 lr: 0.02\n",
      "iteration: 92600 loss: 0.0018 lr: 0.02\n",
      "iteration: 92700 loss: 0.0018 lr: 0.02\n",
      "iteration: 92800 loss: 0.0017 lr: 0.02\n",
      "iteration: 92900 loss: 0.0017 lr: 0.02\n",
      "iteration: 93000 loss: 0.0018 lr: 0.02\n",
      "iteration: 93100 loss: 0.0016 lr: 0.02\n",
      "iteration: 93200 loss: 0.0018 lr: 0.02\n",
      "iteration: 93300 loss: 0.0019 lr: 0.02\n",
      "iteration: 93400 loss: 0.0016 lr: 0.02\n",
      "iteration: 93500 loss: 0.0018 lr: 0.02\n",
      "iteration: 93600 loss: 0.0016 lr: 0.02\n",
      "iteration: 93700 loss: 0.0016 lr: 0.02\n",
      "iteration: 93800 loss: 0.0020 lr: 0.02\n",
      "iteration: 93900 loss: 0.0019 lr: 0.02\n",
      "iteration: 94000 loss: 0.0017 lr: 0.02\n",
      "iteration: 94100 loss: 0.0018 lr: 0.02\n",
      "iteration: 94200 loss: 0.0020 lr: 0.02\n",
      "iteration: 94300 loss: 0.0018 lr: 0.02\n",
      "iteration: 94400 loss: 0.0018 lr: 0.02\n",
      "iteration: 94500 loss: 0.0017 lr: 0.02\n",
      "iteration: 94600 loss: 0.0018 lr: 0.02\n",
      "iteration: 94700 loss: 0.0017 lr: 0.02\n",
      "iteration: 94800 loss: 0.0019 lr: 0.02\n",
      "iteration: 94900 loss: 0.0017 lr: 0.02\n",
      "iteration: 95000 loss: 0.0016 lr: 0.02\n",
      "iteration: 95100 loss: 0.0017 lr: 0.02\n",
      "iteration: 95200 loss: 0.0017 lr: 0.02\n",
      "iteration: 95300 loss: 0.0018 lr: 0.02\n",
      "iteration: 95400 loss: 0.0019 lr: 0.02\n",
      "iteration: 95500 loss: 0.0017 lr: 0.02\n",
      "iteration: 95600 loss: 0.0016 lr: 0.02\n",
      "iteration: 95700 loss: 0.0017 lr: 0.02\n",
      "iteration: 95800 loss: 0.0018 lr: 0.02\n",
      "iteration: 95900 loss: 0.0016 lr: 0.02\n",
      "iteration: 96000 loss: 0.0017 lr: 0.02\n",
      "iteration: 96100 loss: 0.0018 lr: 0.02\n",
      "iteration: 96200 loss: 0.0018 lr: 0.02\n",
      "iteration: 96300 loss: 0.0018 lr: 0.02\n",
      "iteration: 96400 loss: 0.0016 lr: 0.02\n",
      "iteration: 96500 loss: 0.0017 lr: 0.02\n",
      "iteration: 96600 loss: 0.0017 lr: 0.02\n",
      "iteration: 96700 loss: 0.0020 lr: 0.02\n",
      "iteration: 96800 loss: 0.0018 lr: 0.02\n",
      "iteration: 96900 loss: 0.0017 lr: 0.02\n",
      "iteration: 97000 loss: 0.0018 lr: 0.02\n",
      "iteration: 97100 loss: 0.0018 lr: 0.02\n",
      "iteration: 97200 loss: 0.0017 lr: 0.02\n",
      "iteration: 97300 loss: 0.0017 lr: 0.02\n",
      "iteration: 97400 loss: 0.0016 lr: 0.02\n",
      "iteration: 97500 loss: 0.0016 lr: 0.02\n",
      "iteration: 97600 loss: 0.0014 lr: 0.02\n",
      "iteration: 97700 loss: 0.0017 lr: 0.02\n",
      "iteration: 97800 loss: 0.0017 lr: 0.02\n",
      "iteration: 97900 loss: 0.0018 lr: 0.02\n",
      "iteration: 98000 loss: 0.0019 lr: 0.02\n",
      "iteration: 98100 loss: 0.0018 lr: 0.02\n",
      "iteration: 98200 loss: 0.0017 lr: 0.02\n",
      "iteration: 98300 loss: 0.0018 lr: 0.02\n",
      "iteration: 98400 loss: 0.0017 lr: 0.02\n",
      "iteration: 98500 loss: 0.0017 lr: 0.02\n",
      "iteration: 98600 loss: 0.0017 lr: 0.02\n",
      "iteration: 98700 loss: 0.0018 lr: 0.02\n",
      "iteration: 98800 loss: 0.0018 lr: 0.02\n",
      "iteration: 98900 loss: 0.0016 lr: 0.02\n",
      "iteration: 99000 loss: 0.0016 lr: 0.02\n",
      "iteration: 99100 loss: 0.0017 lr: 0.02\n",
      "iteration: 99200 loss: 0.0018 lr: 0.02\n",
      "iteration: 99300 loss: 0.0016 lr: 0.02\n",
      "iteration: 99400 loss: 0.0017 lr: 0.02\n",
      "iteration: 99500 loss: 0.0019 lr: 0.02\n",
      "iteration: 99600 loss: 0.0018 lr: 0.02\n",
      "iteration: 99700 loss: 0.0018 lr: 0.02\n",
      "iteration: 99800 loss: 0.0019 lr: 0.02\n",
      "iteration: 99900 loss: 0.0018 lr: 0.02\n",
      "iteration: 100000 loss: 0.0016 lr: 0.02\n",
      "iteration: 100100 loss: 0.0017 lr: 0.02\n",
      "iteration: 100200 loss: 0.0018 lr: 0.02\n",
      "iteration: 100300 loss: 0.0017 lr: 0.02\n",
      "iteration: 100400 loss: 0.0016 lr: 0.02\n",
      "iteration: 100500 loss: 0.0016 lr: 0.02\n",
      "iteration: 100600 loss: 0.0017 lr: 0.02\n",
      "iteration: 100700 loss: 0.0017 lr: 0.02\n",
      "iteration: 100800 loss: 0.0016 lr: 0.02\n",
      "iteration: 100900 loss: 0.0016 lr: 0.02\n",
      "iteration: 101000 loss: 0.0019 lr: 0.02\n",
      "iteration: 101100 loss: 0.0016 lr: 0.02\n",
      "iteration: 101200 loss: 0.0016 lr: 0.02\n",
      "iteration: 101300 loss: 0.0017 lr: 0.02\n",
      "iteration: 101400 loss: 0.0017 lr: 0.02\n",
      "iteration: 101500 loss: 0.0016 lr: 0.02\n",
      "iteration: 101600 loss: 0.0016 lr: 0.02\n",
      "iteration: 101700 loss: 0.0017 lr: 0.02\n",
      "iteration: 101800 loss: 0.0017 lr: 0.02\n",
      "iteration: 101900 loss: 0.0017 lr: 0.02\n",
      "iteration: 102000 loss: 0.0016 lr: 0.02\n",
      "iteration: 102100 loss: 0.0017 lr: 0.02\n",
      "iteration: 102200 loss: 0.0017 lr: 0.02\n",
      "iteration: 102300 loss: 0.0017 lr: 0.02\n",
      "iteration: 102400 loss: 0.0017 lr: 0.02\n",
      "iteration: 102500 loss: 0.0016 lr: 0.02\n",
      "iteration: 102600 loss: 0.0016 lr: 0.02\n",
      "iteration: 102700 loss: 0.0017 lr: 0.02\n",
      "iteration: 102800 loss: 0.0016 lr: 0.02\n",
      "iteration: 102900 loss: 0.0016 lr: 0.02\n",
      "iteration: 103000 loss: 0.0016 lr: 0.02\n",
      "iteration: 103100 loss: 0.0017 lr: 0.02\n",
      "iteration: 103200 loss: 0.0016 lr: 0.02\n",
      "iteration: 103300 loss: 0.0017 lr: 0.02\n",
      "iteration: 103400 loss: 0.0017 lr: 0.02\n",
      "iteration: 103500 loss: 0.0015 lr: 0.02\n",
      "iteration: 103600 loss: 0.0016 lr: 0.02\n",
      "iteration: 103700 loss: 0.0016 lr: 0.02\n",
      "iteration: 103800 loss: 0.0016 lr: 0.02\n",
      "iteration: 103900 loss: 0.0017 lr: 0.02\n",
      "iteration: 104000 loss: 0.0017 lr: 0.02\n",
      "iteration: 104100 loss: 0.0017 lr: 0.02\n",
      "iteration: 104200 loss: 0.0017 lr: 0.02\n",
      "iteration: 104300 loss: 0.0016 lr: 0.02\n",
      "iteration: 104400 loss: 0.0016 lr: 0.02\n",
      "iteration: 104500 loss: 0.0015 lr: 0.02\n",
      "iteration: 104600 loss: 0.0018 lr: 0.02\n",
      "iteration: 104700 loss: 0.0017 lr: 0.02\n",
      "iteration: 104800 loss: 0.0015 lr: 0.02\n",
      "iteration: 104900 loss: 0.0017 lr: 0.02\n",
      "iteration: 105000 loss: 0.0017 lr: 0.02\n",
      "iteration: 105100 loss: 0.0018 lr: 0.02\n",
      "iteration: 105200 loss: 0.0017 lr: 0.02\n",
      "iteration: 105300 loss: 0.0018 lr: 0.02\n",
      "iteration: 105400 loss: 0.0017 lr: 0.02\n",
      "iteration: 105500 loss: 0.0017 lr: 0.02\n",
      "iteration: 105600 loss: 0.0018 lr: 0.02\n",
      "iteration: 105700 loss: 0.0016 lr: 0.02\n",
      "iteration: 105800 loss: 0.0017 lr: 0.02\n",
      "iteration: 105900 loss: 0.0018 lr: 0.02\n",
      "iteration: 106000 loss: 0.0018 lr: 0.02\n",
      "iteration: 106100 loss: 0.0017 lr: 0.02\n",
      "iteration: 106200 loss: 0.0018 lr: 0.02\n",
      "iteration: 106300 loss: 0.0019 lr: 0.02\n",
      "iteration: 106400 loss: 0.0016 lr: 0.02\n",
      "iteration: 106500 loss: 0.0017 lr: 0.02\n",
      "iteration: 106600 loss: 0.0016 lr: 0.02\n",
      "iteration: 106700 loss: 0.0016 lr: 0.02\n",
      "iteration: 106800 loss: 0.0017 lr: 0.02\n",
      "iteration: 106900 loss: 0.0016 lr: 0.02\n",
      "iteration: 107000 loss: 0.0017 lr: 0.02\n",
      "iteration: 107100 loss: 0.0018 lr: 0.02\n",
      "iteration: 107200 loss: 0.0017 lr: 0.02\n",
      "iteration: 107300 loss: 0.0018 lr: 0.02\n",
      "iteration: 107400 loss: 0.0016 lr: 0.02\n",
      "iteration: 107500 loss: 0.0016 lr: 0.02\n",
      "iteration: 107600 loss: 0.0018 lr: 0.02\n",
      "iteration: 107700 loss: 0.0016 lr: 0.02\n",
      "iteration: 107800 loss: 0.0017 lr: 0.02\n",
      "iteration: 107900 loss: 0.0018 lr: 0.02\n",
      "iteration: 108000 loss: 0.0017 lr: 0.02\n",
      "iteration: 108100 loss: 0.0016 lr: 0.02\n",
      "iteration: 108200 loss: 0.0013 lr: 0.02\n",
      "iteration: 108300 loss: 0.0015 lr: 0.02\n",
      "iteration: 108400 loss: 0.0018 lr: 0.02\n",
      "iteration: 108500 loss: 0.0017 lr: 0.02\n",
      "iteration: 108600 loss: 0.0018 lr: 0.02\n",
      "iteration: 108700 loss: 0.0015 lr: 0.02\n",
      "iteration: 108800 loss: 0.0017 lr: 0.02\n",
      "iteration: 108900 loss: 0.0017 lr: 0.02\n",
      "iteration: 109000 loss: 0.0016 lr: 0.02\n",
      "iteration: 109100 loss: 0.0017 lr: 0.02\n",
      "iteration: 109200 loss: 0.0017 lr: 0.02\n",
      "iteration: 109300 loss: 0.0016 lr: 0.02\n",
      "iteration: 109400 loss: 0.0015 lr: 0.02\n",
      "iteration: 109500 loss: 0.0016 lr: 0.02\n",
      "iteration: 109600 loss: 0.0015 lr: 0.02\n",
      "iteration: 109700 loss: 0.0016 lr: 0.02\n",
      "iteration: 109800 loss: 0.0017 lr: 0.02\n",
      "iteration: 109900 loss: 0.0018 lr: 0.02\n",
      "iteration: 110000 loss: 0.0017 lr: 0.02\n",
      "iteration: 110100 loss: 0.0017 lr: 0.02\n",
      "iteration: 110200 loss: 0.0016 lr: 0.02\n",
      "iteration: 110300 loss: 0.0017 lr: 0.02\n",
      "iteration: 110400 loss: 0.0017 lr: 0.02\n",
      "iteration: 110500 loss: 0.0016 lr: 0.02\n",
      "iteration: 110600 loss: 0.0017 lr: 0.02\n",
      "iteration: 110700 loss: 0.0015 lr: 0.02\n",
      "iteration: 110800 loss: 0.0016 lr: 0.02\n",
      "iteration: 110900 loss: 0.0018 lr: 0.02\n",
      "iteration: 111000 loss: 0.0016 lr: 0.02\n",
      "iteration: 111100 loss: 0.0017 lr: 0.02\n",
      "iteration: 111200 loss: 0.0017 lr: 0.02\n",
      "iteration: 111300 loss: 0.0017 lr: 0.02\n",
      "iteration: 111400 loss: 0.0017 lr: 0.02\n",
      "iteration: 111500 loss: 0.0018 lr: 0.02\n",
      "iteration: 111600 loss: 0.0017 lr: 0.02\n",
      "iteration: 111700 loss: 0.0016 lr: 0.02\n",
      "iteration: 111800 loss: 0.0016 lr: 0.02\n",
      "iteration: 111900 loss: 0.0017 lr: 0.02\n",
      "iteration: 112000 loss: 0.0018 lr: 0.02\n",
      "iteration: 112100 loss: 0.0016 lr: 0.02\n",
      "iteration: 112200 loss: 0.0016 lr: 0.02\n",
      "iteration: 112300 loss: 0.0015 lr: 0.02\n",
      "iteration: 112400 loss: 0.0017 lr: 0.02\n",
      "iteration: 112500 loss: 0.0016 lr: 0.02\n",
      "iteration: 112600 loss: 0.0017 lr: 0.02\n",
      "iteration: 112700 loss: 0.0015 lr: 0.02\n",
      "iteration: 112800 loss: 0.0018 lr: 0.02\n",
      "iteration: 112900 loss: 0.0017 lr: 0.02\n",
      "iteration: 113000 loss: 0.0017 lr: 0.02\n",
      "iteration: 113100 loss: 0.0015 lr: 0.02\n",
      "iteration: 113200 loss: 0.0017 lr: 0.02\n",
      "iteration: 113300 loss: 0.0016 lr: 0.02\n",
      "iteration: 113400 loss: 0.0016 lr: 0.02\n",
      "iteration: 113500 loss: 0.0016 lr: 0.02\n",
      "iteration: 113600 loss: 0.0015 lr: 0.02\n",
      "iteration: 113700 loss: 0.0016 lr: 0.02\n",
      "iteration: 113800 loss: 0.0017 lr: 0.02\n",
      "iteration: 113900 loss: 0.0018 lr: 0.02\n",
      "iteration: 114000 loss: 0.0015 lr: 0.02\n",
      "iteration: 114100 loss: 0.0017 lr: 0.02\n",
      "iteration: 114200 loss: 0.0017 lr: 0.02\n",
      "iteration: 114300 loss: 0.0016 lr: 0.02\n",
      "iteration: 114400 loss: 0.0016 lr: 0.02\n",
      "iteration: 114500 loss: 0.0013 lr: 0.02\n",
      "iteration: 114600 loss: 0.0017 lr: 0.02\n",
      "iteration: 114700 loss: 0.0016 lr: 0.02\n",
      "iteration: 114800 loss: 0.0016 lr: 0.02\n",
      "iteration: 114900 loss: 0.0015 lr: 0.02\n",
      "iteration: 115000 loss: 0.0016 lr: 0.02\n",
      "iteration: 115100 loss: 0.0014 lr: 0.02\n",
      "iteration: 115200 loss: 0.0017 lr: 0.02\n",
      "iteration: 115300 loss: 0.0015 lr: 0.02\n",
      "iteration: 115400 loss: 0.0016 lr: 0.02\n",
      "iteration: 115500 loss: 0.0016 lr: 0.02\n",
      "iteration: 115600 loss: 0.0016 lr: 0.02\n",
      "iteration: 115700 loss: 0.0015 lr: 0.02\n",
      "iteration: 115800 loss: 0.0015 lr: 0.02\n",
      "iteration: 115900 loss: 0.0014 lr: 0.02\n",
      "iteration: 116000 loss: 0.0017 lr: 0.02\n",
      "iteration: 116100 loss: 0.0014 lr: 0.02\n",
      "iteration: 116200 loss: 0.0017 lr: 0.02\n",
      "iteration: 116300 loss: 0.0017 lr: 0.02\n",
      "iteration: 116400 loss: 0.0016 lr: 0.02\n",
      "iteration: 116500 loss: 0.0016 lr: 0.02\n",
      "iteration: 116600 loss: 0.0016 lr: 0.02\n",
      "iteration: 116700 loss: 0.0019 lr: 0.02\n",
      "iteration: 116800 loss: 0.0015 lr: 0.02\n",
      "iteration: 116900 loss: 0.0015 lr: 0.02\n",
      "iteration: 117000 loss: 0.0016 lr: 0.02\n",
      "iteration: 117100 loss: 0.0016 lr: 0.02\n",
      "iteration: 117200 loss: 0.0018 lr: 0.02\n",
      "iteration: 117300 loss: 0.0015 lr: 0.02\n",
      "iteration: 117400 loss: 0.0017 lr: 0.02\n",
      "iteration: 117500 loss: 0.0015 lr: 0.02\n",
      "iteration: 117600 loss: 0.0015 lr: 0.02\n",
      "iteration: 117700 loss: 0.0016 lr: 0.02\n",
      "iteration: 117800 loss: 0.0015 lr: 0.02\n",
      "iteration: 117900 loss: 0.0017 lr: 0.02\n",
      "iteration: 118000 loss: 0.0017 lr: 0.02\n",
      "iteration: 118100 loss: 0.0015 lr: 0.02\n",
      "iteration: 118200 loss: 0.0016 lr: 0.02\n",
      "iteration: 118300 loss: 0.0018 lr: 0.02\n",
      "iteration: 118400 loss: 0.0015 lr: 0.02\n",
      "iteration: 118500 loss: 0.0017 lr: 0.02\n",
      "iteration: 118600 loss: 0.0015 lr: 0.02\n",
      "iteration: 118700 loss: 0.0016 lr: 0.02\n",
      "iteration: 118800 loss: 0.0015 lr: 0.02\n",
      "iteration: 118900 loss: 0.0016 lr: 0.02\n",
      "iteration: 119000 loss: 0.0018 lr: 0.02\n",
      "iteration: 119100 loss: 0.0015 lr: 0.02\n",
      "iteration: 119200 loss: 0.0015 lr: 0.02\n",
      "iteration: 119300 loss: 0.0017 lr: 0.02\n",
      "iteration: 119400 loss: 0.0016 lr: 0.02\n",
      "iteration: 119500 loss: 0.0016 lr: 0.02\n",
      "iteration: 119600 loss: 0.0016 lr: 0.02\n",
      "iteration: 119700 loss: 0.0014 lr: 0.02\n",
      "iteration: 119800 loss: 0.0016 lr: 0.02\n",
      "iteration: 119900 loss: 0.0017 lr: 0.02\n",
      "iteration: 120000 loss: 0.0016 lr: 0.02\n",
      "iteration: 120100 loss: 0.0017 lr: 0.02\n",
      "iteration: 120200 loss: 0.0015 lr: 0.02\n",
      "iteration: 120300 loss: 0.0017 lr: 0.02\n",
      "iteration: 120400 loss: 0.0016 lr: 0.02\n",
      "iteration: 120500 loss: 0.0016 lr: 0.02\n",
      "iteration: 120600 loss: 0.0016 lr: 0.02\n",
      "iteration: 120700 loss: 0.0015 lr: 0.02\n",
      "iteration: 120800 loss: 0.0016 lr: 0.02\n",
      "iteration: 120900 loss: 0.0018 lr: 0.02\n",
      "iteration: 121000 loss: 0.0017 lr: 0.02\n",
      "iteration: 121100 loss: 0.0015 lr: 0.02\n",
      "iteration: 121200 loss: 0.0015 lr: 0.02\n",
      "iteration: 121300 loss: 0.0016 lr: 0.02\n",
      "iteration: 121400 loss: 0.0018 lr: 0.02\n",
      "iteration: 121500 loss: 0.0014 lr: 0.02\n",
      "iteration: 121600 loss: 0.0016 lr: 0.02\n",
      "iteration: 121700 loss: 0.0016 lr: 0.02\n",
      "iteration: 121800 loss: 0.0016 lr: 0.02\n",
      "iteration: 121900 loss: 0.0016 lr: 0.02\n",
      "iteration: 122000 loss: 0.0016 lr: 0.02\n",
      "iteration: 122100 loss: 0.0015 lr: 0.02\n",
      "iteration: 122200 loss: 0.0015 lr: 0.02\n",
      "iteration: 122300 loss: 0.0016 lr: 0.02\n",
      "iteration: 122400 loss: 0.0015 lr: 0.02\n",
      "iteration: 122500 loss: 0.0015 lr: 0.02\n",
      "iteration: 122600 loss: 0.0016 lr: 0.02\n",
      "iteration: 122700 loss: 0.0016 lr: 0.02\n",
      "iteration: 122800 loss: 0.0016 lr: 0.02\n",
      "iteration: 122900 loss: 0.0016 lr: 0.02\n",
      "iteration: 123000 loss: 0.0015 lr: 0.02\n",
      "iteration: 123100 loss: 0.0014 lr: 0.02\n",
      "iteration: 123200 loss: 0.0016 lr: 0.02\n",
      "iteration: 123300 loss: 0.0017 lr: 0.02\n",
      "iteration: 123400 loss: 0.0017 lr: 0.02\n",
      "iteration: 123500 loss: 0.0015 lr: 0.02\n",
      "iteration: 123600 loss: 0.0014 lr: 0.02\n",
      "iteration: 123700 loss: 0.0015 lr: 0.02\n",
      "iteration: 123800 loss: 0.0016 lr: 0.02\n",
      "iteration: 123900 loss: 0.0015 lr: 0.02\n",
      "iteration: 124000 loss: 0.0016 lr: 0.02\n",
      "iteration: 124100 loss: 0.0018 lr: 0.02\n",
      "iteration: 124200 loss: 0.0016 lr: 0.02\n",
      "iteration: 124300 loss: 0.0017 lr: 0.02\n",
      "iteration: 124400 loss: 0.0016 lr: 0.02\n",
      "iteration: 124500 loss: 0.0017 lr: 0.02\n",
      "iteration: 124600 loss: 0.0015 lr: 0.02\n",
      "iteration: 124700 loss: 0.0016 lr: 0.02\n",
      "iteration: 124800 loss: 0.0017 lr: 0.02\n",
      "iteration: 124900 loss: 0.0015 lr: 0.02\n",
      "iteration: 125000 loss: 0.0015 lr: 0.02\n",
      "iteration: 125100 loss: 0.0015 lr: 0.02\n",
      "iteration: 125200 loss: 0.0014 lr: 0.02\n",
      "iteration: 125300 loss: 0.0015 lr: 0.02\n",
      "iteration: 125400 loss: 0.0017 lr: 0.02\n",
      "iteration: 125500 loss: 0.0015 lr: 0.02\n",
      "iteration: 125600 loss: 0.0016 lr: 0.02\n",
      "iteration: 125700 loss: 0.0015 lr: 0.02\n",
      "iteration: 125800 loss: 0.0016 lr: 0.02\n",
      "iteration: 125900 loss: 0.0017 lr: 0.02\n",
      "iteration: 126000 loss: 0.0016 lr: 0.02\n",
      "iteration: 126100 loss: 0.0015 lr: 0.02\n",
      "iteration: 126200 loss: 0.0015 lr: 0.02\n",
      "iteration: 126300 loss: 0.0015 lr: 0.02\n",
      "iteration: 126400 loss: 0.0016 lr: 0.02\n",
      "iteration: 126500 loss: 0.0015 lr: 0.02\n",
      "iteration: 126600 loss: 0.0015 lr: 0.02\n",
      "iteration: 126700 loss: 0.0015 lr: 0.02\n",
      "iteration: 126800 loss: 0.0015 lr: 0.02\n",
      "iteration: 126900 loss: 0.0017 lr: 0.02\n",
      "iteration: 127000 loss: 0.0016 lr: 0.02\n",
      "iteration: 127100 loss: 0.0015 lr: 0.02\n",
      "iteration: 127200 loss: 0.0017 lr: 0.02\n",
      "iteration: 127300 loss: 0.0017 lr: 0.02\n",
      "iteration: 127400 loss: 0.0017 lr: 0.02\n",
      "iteration: 127500 loss: 0.0015 lr: 0.02\n",
      "iteration: 127600 loss: 0.0015 lr: 0.02\n",
      "iteration: 127700 loss: 0.0015 lr: 0.02\n",
      "iteration: 127800 loss: 0.0016 lr: 0.02\n",
      "iteration: 127900 loss: 0.0015 lr: 0.02\n",
      "iteration: 128000 loss: 0.0015 lr: 0.02\n",
      "iteration: 128100 loss: 0.0015 lr: 0.02\n",
      "iteration: 128200 loss: 0.0015 lr: 0.02\n",
      "iteration: 128300 loss: 0.0018 lr: 0.02\n",
      "iteration: 128400 loss: 0.0017 lr: 0.02\n",
      "iteration: 128500 loss: 0.0016 lr: 0.02\n",
      "iteration: 128600 loss: 0.0015 lr: 0.02\n",
      "iteration: 128700 loss: 0.0018 lr: 0.02\n",
      "iteration: 128800 loss: 0.0016 lr: 0.02\n",
      "iteration: 128900 loss: 0.0014 lr: 0.02\n",
      "iteration: 129000 loss: 0.0014 lr: 0.02\n",
      "iteration: 129100 loss: 0.0015 lr: 0.02\n",
      "iteration: 129200 loss: 0.0015 lr: 0.02\n",
      "iteration: 129300 loss: 0.0016 lr: 0.02\n",
      "iteration: 129400 loss: 0.0014 lr: 0.02\n",
      "iteration: 129500 loss: 0.0015 lr: 0.02\n",
      "iteration: 129600 loss: 0.0015 lr: 0.02\n",
      "iteration: 129700 loss: 0.0015 lr: 0.02\n",
      "iteration: 129800 loss: 0.0015 lr: 0.02\n",
      "iteration: 129900 loss: 0.0015 lr: 0.02\n",
      "iteration: 130000 loss: 0.0016 lr: 0.02\n",
      "iteration: 130100 loss: 0.0014 lr: 0.02\n",
      "iteration: 130200 loss: 0.0015 lr: 0.02\n",
      "iteration: 130300 loss: 0.0017 lr: 0.02\n",
      "iteration: 130400 loss: 0.0016 lr: 0.02\n",
      "iteration: 130500 loss: 0.0015 lr: 0.02\n",
      "iteration: 130600 loss: 0.0015 lr: 0.02\n",
      "iteration: 130700 loss: 0.0015 lr: 0.02\n",
      "iteration: 130800 loss: 0.0014 lr: 0.02\n",
      "iteration: 130900 loss: 0.0015 lr: 0.02\n",
      "iteration: 131000 loss: 0.0015 lr: 0.02\n",
      "iteration: 131100 loss: 0.0015 lr: 0.02\n",
      "iteration: 131200 loss: 0.0015 lr: 0.02\n",
      "iteration: 131300 loss: 0.0015 lr: 0.02\n",
      "iteration: 131400 loss: 0.0016 lr: 0.02\n",
      "iteration: 131500 loss: 0.0017 lr: 0.02\n",
      "iteration: 131600 loss: 0.0016 lr: 0.02\n",
      "iteration: 131700 loss: 0.0014 lr: 0.02\n",
      "iteration: 131800 loss: 0.0015 lr: 0.02\n",
      "iteration: 131900 loss: 0.0013 lr: 0.02\n",
      "iteration: 132000 loss: 0.0016 lr: 0.02\n",
      "iteration: 132100 loss: 0.0015 lr: 0.02\n",
      "iteration: 132200 loss: 0.0016 lr: 0.02\n",
      "iteration: 132300 loss: 0.0017 lr: 0.02\n",
      "iteration: 132400 loss: 0.0015 lr: 0.02\n",
      "iteration: 132500 loss: 0.0015 lr: 0.02\n",
      "iteration: 132600 loss: 0.0014 lr: 0.02\n",
      "iteration: 132700 loss: 0.0014 lr: 0.02\n",
      "iteration: 132800 loss: 0.0015 lr: 0.02\n",
      "iteration: 132900 loss: 0.0016 lr: 0.02\n",
      "iteration: 133000 loss: 0.0015 lr: 0.02\n",
      "iteration: 133100 loss: 0.0015 lr: 0.02\n",
      "iteration: 133200 loss: 0.0015 lr: 0.02\n",
      "iteration: 133300 loss: 0.0016 lr: 0.02\n",
      "iteration: 133400 loss: 0.0015 lr: 0.02\n",
      "iteration: 133500 loss: 0.0015 lr: 0.02\n",
      "iteration: 133600 loss: 0.0015 lr: 0.02\n",
      "iteration: 133700 loss: 0.0016 lr: 0.02\n",
      "iteration: 133800 loss: 0.0016 lr: 0.02\n",
      "iteration: 133900 loss: 0.0016 lr: 0.02\n",
      "iteration: 134000 loss: 0.0015 lr: 0.02\n",
      "iteration: 134100 loss: 0.0016 lr: 0.02\n",
      "iteration: 134200 loss: 0.0016 lr: 0.02\n",
      "iteration: 134300 loss: 0.0016 lr: 0.02\n",
      "iteration: 134400 loss: 0.0017 lr: 0.02\n",
      "iteration: 134500 loss: 0.0014 lr: 0.02\n",
      "iteration: 134600 loss: 0.0014 lr: 0.02\n",
      "iteration: 134700 loss: 0.0015 lr: 0.02\n",
      "iteration: 134800 loss: 0.0015 lr: 0.02\n",
      "iteration: 134900 loss: 0.0015 lr: 0.02\n",
      "iteration: 135000 loss: 0.0016 lr: 0.02\n",
      "iteration: 135100 loss: 0.0015 lr: 0.02\n",
      "iteration: 135200 loss: 0.0016 lr: 0.02\n",
      "iteration: 135300 loss: 0.0014 lr: 0.02\n",
      "iteration: 135400 loss: 0.0014 lr: 0.02\n",
      "iteration: 135500 loss: 0.0016 lr: 0.02\n",
      "iteration: 135600 loss: 0.0015 lr: 0.02\n",
      "iteration: 135700 loss: 0.0015 lr: 0.02\n",
      "iteration: 135800 loss: 0.0016 lr: 0.02\n",
      "iteration: 135900 loss: 0.0015 lr: 0.02\n",
      "iteration: 136000 loss: 0.0017 lr: 0.02\n",
      "iteration: 136100 loss: 0.0015 lr: 0.02\n",
      "iteration: 136200 loss: 0.0015 lr: 0.02\n",
      "iteration: 136300 loss: 0.0014 lr: 0.02\n",
      "iteration: 136400 loss: 0.0015 lr: 0.02\n",
      "iteration: 136500 loss: 0.0016 lr: 0.02\n",
      "iteration: 136600 loss: 0.0016 lr: 0.02\n",
      "iteration: 136700 loss: 0.0016 lr: 0.02\n",
      "iteration: 136800 loss: 0.0014 lr: 0.02\n",
      "iteration: 136900 loss: 0.0014 lr: 0.02\n",
      "iteration: 137000 loss: 0.0016 lr: 0.02\n",
      "iteration: 137100 loss: 0.0014 lr: 0.02\n",
      "iteration: 137200 loss: 0.0014 lr: 0.02\n",
      "iteration: 137300 loss: 0.0015 lr: 0.02\n",
      "iteration: 137400 loss: 0.0016 lr: 0.02\n",
      "iteration: 137500 loss: 0.0014 lr: 0.02\n",
      "iteration: 137600 loss: 0.0015 lr: 0.02\n",
      "iteration: 137700 loss: 0.0014 lr: 0.02\n",
      "iteration: 137800 loss: 0.0016 lr: 0.02\n",
      "iteration: 137900 loss: 0.0014 lr: 0.02\n",
      "iteration: 138000 loss: 0.0014 lr: 0.02\n",
      "iteration: 138100 loss: 0.0015 lr: 0.02\n",
      "iteration: 138200 loss: 0.0015 lr: 0.02\n",
      "iteration: 138300 loss: 0.0015 lr: 0.02\n",
      "iteration: 138400 loss: 0.0014 lr: 0.02\n",
      "iteration: 138500 loss: 0.0014 lr: 0.02\n",
      "iteration: 138600 loss: 0.0015 lr: 0.02\n",
      "iteration: 138700 loss: 0.0016 lr: 0.02\n",
      "iteration: 138800 loss: 0.0014 lr: 0.02\n",
      "iteration: 138900 loss: 0.0016 lr: 0.02\n",
      "iteration: 139000 loss: 0.0015 lr: 0.02\n",
      "iteration: 139100 loss: 0.0016 lr: 0.02\n",
      "iteration: 139200 loss: 0.0015 lr: 0.02\n",
      "iteration: 139300 loss: 0.0014 lr: 0.02\n",
      "iteration: 139400 loss: 0.0016 lr: 0.02\n",
      "iteration: 139500 loss: 0.0015 lr: 0.02\n",
      "iteration: 139600 loss: 0.0015 lr: 0.02\n",
      "iteration: 139700 loss: 0.0015 lr: 0.02\n",
      "iteration: 139800 loss: 0.0015 lr: 0.02\n",
      "iteration: 139900 loss: 0.0015 lr: 0.02\n",
      "iteration: 140000 loss: 0.0016 lr: 0.02\n",
      "iteration: 140100 loss: 0.0014 lr: 0.02\n",
      "iteration: 140200 loss: 0.0015 lr: 0.02\n",
      "iteration: 140300 loss: 0.0015 lr: 0.02\n",
      "iteration: 140400 loss: 0.0015 lr: 0.02\n",
      "iteration: 140500 loss: 0.0014 lr: 0.02\n",
      "iteration: 140600 loss: 0.0014 lr: 0.02\n",
      "iteration: 140700 loss: 0.0014 lr: 0.02\n",
      "iteration: 140800 loss: 0.0016 lr: 0.02\n",
      "iteration: 140900 loss: 0.0015 lr: 0.02\n",
      "iteration: 141000 loss: 0.0015 lr: 0.02\n",
      "iteration: 141100 loss: 0.0016 lr: 0.02\n",
      "iteration: 141200 loss: 0.0015 lr: 0.02\n",
      "iteration: 141300 loss: 0.0012 lr: 0.02\n",
      "iteration: 141400 loss: 0.0015 lr: 0.02\n",
      "iteration: 141500 loss: 0.0014 lr: 0.02\n",
      "iteration: 141600 loss: 0.0016 lr: 0.02\n",
      "iteration: 141700 loss: 0.0015 lr: 0.02\n",
      "iteration: 141800 loss: 0.0016 lr: 0.02\n",
      "iteration: 141900 loss: 0.0015 lr: 0.02\n",
      "iteration: 142000 loss: 0.0014 lr: 0.02\n",
      "iteration: 142100 loss: 0.0014 lr: 0.02\n",
      "iteration: 142200 loss: 0.0015 lr: 0.02\n",
      "iteration: 142300 loss: 0.0015 lr: 0.02\n",
      "iteration: 142400 loss: 0.0015 lr: 0.02\n",
      "iteration: 142500 loss: 0.0014 lr: 0.02\n",
      "iteration: 142600 loss: 0.0016 lr: 0.02\n",
      "iteration: 142700 loss: 0.0015 lr: 0.02\n",
      "iteration: 142800 loss: 0.0016 lr: 0.02\n",
      "iteration: 142900 loss: 0.0014 lr: 0.02\n",
      "iteration: 143000 loss: 0.0014 lr: 0.02\n",
      "iteration: 143100 loss: 0.0016 lr: 0.02\n",
      "iteration: 143200 loss: 0.0015 lr: 0.02\n",
      "iteration: 143300 loss: 0.0015 lr: 0.02\n",
      "iteration: 143400 loss: 0.0014 lr: 0.02\n",
      "iteration: 143500 loss: 0.0015 lr: 0.02\n",
      "iteration: 143600 loss: 0.0015 lr: 0.02\n",
      "iteration: 143700 loss: 0.0014 lr: 0.02\n",
      "iteration: 143800 loss: 0.0014 lr: 0.02\n",
      "iteration: 143900 loss: 0.0014 lr: 0.02\n",
      "iteration: 144000 loss: 0.0015 lr: 0.02\n",
      "iteration: 144100 loss: 0.0016 lr: 0.02\n",
      "iteration: 144200 loss: 0.0015 lr: 0.02\n",
      "iteration: 144300 loss: 0.0015 lr: 0.02\n",
      "iteration: 144400 loss: 0.0016 lr: 0.02\n",
      "iteration: 144500 loss: 0.0015 lr: 0.02\n",
      "iteration: 144600 loss: 0.0017 lr: 0.02\n",
      "iteration: 144700 loss: 0.0015 lr: 0.02\n",
      "iteration: 144800 loss: 0.0016 lr: 0.02\n",
      "iteration: 144900 loss: 0.0014 lr: 0.02\n",
      "iteration: 145000 loss: 0.0015 lr: 0.02\n",
      "iteration: 145100 loss: 0.0015 lr: 0.02\n",
      "iteration: 145200 loss: 0.0013 lr: 0.02\n",
      "iteration: 145300 loss: 0.0015 lr: 0.02\n",
      "iteration: 145400 loss: 0.0015 lr: 0.02\n",
      "iteration: 145500 loss: 0.0014 lr: 0.02\n",
      "iteration: 145600 loss: 0.0014 lr: 0.02\n",
      "iteration: 145700 loss: 0.0014 lr: 0.02\n",
      "iteration: 145800 loss: 0.0015 lr: 0.02\n",
      "iteration: 145900 loss: 0.0014 lr: 0.02\n",
      "iteration: 146000 loss: 0.0015 lr: 0.02\n",
      "iteration: 146100 loss: 0.0014 lr: 0.02\n",
      "iteration: 146200 loss: 0.0014 lr: 0.02\n",
      "iteration: 146300 loss: 0.0015 lr: 0.02\n",
      "iteration: 146400 loss: 0.0014 lr: 0.02\n",
      "iteration: 146500 loss: 0.0015 lr: 0.02\n",
      "iteration: 146600 loss: 0.0015 lr: 0.02\n",
      "iteration: 146700 loss: 0.0016 lr: 0.02\n",
      "iteration: 146800 loss: 0.0014 lr: 0.02\n",
      "iteration: 146900 loss: 0.0014 lr: 0.02\n",
      "iteration: 147000 loss: 0.0015 lr: 0.02\n",
      "iteration: 147100 loss: 0.0015 lr: 0.02\n",
      "iteration: 147200 loss: 0.0014 lr: 0.02\n",
      "iteration: 147300 loss: 0.0014 lr: 0.02\n",
      "iteration: 147400 loss: 0.0015 lr: 0.02\n",
      "iteration: 147500 loss: 0.0015 lr: 0.02\n",
      "iteration: 147600 loss: 0.0015 lr: 0.02\n",
      "iteration: 147700 loss: 0.0015 lr: 0.02\n",
      "iteration: 147800 loss: 0.0014 lr: 0.02\n",
      "iteration: 147900 loss: 0.0015 lr: 0.02\n",
      "iteration: 148000 loss: 0.0015 lr: 0.02\n",
      "iteration: 148100 loss: 0.0015 lr: 0.02\n",
      "iteration: 148200 loss: 0.0014 lr: 0.02\n",
      "iteration: 148300 loss: 0.0015 lr: 0.02\n",
      "iteration: 148400 loss: 0.0016 lr: 0.02\n",
      "iteration: 148500 loss: 0.0016 lr: 0.02\n",
      "iteration: 148600 loss: 0.0013 lr: 0.02\n",
      "iteration: 148700 loss: 0.0014 lr: 0.02\n",
      "iteration: 148800 loss: 0.0014 lr: 0.02\n",
      "iteration: 148900 loss: 0.0015 lr: 0.02\n",
      "iteration: 149000 loss: 0.0014 lr: 0.02\n",
      "iteration: 149100 loss: 0.0014 lr: 0.02\n",
      "iteration: 149200 loss: 0.0015 lr: 0.02\n",
      "iteration: 149300 loss: 0.0014 lr: 0.02\n",
      "iteration: 149400 loss: 0.0017 lr: 0.02\n",
      "iteration: 149500 loss: 0.0014 lr: 0.02\n",
      "iteration: 149600 loss: 0.0015 lr: 0.02\n",
      "iteration: 149700 loss: 0.0014 lr: 0.02\n",
      "iteration: 149800 loss: 0.0015 lr: 0.02\n",
      "iteration: 149900 loss: 0.0015 lr: 0.02\n",
      "iteration: 150000 loss: 0.0014 lr: 0.02\n",
      "iteration: 150100 loss: 0.0016 lr: 0.02\n",
      "iteration: 150200 loss: 0.0017 lr: 0.02\n",
      "iteration: 150300 loss: 0.0015 lr: 0.02\n",
      "iteration: 150400 loss: 0.0015 lr: 0.02\n",
      "iteration: 150500 loss: 0.0016 lr: 0.02\n",
      "iteration: 150600 loss: 0.0014 lr: 0.02\n",
      "iteration: 150700 loss: 0.0014 lr: 0.02\n",
      "iteration: 150800 loss: 0.0014 lr: 0.02\n",
      "iteration: 150900 loss: 0.0015 lr: 0.02\n",
      "iteration: 151000 loss: 0.0016 lr: 0.02\n",
      "iteration: 151100 loss: 0.0015 lr: 0.02\n",
      "iteration: 151200 loss: 0.0013 lr: 0.02\n",
      "iteration: 151300 loss: 0.0014 lr: 0.02\n",
      "iteration: 151400 loss: 0.0015 lr: 0.02\n",
      "iteration: 151500 loss: 0.0015 lr: 0.02\n",
      "iteration: 151600 loss: 0.0015 lr: 0.02\n",
      "iteration: 151700 loss: 0.0015 lr: 0.02\n",
      "iteration: 151800 loss: 0.0016 lr: 0.02\n",
      "iteration: 151900 loss: 0.0015 lr: 0.02\n",
      "iteration: 152000 loss: 0.0016 lr: 0.02\n",
      "iteration: 152100 loss: 0.0014 lr: 0.02\n",
      "iteration: 152200 loss: 0.0015 lr: 0.02\n",
      "iteration: 152300 loss: 0.0013 lr: 0.02\n",
      "iteration: 152400 loss: 0.0014 lr: 0.02\n",
      "iteration: 152500 loss: 0.0014 lr: 0.02\n",
      "iteration: 152600 loss: 0.0015 lr: 0.02\n",
      "iteration: 152700 loss: 0.0015 lr: 0.02\n",
      "iteration: 152800 loss: 0.0016 lr: 0.02\n",
      "iteration: 152900 loss: 0.0015 lr: 0.02\n",
      "iteration: 153000 loss: 0.0014 lr: 0.02\n",
      "iteration: 153100 loss: 0.0015 lr: 0.02\n",
      "iteration: 153200 loss: 0.0015 lr: 0.02\n",
      "iteration: 153300 loss: 0.0014 lr: 0.02\n",
      "iteration: 153400 loss: 0.0014 lr: 0.02\n",
      "iteration: 153500 loss: 0.0015 lr: 0.02\n",
      "iteration: 153600 loss: 0.0015 lr: 0.02\n",
      "iteration: 153700 loss: 0.0014 lr: 0.02\n",
      "iteration: 153800 loss: 0.0014 lr: 0.02\n",
      "iteration: 153900 loss: 0.0015 lr: 0.02\n",
      "iteration: 154000 loss: 0.0015 lr: 0.02\n",
      "iteration: 154100 loss: 0.0015 lr: 0.02\n",
      "iteration: 154200 loss: 0.0014 lr: 0.02\n",
      "iteration: 154300 loss: 0.0012 lr: 0.02\n",
      "iteration: 154400 loss: 0.0016 lr: 0.02\n",
      "iteration: 154500 loss: 0.0015 lr: 0.02\n",
      "iteration: 154600 loss: 0.0014 lr: 0.02\n",
      "iteration: 154700 loss: 0.0015 lr: 0.02\n",
      "iteration: 154800 loss: 0.0013 lr: 0.02\n",
      "iteration: 154900 loss: 0.0015 lr: 0.02\n",
      "iteration: 155000 loss: 0.0014 lr: 0.02\n",
      "iteration: 155100 loss: 0.0014 lr: 0.02\n",
      "iteration: 155200 loss: 0.0014 lr: 0.02\n",
      "iteration: 155300 loss: 0.0014 lr: 0.02\n",
      "iteration: 155400 loss: 0.0015 lr: 0.02\n",
      "iteration: 155500 loss: 0.0015 lr: 0.02\n",
      "iteration: 155600 loss: 0.0014 lr: 0.02\n",
      "iteration: 155700 loss: 0.0015 lr: 0.02\n",
      "iteration: 155800 loss: 0.0014 lr: 0.02\n",
      "iteration: 155900 loss: 0.0016 lr: 0.02\n",
      "iteration: 156000 loss: 0.0016 lr: 0.02\n",
      "iteration: 156100 loss: 0.0016 lr: 0.02\n",
      "iteration: 156200 loss: 0.0014 lr: 0.02\n",
      "iteration: 156300 loss: 0.0015 lr: 0.02\n",
      "iteration: 156400 loss: 0.0014 lr: 0.02\n",
      "iteration: 156500 loss: 0.0013 lr: 0.02\n",
      "iteration: 156600 loss: 0.0015 lr: 0.02\n",
      "iteration: 156700 loss: 0.0016 lr: 0.02\n",
      "iteration: 156800 loss: 0.0014 lr: 0.02\n",
      "iteration: 156900 loss: 0.0016 lr: 0.02\n",
      "iteration: 157000 loss: 0.0013 lr: 0.02\n",
      "iteration: 157100 loss: 0.0014 lr: 0.02\n",
      "iteration: 157200 loss: 0.0013 lr: 0.02\n",
      "iteration: 157300 loss: 0.0014 lr: 0.02\n",
      "iteration: 157400 loss: 0.0014 lr: 0.02\n",
      "iteration: 157500 loss: 0.0014 lr: 0.02\n",
      "iteration: 157600 loss: 0.0013 lr: 0.02\n",
      "iteration: 157700 loss: 0.0014 lr: 0.02\n",
      "iteration: 157800 loss: 0.0014 lr: 0.02\n",
      "iteration: 157900 loss: 0.0015 lr: 0.02\n",
      "iteration: 158000 loss: 0.0017 lr: 0.02\n",
      "iteration: 158100 loss: 0.0014 lr: 0.02\n",
      "iteration: 158200 loss: 0.0016 lr: 0.02\n",
      "iteration: 158300 loss: 0.0014 lr: 0.02\n",
      "iteration: 158400 loss: 0.0014 lr: 0.02\n",
      "iteration: 158500 loss: 0.0014 lr: 0.02\n",
      "iteration: 158600 loss: 0.0015 lr: 0.02\n",
      "iteration: 158700 loss: 0.0016 lr: 0.02\n",
      "iteration: 158800 loss: 0.0014 lr: 0.02\n",
      "iteration: 158900 loss: 0.0012 lr: 0.02\n",
      "iteration: 159000 loss: 0.0013 lr: 0.02\n",
      "iteration: 159100 loss: 0.0015 lr: 0.02\n",
      "iteration: 159200 loss: 0.0014 lr: 0.02\n",
      "iteration: 159300 loss: 0.0013 lr: 0.02\n",
      "iteration: 159400 loss: 0.0014 lr: 0.02\n",
      "iteration: 159500 loss: 0.0013 lr: 0.02\n",
      "iteration: 159600 loss: 0.0017 lr: 0.02\n",
      "iteration: 159700 loss: 0.0014 lr: 0.02\n",
      "iteration: 159800 loss: 0.0014 lr: 0.02\n",
      "iteration: 159900 loss: 0.0014 lr: 0.02\n",
      "iteration: 160000 loss: 0.0014 lr: 0.02\n",
      "iteration: 160100 loss: 0.0014 lr: 0.02\n",
      "iteration: 160200 loss: 0.0014 lr: 0.02\n",
      "iteration: 160300 loss: 0.0015 lr: 0.02\n",
      "iteration: 160400 loss: 0.0015 lr: 0.02\n",
      "iteration: 160500 loss: 0.0016 lr: 0.02\n",
      "iteration: 160600 loss: 0.0014 lr: 0.02\n",
      "iteration: 160700 loss: 0.0014 lr: 0.02\n",
      "iteration: 160800 loss: 0.0015 lr: 0.02\n",
      "iteration: 160900 loss: 0.0015 lr: 0.02\n",
      "iteration: 161000 loss: 0.0013 lr: 0.02\n",
      "iteration: 161100 loss: 0.0014 lr: 0.02\n",
      "iteration: 161200 loss: 0.0012 lr: 0.02\n",
      "iteration: 161300 loss: 0.0015 lr: 0.02\n",
      "iteration: 161400 loss: 0.0015 lr: 0.02\n",
      "iteration: 161500 loss: 0.0013 lr: 0.02\n",
      "iteration: 161600 loss: 0.0014 lr: 0.02\n",
      "iteration: 161700 loss: 0.0015 lr: 0.02\n",
      "iteration: 161800 loss: 0.0014 lr: 0.02\n",
      "iteration: 161900 loss: 0.0015 lr: 0.02\n",
      "iteration: 162000 loss: 0.0014 lr: 0.02\n",
      "iteration: 162100 loss: 0.0014 lr: 0.02\n",
      "iteration: 162200 loss: 0.0014 lr: 0.02\n",
      "iteration: 162300 loss: 0.0014 lr: 0.02\n",
      "iteration: 162400 loss: 0.0016 lr: 0.02\n",
      "iteration: 162500 loss: 0.0013 lr: 0.02\n",
      "iteration: 162600 loss: 0.0015 lr: 0.02\n",
      "iteration: 162700 loss: 0.0014 lr: 0.02\n",
      "iteration: 162800 loss: 0.0015 lr: 0.02\n",
      "iteration: 162900 loss: 0.0015 lr: 0.02\n",
      "iteration: 163000 loss: 0.0015 lr: 0.02\n",
      "iteration: 163100 loss: 0.0014 lr: 0.02\n",
      "iteration: 163200 loss: 0.0015 lr: 0.02\n",
      "iteration: 163300 loss: 0.0015 lr: 0.02\n",
      "iteration: 163400 loss: 0.0014 lr: 0.02\n",
      "iteration: 163500 loss: 0.0014 lr: 0.02\n",
      "iteration: 163600 loss: 0.0013 lr: 0.02\n",
      "iteration: 163700 loss: 0.0016 lr: 0.02\n",
      "iteration: 163800 loss: 0.0016 lr: 0.02\n",
      "iteration: 163900 loss: 0.0014 lr: 0.02\n",
      "iteration: 164000 loss: 0.0013 lr: 0.02\n",
      "iteration: 164100 loss: 0.0014 lr: 0.02\n",
      "iteration: 164200 loss: 0.0014 lr: 0.02\n",
      "iteration: 164300 loss: 0.0015 lr: 0.02\n",
      "iteration: 164400 loss: 0.0014 lr: 0.02\n",
      "iteration: 164500 loss: 0.0015 lr: 0.02\n",
      "iteration: 164600 loss: 0.0015 lr: 0.02\n",
      "iteration: 164700 loss: 0.0014 lr: 0.02\n",
      "iteration: 164800 loss: 0.0015 lr: 0.02\n",
      "iteration: 164900 loss: 0.0012 lr: 0.02\n",
      "iteration: 165000 loss: 0.0015 lr: 0.02\n",
      "iteration: 165100 loss: 0.0016 lr: 0.02\n",
      "iteration: 165200 loss: 0.0015 lr: 0.02\n",
      "iteration: 165300 loss: 0.0015 lr: 0.02\n",
      "iteration: 165400 loss: 0.0014 lr: 0.02\n",
      "iteration: 165500 loss: 0.0014 lr: 0.02\n",
      "iteration: 165600 loss: 0.0014 lr: 0.02\n",
      "iteration: 165700 loss: 0.0014 lr: 0.02\n",
      "iteration: 165800 loss: 0.0015 lr: 0.02\n",
      "iteration: 165900 loss: 0.0014 lr: 0.02\n",
      "iteration: 166000 loss: 0.0014 lr: 0.02\n",
      "iteration: 166100 loss: 0.0016 lr: 0.02\n",
      "iteration: 166200 loss: 0.0016 lr: 0.02\n",
      "iteration: 166300 loss: 0.0014 lr: 0.02\n",
      "iteration: 166400 loss: 0.0016 lr: 0.02\n",
      "iteration: 166500 loss: 0.0013 lr: 0.02\n",
      "iteration: 166600 loss: 0.0013 lr: 0.02\n",
      "iteration: 166700 loss: 0.0014 lr: 0.02\n",
      "iteration: 166800 loss: 0.0014 lr: 0.02\n",
      "iteration: 166900 loss: 0.0015 lr: 0.02\n",
      "iteration: 167000 loss: 0.0014 lr: 0.02\n",
      "iteration: 167100 loss: 0.0013 lr: 0.02\n",
      "iteration: 167200 loss: 0.0014 lr: 0.02\n",
      "iteration: 167300 loss: 0.0014 lr: 0.02\n",
      "iteration: 167400 loss: 0.0014 lr: 0.02\n",
      "iteration: 167500 loss: 0.0015 lr: 0.02\n",
      "iteration: 167600 loss: 0.0014 lr: 0.02\n",
      "iteration: 167700 loss: 0.0014 lr: 0.02\n",
      "iteration: 167800 loss: 0.0014 lr: 0.02\n",
      "iteration: 167900 loss: 0.0013 lr: 0.02\n",
      "iteration: 168000 loss: 0.0013 lr: 0.02\n",
      "iteration: 168100 loss: 0.0013 lr: 0.02\n",
      "iteration: 168200 loss: 0.0015 lr: 0.02\n",
      "iteration: 168300 loss: 0.0014 lr: 0.02\n",
      "iteration: 168400 loss: 0.0014 lr: 0.02\n",
      "iteration: 168500 loss: 0.0014 lr: 0.02\n",
      "iteration: 168600 loss: 0.0014 lr: 0.02\n",
      "iteration: 168700 loss: 0.0013 lr: 0.02\n",
      "iteration: 168800 loss: 0.0016 lr: 0.02\n",
      "iteration: 168900 loss: 0.0014 lr: 0.02\n",
      "iteration: 169000 loss: 0.0014 lr: 0.02\n",
      "iteration: 169100 loss: 0.0013 lr: 0.02\n",
      "iteration: 169200 loss: 0.0013 lr: 0.02\n",
      "iteration: 169300 loss: 0.0015 lr: 0.02\n",
      "iteration: 169400 loss: 0.0014 lr: 0.02\n",
      "iteration: 169500 loss: 0.0014 lr: 0.02\n",
      "iteration: 169600 loss: 0.0016 lr: 0.02\n",
      "iteration: 169700 loss: 0.0013 lr: 0.02\n",
      "iteration: 169800 loss: 0.0013 lr: 0.02\n",
      "iteration: 169900 loss: 0.0014 lr: 0.02\n",
      "iteration: 170000 loss: 0.0013 lr: 0.02\n",
      "iteration: 170100 loss: 0.0014 lr: 0.02\n",
      "iteration: 170200 loss: 0.0015 lr: 0.02\n",
      "iteration: 170300 loss: 0.0013 lr: 0.02\n",
      "iteration: 170400 loss: 0.0013 lr: 0.02\n",
      "iteration: 170500 loss: 0.0014 lr: 0.02\n",
      "iteration: 170600 loss: 0.0013 lr: 0.02\n",
      "iteration: 170700 loss: 0.0015 lr: 0.02\n",
      "iteration: 170800 loss: 0.0014 lr: 0.02\n",
      "iteration: 170900 loss: 0.0013 lr: 0.02\n",
      "iteration: 171000 loss: 0.0014 lr: 0.02\n",
      "iteration: 171100 loss: 0.0013 lr: 0.02\n",
      "iteration: 171200 loss: 0.0014 lr: 0.02\n",
      "iteration: 171300 loss: 0.0014 lr: 0.02\n",
      "iteration: 171400 loss: 0.0013 lr: 0.02\n",
      "iteration: 171500 loss: 0.0015 lr: 0.02\n",
      "iteration: 171600 loss: 0.0012 lr: 0.02\n",
      "iteration: 171700 loss: 0.0014 lr: 0.02\n",
      "iteration: 171800 loss: 0.0013 lr: 0.02\n",
      "iteration: 171900 loss: 0.0013 lr: 0.02\n",
      "iteration: 172000 loss: 0.0015 lr: 0.02\n",
      "iteration: 172100 loss: 0.0013 lr: 0.02\n",
      "iteration: 172200 loss: 0.0014 lr: 0.02\n",
      "iteration: 172300 loss: 0.0016 lr: 0.02\n",
      "iteration: 172400 loss: 0.0014 lr: 0.02\n",
      "iteration: 172500 loss: 0.0014 lr: 0.02\n",
      "iteration: 172600 loss: 0.0013 lr: 0.02\n",
      "iteration: 172700 loss: 0.0014 lr: 0.02\n",
      "iteration: 172800 loss: 0.0014 lr: 0.02\n",
      "iteration: 172900 loss: 0.0013 lr: 0.02\n",
      "iteration: 173000 loss: 0.0013 lr: 0.02\n",
      "iteration: 173100 loss: 0.0014 lr: 0.02\n",
      "iteration: 173200 loss: 0.0013 lr: 0.02\n",
      "iteration: 173300 loss: 0.0014 lr: 0.02\n",
      "iteration: 173400 loss: 0.0013 lr: 0.02\n",
      "iteration: 173500 loss: 0.0013 lr: 0.02\n",
      "iteration: 173600 loss: 0.0015 lr: 0.02\n",
      "iteration: 173700 loss: 0.0014 lr: 0.02\n",
      "iteration: 173800 loss: 0.0013 lr: 0.02\n",
      "iteration: 173900 loss: 0.0013 lr: 0.02\n",
      "iteration: 174000 loss: 0.0014 lr: 0.02\n",
      "iteration: 174100 loss: 0.0014 lr: 0.02\n",
      "iteration: 174200 loss: 0.0014 lr: 0.02\n",
      "iteration: 174300 loss: 0.0015 lr: 0.02\n",
      "iteration: 174400 loss: 0.0014 lr: 0.02\n",
      "iteration: 174500 loss: 0.0013 lr: 0.02\n",
      "iteration: 174600 loss: 0.0013 lr: 0.02\n",
      "iteration: 174700 loss: 0.0015 lr: 0.02\n",
      "iteration: 174800 loss: 0.0013 lr: 0.02\n",
      "iteration: 174900 loss: 0.0014 lr: 0.02\n",
      "iteration: 175000 loss: 0.0012 lr: 0.02\n",
      "iteration: 175100 loss: 0.0014 lr: 0.02\n",
      "iteration: 175200 loss: 0.0014 lr: 0.02\n",
      "iteration: 175300 loss: 0.0013 lr: 0.02\n",
      "iteration: 175400 loss: 0.0016 lr: 0.02\n",
      "iteration: 175500 loss: 0.0015 lr: 0.02\n",
      "iteration: 175600 loss: 0.0014 lr: 0.02\n",
      "iteration: 175700 loss: 0.0013 lr: 0.02\n",
      "iteration: 175800 loss: 0.0014 lr: 0.02\n",
      "iteration: 175900 loss: 0.0015 lr: 0.02\n",
      "iteration: 176000 loss: 0.0014 lr: 0.02\n",
      "iteration: 176100 loss: 0.0013 lr: 0.02\n",
      "iteration: 176200 loss: 0.0013 lr: 0.02\n",
      "iteration: 176300 loss: 0.0013 lr: 0.02\n",
      "iteration: 176400 loss: 0.0014 lr: 0.02\n",
      "iteration: 176500 loss: 0.0013 lr: 0.02\n",
      "iteration: 176600 loss: 0.0014 lr: 0.02\n",
      "iteration: 176700 loss: 0.0014 lr: 0.02\n",
      "iteration: 176800 loss: 0.0014 lr: 0.02\n",
      "iteration: 176900 loss: 0.0013 lr: 0.02\n",
      "iteration: 177000 loss: 0.0013 lr: 0.02\n",
      "iteration: 177100 loss: 0.0012 lr: 0.02\n",
      "iteration: 177200 loss: 0.0014 lr: 0.02\n",
      "iteration: 177300 loss: 0.0013 lr: 0.02\n",
      "iteration: 177400 loss: 0.0014 lr: 0.02\n",
      "iteration: 177500 loss: 0.0012 lr: 0.02\n",
      "iteration: 177600 loss: 0.0014 lr: 0.02\n",
      "iteration: 177700 loss: 0.0015 lr: 0.02\n",
      "iteration: 177800 loss: 0.0014 lr: 0.02\n",
      "iteration: 177900 loss: 0.0013 lr: 0.02\n",
      "iteration: 178000 loss: 0.0013 lr: 0.02\n",
      "iteration: 178100 loss: 0.0016 lr: 0.02\n",
      "iteration: 178200 loss: 0.0014 lr: 0.02\n",
      "iteration: 178300 loss: 0.0012 lr: 0.02\n",
      "iteration: 178400 loss: 0.0014 lr: 0.02\n",
      "iteration: 178500 loss: 0.0015 lr: 0.02\n",
      "iteration: 178600 loss: 0.0014 lr: 0.02\n",
      "iteration: 178700 loss: 0.0015 lr: 0.02\n",
      "iteration: 178800 loss: 0.0013 lr: 0.02\n",
      "iteration: 178900 loss: 0.0013 lr: 0.02\n",
      "iteration: 179000 loss: 0.0014 lr: 0.02\n",
      "iteration: 179100 loss: 0.0013 lr: 0.02\n",
      "iteration: 179200 loss: 0.0014 lr: 0.02\n",
      "iteration: 179300 loss: 0.0013 lr: 0.02\n",
      "iteration: 179400 loss: 0.0015 lr: 0.02\n",
      "iteration: 179500 loss: 0.0015 lr: 0.02\n",
      "iteration: 179600 loss: 0.0013 lr: 0.02\n",
      "iteration: 179700 loss: 0.0013 lr: 0.02\n",
      "iteration: 179800 loss: 0.0012 lr: 0.02\n",
      "iteration: 179900 loss: 0.0013 lr: 0.02\n",
      "iteration: 180000 loss: 0.0014 lr: 0.02\n",
      "iteration: 180100 loss: 0.0016 lr: 0.02\n",
      "iteration: 180200 loss: 0.0014 lr: 0.02\n",
      "iteration: 180300 loss: 0.0012 lr: 0.02\n",
      "iteration: 180400 loss: 0.0012 lr: 0.02\n",
      "iteration: 180500 loss: 0.0013 lr: 0.02\n",
      "iteration: 180600 loss: 0.0012 lr: 0.02\n",
      "iteration: 180700 loss: 0.0012 lr: 0.02\n",
      "iteration: 180800 loss: 0.0012 lr: 0.02\n",
      "iteration: 180900 loss: 0.0014 lr: 0.02\n",
      "iteration: 181000 loss: 0.0014 lr: 0.02\n",
      "iteration: 181100 loss: 0.0013 lr: 0.02\n",
      "iteration: 181200 loss: 0.0013 lr: 0.02\n",
      "iteration: 181300 loss: 0.0013 lr: 0.02\n",
      "iteration: 181400 loss: 0.0013 lr: 0.02\n",
      "iteration: 181500 loss: 0.0013 lr: 0.02\n",
      "iteration: 181600 loss: 0.0013 lr: 0.02\n",
      "iteration: 181700 loss: 0.0012 lr: 0.02\n",
      "iteration: 181800 loss: 0.0015 lr: 0.02\n",
      "iteration: 181900 loss: 0.0013 lr: 0.02\n",
      "iteration: 182000 loss: 0.0014 lr: 0.02\n",
      "iteration: 182100 loss: 0.0013 lr: 0.02\n",
      "iteration: 182200 loss: 0.0014 lr: 0.02\n",
      "iteration: 182300 loss: 0.0016 lr: 0.02\n",
      "iteration: 182400 loss: 0.0012 lr: 0.02\n",
      "iteration: 182500 loss: 0.0015 lr: 0.02\n",
      "iteration: 182600 loss: 0.0014 lr: 0.02\n",
      "iteration: 182700 loss: 0.0014 lr: 0.02\n",
      "iteration: 182800 loss: 0.0013 lr: 0.02\n",
      "iteration: 182900 loss: 0.0014 lr: 0.02\n",
      "iteration: 183000 loss: 0.0013 lr: 0.02\n",
      "iteration: 183100 loss: 0.0014 lr: 0.02\n",
      "iteration: 183200 loss: 0.0014 lr: 0.02\n",
      "iteration: 183300 loss: 0.0015 lr: 0.02\n",
      "iteration: 183400 loss: 0.0015 lr: 0.02\n",
      "iteration: 183500 loss: 0.0014 lr: 0.02\n",
      "iteration: 183600 loss: 0.0015 lr: 0.02\n",
      "iteration: 183700 loss: 0.0013 lr: 0.02\n",
      "iteration: 183800 loss: 0.0013 lr: 0.02\n",
      "iteration: 183900 loss: 0.0013 lr: 0.02\n",
      "iteration: 184000 loss: 0.0013 lr: 0.02\n",
      "iteration: 184100 loss: 0.0013 lr: 0.02\n",
      "iteration: 184200 loss: 0.0014 lr: 0.02\n",
      "iteration: 184300 loss: 0.0012 lr: 0.02\n",
      "iteration: 184400 loss: 0.0013 lr: 0.02\n",
      "iteration: 184500 loss: 0.0016 lr: 0.02\n",
      "iteration: 184600 loss: 0.0015 lr: 0.02\n",
      "iteration: 184700 loss: 0.0013 lr: 0.02\n",
      "iteration: 184800 loss: 0.0014 lr: 0.02\n",
      "iteration: 184900 loss: 0.0013 lr: 0.02\n",
      "iteration: 185000 loss: 0.0015 lr: 0.02\n",
      "iteration: 185100 loss: 0.0015 lr: 0.02\n",
      "iteration: 185200 loss: 0.0012 lr: 0.02\n",
      "iteration: 185300 loss: 0.0014 lr: 0.02\n",
      "iteration: 185400 loss: 0.0015 lr: 0.02\n",
      "iteration: 185500 loss: 0.0013 lr: 0.02\n",
      "iteration: 185600 loss: 0.0014 lr: 0.02\n",
      "iteration: 185700 loss: 0.0013 lr: 0.02\n",
      "iteration: 185800 loss: 0.0014 lr: 0.02\n",
      "iteration: 185900 loss: 0.0014 lr: 0.02\n",
      "iteration: 186000 loss: 0.0014 lr: 0.02\n",
      "iteration: 186100 loss: 0.0014 lr: 0.02\n",
      "iteration: 186200 loss: 0.0012 lr: 0.02\n",
      "iteration: 186300 loss: 0.0013 lr: 0.02\n",
      "iteration: 186400 loss: 0.0015 lr: 0.02\n",
      "iteration: 186500 loss: 0.0014 lr: 0.02\n",
      "iteration: 186600 loss: 0.0013 lr: 0.02\n",
      "iteration: 186700 loss: 0.0014 lr: 0.02\n",
      "iteration: 186800 loss: 0.0013 lr: 0.02\n",
      "iteration: 186900 loss: 0.0015 lr: 0.02\n",
      "iteration: 187000 loss: 0.0012 lr: 0.02\n",
      "iteration: 187100 loss: 0.0012 lr: 0.02\n",
      "iteration: 187200 loss: 0.0013 lr: 0.02\n",
      "iteration: 187300 loss: 0.0015 lr: 0.02\n",
      "iteration: 187400 loss: 0.0015 lr: 0.02\n",
      "iteration: 187500 loss: 0.0015 lr: 0.02\n",
      "iteration: 187600 loss: 0.0014 lr: 0.02\n",
      "iteration: 187700 loss: 0.0012 lr: 0.02\n",
      "iteration: 187800 loss: 0.0014 lr: 0.02\n",
      "iteration: 187900 loss: 0.0014 lr: 0.02\n",
      "iteration: 188000 loss: 0.0013 lr: 0.02\n",
      "iteration: 188100 loss: 0.0014 lr: 0.02\n",
      "iteration: 188200 loss: 0.0012 lr: 0.02\n",
      "iteration: 188300 loss: 0.0015 lr: 0.02\n",
      "iteration: 188400 loss: 0.0014 lr: 0.02\n",
      "iteration: 188500 loss: 0.0013 lr: 0.02\n",
      "iteration: 188600 loss: 0.0013 lr: 0.02\n",
      "iteration: 188700 loss: 0.0012 lr: 0.02\n",
      "iteration: 188800 loss: 0.0014 lr: 0.02\n",
      "iteration: 188900 loss: 0.0015 lr: 0.02\n",
      "iteration: 189000 loss: 0.0014 lr: 0.02\n",
      "iteration: 189100 loss: 0.0013 lr: 0.02\n",
      "iteration: 189200 loss: 0.0013 lr: 0.02\n",
      "iteration: 189300 loss: 0.0014 lr: 0.02\n",
      "iteration: 189400 loss: 0.0013 lr: 0.02\n",
      "iteration: 189500 loss: 0.0015 lr: 0.02\n",
      "iteration: 189600 loss: 0.0013 lr: 0.02\n",
      "iteration: 189700 loss: 0.0013 lr: 0.02\n",
      "iteration: 189800 loss: 0.0013 lr: 0.02\n",
      "iteration: 189900 loss: 0.0013 lr: 0.02\n",
      "iteration: 190000 loss: 0.0014 lr: 0.02\n",
      "iteration: 190100 loss: 0.0014 lr: 0.02\n",
      "iteration: 190200 loss: 0.0015 lr: 0.02\n",
      "iteration: 190300 loss: 0.0012 lr: 0.02\n",
      "iteration: 190400 loss: 0.0012 lr: 0.02\n",
      "iteration: 190500 loss: 0.0012 lr: 0.02\n",
      "iteration: 190600 loss: 0.0013 lr: 0.02\n",
      "iteration: 190700 loss: 0.0013 lr: 0.02\n",
      "iteration: 190800 loss: 0.0013 lr: 0.02\n",
      "iteration: 190900 loss: 0.0014 lr: 0.02\n",
      "iteration: 191000 loss: 0.0013 lr: 0.02\n",
      "iteration: 191100 loss: 0.0013 lr: 0.02\n",
      "iteration: 191200 loss: 0.0013 lr: 0.02\n",
      "iteration: 191300 loss: 0.0015 lr: 0.02\n",
      "iteration: 191400 loss: 0.0014 lr: 0.02\n",
      "iteration: 191500 loss: 0.0014 lr: 0.02\n",
      "iteration: 191600 loss: 0.0013 lr: 0.02\n",
      "iteration: 191700 loss: 0.0013 lr: 0.02\n",
      "iteration: 191800 loss: 0.0013 lr: 0.02\n",
      "iteration: 191900 loss: 0.0013 lr: 0.02\n",
      "iteration: 192000 loss: 0.0014 lr: 0.02\n",
      "iteration: 192100 loss: 0.0014 lr: 0.02\n",
      "iteration: 192200 loss: 0.0015 lr: 0.02\n",
      "iteration: 192300 loss: 0.0015 lr: 0.02\n",
      "iteration: 192400 loss: 0.0014 lr: 0.02\n",
      "iteration: 192500 loss: 0.0012 lr: 0.02\n",
      "iteration: 192600 loss: 0.0013 lr: 0.02\n",
      "iteration: 192700 loss: 0.0014 lr: 0.02\n",
      "iteration: 192800 loss: 0.0014 lr: 0.02\n",
      "iteration: 192900 loss: 0.0015 lr: 0.02\n",
      "iteration: 193000 loss: 0.0013 lr: 0.02\n",
      "iteration: 193100 loss: 0.0015 lr: 0.02\n",
      "iteration: 193200 loss: 0.0016 lr: 0.02\n",
      "iteration: 193300 loss: 0.0012 lr: 0.02\n",
      "iteration: 193400 loss: 0.0013 lr: 0.02\n",
      "iteration: 193500 loss: 0.0013 lr: 0.02\n",
      "iteration: 193600 loss: 0.0013 lr: 0.02\n",
      "iteration: 193700 loss: 0.0014 lr: 0.02\n",
      "iteration: 193800 loss: 0.0013 lr: 0.02\n",
      "iteration: 193900 loss: 0.0014 lr: 0.02\n",
      "iteration: 194000 loss: 0.0013 lr: 0.02\n",
      "iteration: 194100 loss: 0.0013 lr: 0.02\n",
      "iteration: 194200 loss: 0.0013 lr: 0.02\n",
      "iteration: 194300 loss: 0.0014 lr: 0.02\n",
      "iteration: 194400 loss: 0.0012 lr: 0.02\n",
      "iteration: 194500 loss: 0.0014 lr: 0.02\n",
      "iteration: 194600 loss: 0.0013 lr: 0.02\n",
      "iteration: 194700 loss: 0.0012 lr: 0.02\n",
      "iteration: 194800 loss: 0.0015 lr: 0.02\n",
      "iteration: 194900 loss: 0.0013 lr: 0.02\n",
      "iteration: 195000 loss: 0.0014 lr: 0.02\n",
      "iteration: 195100 loss: 0.0013 lr: 0.02\n",
      "iteration: 195200 loss: 0.0013 lr: 0.02\n",
      "iteration: 195300 loss: 0.0013 lr: 0.02\n",
      "iteration: 195400 loss: 0.0016 lr: 0.02\n",
      "iteration: 195500 loss: 0.0014 lr: 0.02\n",
      "iteration: 195600 loss: 0.0014 lr: 0.02\n",
      "iteration: 195700 loss: 0.0013 lr: 0.02\n",
      "iteration: 195800 loss: 0.0014 lr: 0.02\n",
      "iteration: 195900 loss: 0.0013 lr: 0.02\n",
      "iteration: 196000 loss: 0.0014 lr: 0.02\n",
      "iteration: 196100 loss: 0.0013 lr: 0.02\n",
      "iteration: 196200 loss: 0.0013 lr: 0.02\n",
      "iteration: 196300 loss: 0.0015 lr: 0.02\n",
      "iteration: 196400 loss: 0.0012 lr: 0.02\n",
      "iteration: 196500 loss: 0.0014 lr: 0.02\n",
      "iteration: 196600 loss: 0.0012 lr: 0.02\n",
      "iteration: 196700 loss: 0.0015 lr: 0.02\n",
      "iteration: 196800 loss: 0.0014 lr: 0.02\n",
      "iteration: 196900 loss: 0.0014 lr: 0.02\n",
      "iteration: 197000 loss: 0.0016 lr: 0.02\n",
      "iteration: 197100 loss: 0.0015 lr: 0.02\n",
      "iteration: 197200 loss: 0.0014 lr: 0.02\n",
      "iteration: 197300 loss: 0.0012 lr: 0.02\n",
      "iteration: 197400 loss: 0.0013 lr: 0.02\n",
      "iteration: 197500 loss: 0.0014 lr: 0.02\n",
      "iteration: 197600 loss: 0.0014 lr: 0.02\n",
      "iteration: 197700 loss: 0.0014 lr: 0.02\n",
      "iteration: 197800 loss: 0.0012 lr: 0.02\n",
      "iteration: 197900 loss: 0.0013 lr: 0.02\n",
      "iteration: 198000 loss: 0.0013 lr: 0.02\n",
      "iteration: 198100 loss: 0.0012 lr: 0.02\n",
      "iteration: 198200 loss: 0.0015 lr: 0.02\n",
      "iteration: 198300 loss: 0.0013 lr: 0.02\n",
      "iteration: 198400 loss: 0.0013 lr: 0.02\n",
      "iteration: 198500 loss: 0.0014 lr: 0.02\n",
      "iteration: 198600 loss: 0.0013 lr: 0.02\n",
      "iteration: 198700 loss: 0.0013 lr: 0.02\n",
      "iteration: 198800 loss: 0.0013 lr: 0.02\n",
      "iteration: 198900 loss: 0.0013 lr: 0.02\n",
      "iteration: 199000 loss: 0.0012 lr: 0.02\n",
      "iteration: 199100 loss: 0.0013 lr: 0.02\n",
      "iteration: 199200 loss: 0.0013 lr: 0.02\n",
      "iteration: 199300 loss: 0.0013 lr: 0.02\n",
      "iteration: 199400 loss: 0.0014 lr: 0.02\n",
      "iteration: 199500 loss: 0.0013 lr: 0.02\n",
      "iteration: 199600 loss: 0.0014 lr: 0.02\n",
      "iteration: 199700 loss: 0.0013 lr: 0.02\n",
      "iteration: 199800 loss: 0.0012 lr: 0.02\n",
      "iteration: 199900 loss: 0.0014 lr: 0.02\n",
      "iteration: 200000 loss: 0.0013 lr: 0.02\n",
      "iteration: 200100 loss: 0.0014 lr: 0.02\n",
      "iteration: 200200 loss: 0.0013 lr: 0.02\n",
      "iteration: 200300 loss: 0.0013 lr: 0.02\n",
      "iteration: 200400 loss: 0.0013 lr: 0.02\n",
      "iteration: 200500 loss: 0.0013 lr: 0.02\n",
      "iteration: 200600 loss: 0.0015 lr: 0.02\n",
      "iteration: 200700 loss: 0.0013 lr: 0.02\n",
      "iteration: 200800 loss: 0.0014 lr: 0.02\n",
      "iteration: 200900 loss: 0.0012 lr: 0.02\n",
      "iteration: 201000 loss: 0.0013 lr: 0.02\n",
      "iteration: 201100 loss: 0.0014 lr: 0.02\n",
      "iteration: 201200 loss: 0.0014 lr: 0.02\n",
      "iteration: 201300 loss: 0.0013 lr: 0.02\n",
      "iteration: 201400 loss: 0.0014 lr: 0.02\n",
      "iteration: 201500 loss: 0.0014 lr: 0.02\n",
      "iteration: 201600 loss: 0.0014 lr: 0.02\n",
      "iteration: 201700 loss: 0.0013 lr: 0.02\n",
      "iteration: 201800 loss: 0.0012 lr: 0.02\n",
      "iteration: 201900 loss: 0.0013 lr: 0.02\n",
      "iteration: 202000 loss: 0.0012 lr: 0.02\n",
      "iteration: 202100 loss: 0.0013 lr: 0.02\n",
      "iteration: 202200 loss: 0.0012 lr: 0.02\n",
      "iteration: 202300 loss: 0.0013 lr: 0.02\n",
      "iteration: 202400 loss: 0.0014 lr: 0.02\n",
      "iteration: 202500 loss: 0.0012 lr: 0.02\n",
      "iteration: 202600 loss: 0.0014 lr: 0.02\n",
      "iteration: 202700 loss: 0.0012 lr: 0.02\n",
      "iteration: 202800 loss: 0.0013 lr: 0.02\n",
      "iteration: 202900 loss: 0.0014 lr: 0.02\n",
      "iteration: 203000 loss: 0.0014 lr: 0.02\n",
      "iteration: 203100 loss: 0.0014 lr: 0.02\n",
      "iteration: 203200 loss: 0.0013 lr: 0.02\n",
      "iteration: 203300 loss: 0.0013 lr: 0.02\n",
      "iteration: 203400 loss: 0.0014 lr: 0.02\n",
      "iteration: 203500 loss: 0.0013 lr: 0.02\n",
      "iteration: 203600 loss: 0.0013 lr: 0.02\n",
      "iteration: 203700 loss: 0.0014 lr: 0.02\n",
      "iteration: 203800 loss: 0.0014 lr: 0.02\n",
      "iteration: 203900 loss: 0.0012 lr: 0.02\n",
      "iteration: 204000 loss: 0.0012 lr: 0.02\n",
      "iteration: 204100 loss: 0.0014 lr: 0.02\n",
      "iteration: 204200 loss: 0.0015 lr: 0.02\n",
      "iteration: 204300 loss: 0.0013 lr: 0.02\n",
      "iteration: 204400 loss: 0.0012 lr: 0.02\n",
      "iteration: 204500 loss: 0.0012 lr: 0.02\n",
      "iteration: 204600 loss: 0.0014 lr: 0.02\n",
      "iteration: 204700 loss: 0.0013 lr: 0.02\n",
      "iteration: 204800 loss: 0.0013 lr: 0.02\n",
      "iteration: 204900 loss: 0.0013 lr: 0.02\n",
      "iteration: 205000 loss: 0.0014 lr: 0.02\n",
      "iteration: 205100 loss: 0.0013 lr: 0.02\n",
      "iteration: 205200 loss: 0.0014 lr: 0.02\n",
      "iteration: 205300 loss: 0.0014 lr: 0.02\n",
      "iteration: 205400 loss: 0.0014 lr: 0.02\n",
      "iteration: 205500 loss: 0.0014 lr: 0.02\n",
      "iteration: 205600 loss: 0.0013 lr: 0.02\n",
      "iteration: 205700 loss: 0.0013 lr: 0.02\n",
      "iteration: 205800 loss: 0.0012 lr: 0.02\n",
      "iteration: 205900 loss: 0.0012 lr: 0.02\n",
      "iteration: 206000 loss: 0.0012 lr: 0.02\n",
      "iteration: 206100 loss: 0.0013 lr: 0.02\n",
      "iteration: 206200 loss: 0.0013 lr: 0.02\n",
      "iteration: 206300 loss: 0.0013 lr: 0.02\n",
      "iteration: 206400 loss: 0.0013 lr: 0.02\n",
      "iteration: 206500 loss: 0.0013 lr: 0.02\n",
      "iteration: 206600 loss: 0.0013 lr: 0.02\n",
      "iteration: 206700 loss: 0.0012 lr: 0.02\n",
      "iteration: 206800 loss: 0.0013 lr: 0.02\n",
      "iteration: 206900 loss: 0.0015 lr: 0.02\n",
      "iteration: 207000 loss: 0.0014 lr: 0.02\n",
      "iteration: 207100 loss: 0.0015 lr: 0.02\n",
      "iteration: 207200 loss: 0.0014 lr: 0.02\n",
      "iteration: 207300 loss: 0.0013 lr: 0.02\n",
      "iteration: 207400 loss: 0.0013 lr: 0.02\n",
      "iteration: 207500 loss: 0.0012 lr: 0.02\n",
      "iteration: 207600 loss: 0.0013 lr: 0.02\n",
      "iteration: 207700 loss: 0.0013 lr: 0.02\n",
      "iteration: 207800 loss: 0.0011 lr: 0.02\n",
      "iteration: 207900 loss: 0.0014 lr: 0.02\n",
      "iteration: 208000 loss: 0.0014 lr: 0.02\n",
      "iteration: 208100 loss: 0.0013 lr: 0.02\n",
      "iteration: 208200 loss: 0.0013 lr: 0.02\n",
      "iteration: 208300 loss: 0.0013 lr: 0.02\n",
      "iteration: 208400 loss: 0.0013 lr: 0.02\n",
      "iteration: 208500 loss: 0.0013 lr: 0.02\n",
      "iteration: 208600 loss: 0.0014 lr: 0.02\n",
      "iteration: 208700 loss: 0.0013 lr: 0.02\n",
      "iteration: 208800 loss: 0.0013 lr: 0.02\n",
      "iteration: 208900 loss: 0.0012 lr: 0.02\n",
      "iteration: 209000 loss: 0.0014 lr: 0.02\n",
      "iteration: 209100 loss: 0.0013 lr: 0.02\n",
      "iteration: 209200 loss: 0.0015 lr: 0.02\n",
      "iteration: 209300 loss: 0.0013 lr: 0.02\n",
      "iteration: 209400 loss: 0.0013 lr: 0.02\n",
      "iteration: 209500 loss: 0.0013 lr: 0.02\n",
      "iteration: 209600 loss: 0.0012 lr: 0.02\n",
      "iteration: 209700 loss: 0.0014 lr: 0.02\n",
      "iteration: 209800 loss: 0.0012 lr: 0.02\n",
      "iteration: 209900 loss: 0.0012 lr: 0.02\n",
      "iteration: 210000 loss: 0.0013 lr: 0.02\n",
      "iteration: 210100 loss: 0.0013 lr: 0.02\n",
      "iteration: 210200 loss: 0.0013 lr: 0.02\n",
      "iteration: 210300 loss: 0.0014 lr: 0.02\n",
      "iteration: 210400 loss: 0.0013 lr: 0.02\n",
      "iteration: 210500 loss: 0.0014 lr: 0.02\n",
      "iteration: 210600 loss: 0.0012 lr: 0.02\n",
      "iteration: 210700 loss: 0.0013 lr: 0.02\n",
      "iteration: 210800 loss: 0.0011 lr: 0.02\n",
      "iteration: 210900 loss: 0.0012 lr: 0.02\n",
      "iteration: 211000 loss: 0.0012 lr: 0.02\n",
      "iteration: 211100 loss: 0.0013 lr: 0.02\n",
      "iteration: 211200 loss: 0.0014 lr: 0.02\n",
      "iteration: 211300 loss: 0.0013 lr: 0.02\n",
      "iteration: 211400 loss: 0.0014 lr: 0.02\n",
      "iteration: 211500 loss: 0.0014 lr: 0.02\n",
      "iteration: 211600 loss: 0.0012 lr: 0.02\n",
      "iteration: 211700 loss: 0.0014 lr: 0.02\n",
      "iteration: 211800 loss: 0.0014 lr: 0.02\n",
      "iteration: 211900 loss: 0.0011 lr: 0.02\n",
      "iteration: 212000 loss: 0.0014 lr: 0.02\n",
      "iteration: 212100 loss: 0.0014 lr: 0.02\n",
      "iteration: 212200 loss: 0.0013 lr: 0.02\n",
      "iteration: 212300 loss: 0.0013 lr: 0.02\n",
      "iteration: 212400 loss: 0.0013 lr: 0.02\n",
      "iteration: 212500 loss: 0.0012 lr: 0.02\n",
      "iteration: 212600 loss: 0.0015 lr: 0.02\n",
      "iteration: 212700 loss: 0.0013 lr: 0.02\n",
      "iteration: 212800 loss: 0.0014 lr: 0.02\n",
      "iteration: 212900 loss: 0.0013 lr: 0.02\n",
      "iteration: 213000 loss: 0.0013 lr: 0.02\n",
      "iteration: 213100 loss: 0.0014 lr: 0.02\n",
      "iteration: 213200 loss: 0.0015 lr: 0.02\n",
      "iteration: 213300 loss: 0.0014 lr: 0.02\n",
      "iteration: 213400 loss: 0.0014 lr: 0.02\n",
      "iteration: 213500 loss: 0.0011 lr: 0.02\n",
      "iteration: 213600 loss: 0.0012 lr: 0.02\n",
      "iteration: 213700 loss: 0.0015 lr: 0.02\n",
      "iteration: 213800 loss: 0.0015 lr: 0.02\n",
      "iteration: 213900 loss: 0.0013 lr: 0.02\n",
      "iteration: 214000 loss: 0.0013 lr: 0.02\n",
      "iteration: 214100 loss: 0.0014 lr: 0.02\n",
      "iteration: 214200 loss: 0.0015 lr: 0.02\n",
      "iteration: 214300 loss: 0.0013 lr: 0.02\n",
      "iteration: 214400 loss: 0.0014 lr: 0.02\n",
      "iteration: 214500 loss: 0.0015 lr: 0.02\n",
      "iteration: 214600 loss: 0.0014 lr: 0.02\n",
      "iteration: 214700 loss: 0.0012 lr: 0.02\n",
      "iteration: 214800 loss: 0.0015 lr: 0.02\n",
      "iteration: 214900 loss: 0.0014 lr: 0.02\n",
      "iteration: 215000 loss: 0.0014 lr: 0.02\n",
      "iteration: 215100 loss: 0.0013 lr: 0.02\n",
      "iteration: 215200 loss: 0.0013 lr: 0.02\n",
      "iteration: 215300 loss: 0.0013 lr: 0.02\n",
      "iteration: 215400 loss: 0.0013 lr: 0.02\n",
      "iteration: 215500 loss: 0.0014 lr: 0.02\n",
      "iteration: 215600 loss: 0.0012 lr: 0.02\n",
      "iteration: 215700 loss: 0.0013 lr: 0.02\n",
      "iteration: 215800 loss: 0.0014 lr: 0.02\n",
      "iteration: 215900 loss: 0.0012 lr: 0.02\n",
      "iteration: 216000 loss: 0.0013 lr: 0.02\n",
      "iteration: 216100 loss: 0.0014 lr: 0.02\n",
      "iteration: 216200 loss: 0.0013 lr: 0.02\n",
      "iteration: 216300 loss: 0.0013 lr: 0.02\n",
      "iteration: 216400 loss: 0.0013 lr: 0.02\n",
      "iteration: 216500 loss: 0.0013 lr: 0.02\n",
      "iteration: 216600 loss: 0.0015 lr: 0.02\n",
      "iteration: 216700 loss: 0.0012 lr: 0.02\n",
      "iteration: 216800 loss: 0.0013 lr: 0.02\n",
      "iteration: 216900 loss: 0.0013 lr: 0.02\n",
      "iteration: 217000 loss: 0.0014 lr: 0.02\n",
      "iteration: 217100 loss: 0.0014 lr: 0.02\n",
      "iteration: 217200 loss: 0.0015 lr: 0.02\n",
      "iteration: 217300 loss: 0.0013 lr: 0.02\n",
      "iteration: 217400 loss: 0.0012 lr: 0.02\n",
      "iteration: 217500 loss: 0.0014 lr: 0.02\n",
      "iteration: 217600 loss: 0.0014 lr: 0.02\n",
      "iteration: 217700 loss: 0.0013 lr: 0.02\n",
      "iteration: 217800 loss: 0.0013 lr: 0.02\n",
      "iteration: 217900 loss: 0.0014 lr: 0.02\n",
      "iteration: 218000 loss: 0.0014 lr: 0.02\n",
      "iteration: 218100 loss: 0.0013 lr: 0.02\n",
      "iteration: 218200 loss: 0.0013 lr: 0.02\n",
      "iteration: 218300 loss: 0.0013 lr: 0.02\n",
      "iteration: 218400 loss: 0.0013 lr: 0.02\n",
      "iteration: 218500 loss: 0.0013 lr: 0.02\n",
      "iteration: 218600 loss: 0.0013 lr: 0.02\n",
      "iteration: 218700 loss: 0.0012 lr: 0.02\n",
      "iteration: 218800 loss: 0.0015 lr: 0.02\n",
      "iteration: 218900 loss: 0.0013 lr: 0.02\n",
      "iteration: 219000 loss: 0.0014 lr: 0.02\n",
      "iteration: 219100 loss: 0.0014 lr: 0.02\n",
      "iteration: 219200 loss: 0.0013 lr: 0.02\n",
      "iteration: 219300 loss: 0.0014 lr: 0.02\n",
      "iteration: 219400 loss: 0.0015 lr: 0.02\n",
      "iteration: 219500 loss: 0.0014 lr: 0.02\n",
      "iteration: 219600 loss: 0.0012 lr: 0.02\n",
      "iteration: 219700 loss: 0.0014 lr: 0.02\n",
      "iteration: 219800 loss: 0.0014 lr: 0.02\n",
      "iteration: 219900 loss: 0.0013 lr: 0.02\n",
      "iteration: 220000 loss: 0.0013 lr: 0.02\n",
      "iteration: 220100 loss: 0.0013 lr: 0.02\n",
      "iteration: 220200 loss: 0.0012 lr: 0.02\n",
      "iteration: 220300 loss: 0.0014 lr: 0.02\n",
      "iteration: 220400 loss: 0.0011 lr: 0.02\n",
      "iteration: 220500 loss: 0.0013 lr: 0.02\n",
      "iteration: 220600 loss: 0.0014 lr: 0.02\n",
      "iteration: 220700 loss: 0.0012 lr: 0.02\n",
      "iteration: 220800 loss: 0.0013 lr: 0.02\n",
      "iteration: 220900 loss: 0.0012 lr: 0.02\n",
      "iteration: 221000 loss: 0.0014 lr: 0.02\n",
      "iteration: 221100 loss: 0.0013 lr: 0.02\n",
      "iteration: 221200 loss: 0.0012 lr: 0.02\n",
      "iteration: 221300 loss: 0.0012 lr: 0.02\n",
      "iteration: 221400 loss: 0.0013 lr: 0.02\n",
      "iteration: 221500 loss: 0.0012 lr: 0.02\n",
      "iteration: 221600 loss: 0.0014 lr: 0.02\n",
      "iteration: 221700 loss: 0.0015 lr: 0.02\n",
      "iteration: 221800 loss: 0.0013 lr: 0.02\n",
      "iteration: 221900 loss: 0.0013 lr: 0.02\n",
      "iteration: 222000 loss: 0.0014 lr: 0.02\n",
      "iteration: 222100 loss: 0.0012 lr: 0.02\n",
      "iteration: 222200 loss: 0.0013 lr: 0.02\n",
      "iteration: 222300 loss: 0.0012 lr: 0.02\n",
      "iteration: 222400 loss: 0.0012 lr: 0.02\n",
      "iteration: 222500 loss: 0.0014 lr: 0.02\n",
      "iteration: 222600 loss: 0.0014 lr: 0.02\n",
      "iteration: 222700 loss: 0.0012 lr: 0.02\n",
      "iteration: 222800 loss: 0.0013 lr: 0.02\n",
      "iteration: 222900 loss: 0.0012 lr: 0.02\n",
      "iteration: 223000 loss: 0.0011 lr: 0.02\n",
      "iteration: 223100 loss: 0.0012 lr: 0.02\n",
      "iteration: 223200 loss: 0.0012 lr: 0.02\n",
      "iteration: 223300 loss: 0.0012 lr: 0.02\n",
      "iteration: 223400 loss: 0.0012 lr: 0.02\n",
      "iteration: 223500 loss: 0.0013 lr: 0.02\n",
      "iteration: 223600 loss: 0.0013 lr: 0.02\n",
      "iteration: 223700 loss: 0.0013 lr: 0.02\n",
      "iteration: 223800 loss: 0.0013 lr: 0.02\n",
      "iteration: 223900 loss: 0.0012 lr: 0.02\n",
      "iteration: 224000 loss: 0.0013 lr: 0.02\n",
      "iteration: 224100 loss: 0.0013 lr: 0.02\n",
      "iteration: 224200 loss: 0.0012 lr: 0.02\n",
      "iteration: 224300 loss: 0.0014 lr: 0.02\n",
      "iteration: 224400 loss: 0.0012 lr: 0.02\n",
      "iteration: 224500 loss: 0.0012 lr: 0.02\n",
      "iteration: 224600 loss: 0.0013 lr: 0.02\n",
      "iteration: 224700 loss: 0.0013 lr: 0.02\n",
      "iteration: 224800 loss: 0.0011 lr: 0.02\n",
      "iteration: 224900 loss: 0.0012 lr: 0.02\n",
      "iteration: 225000 loss: 0.0013 lr: 0.02\n",
      "iteration: 225100 loss: 0.0014 lr: 0.02\n",
      "iteration: 225200 loss: 0.0013 lr: 0.02\n",
      "iteration: 225300 loss: 0.0013 lr: 0.02\n",
      "iteration: 225400 loss: 0.0015 lr: 0.02\n",
      "iteration: 225500 loss: 0.0013 lr: 0.02\n",
      "iteration: 225600 loss: 0.0015 lr: 0.02\n",
      "iteration: 225700 loss: 0.0013 lr: 0.02\n",
      "iteration: 225800 loss: 0.0013 lr: 0.02\n",
      "iteration: 225900 loss: 0.0012 lr: 0.02\n",
      "iteration: 226000 loss: 0.0012 lr: 0.02\n",
      "iteration: 226100 loss: 0.0014 lr: 0.02\n",
      "iteration: 226200 loss: 0.0013 lr: 0.02\n",
      "iteration: 226300 loss: 0.0015 lr: 0.02\n",
      "iteration: 226400 loss: 0.0013 lr: 0.02\n",
      "iteration: 226500 loss: 0.0013 lr: 0.02\n",
      "iteration: 226600 loss: 0.0013 lr: 0.02\n",
      "iteration: 226700 loss: 0.0014 lr: 0.02\n",
      "iteration: 226800 loss: 0.0013 lr: 0.02\n",
      "iteration: 226900 loss: 0.0015 lr: 0.02\n",
      "iteration: 227000 loss: 0.0013 lr: 0.02\n",
      "iteration: 227100 loss: 0.0013 lr: 0.02\n",
      "iteration: 227200 loss: 0.0013 lr: 0.02\n",
      "iteration: 227300 loss: 0.0012 lr: 0.02\n",
      "iteration: 227400 loss: 0.0013 lr: 0.02\n",
      "iteration: 227500 loss: 0.0013 lr: 0.02\n",
      "iteration: 227600 loss: 0.0013 lr: 0.02\n",
      "iteration: 227700 loss: 0.0013 lr: 0.02\n",
      "iteration: 227800 loss: 0.0012 lr: 0.02\n",
      "iteration: 227900 loss: 0.0013 lr: 0.02\n",
      "iteration: 228000 loss: 0.0013 lr: 0.02\n",
      "iteration: 228100 loss: 0.0013 lr: 0.02\n",
      "iteration: 228200 loss: 0.0013 lr: 0.02\n",
      "iteration: 228300 loss: 0.0013 lr: 0.02\n",
      "iteration: 228400 loss: 0.0012 lr: 0.02\n",
      "iteration: 228500 loss: 0.0013 lr: 0.02\n",
      "iteration: 228600 loss: 0.0012 lr: 0.02\n",
      "iteration: 228700 loss: 0.0014 lr: 0.02\n",
      "iteration: 228800 loss: 0.0013 lr: 0.02\n",
      "iteration: 228900 loss: 0.0013 lr: 0.02\n",
      "iteration: 229000 loss: 0.0012 lr: 0.02\n",
      "iteration: 229100 loss: 0.0013 lr: 0.02\n",
      "iteration: 229200 loss: 0.0012 lr: 0.02\n",
      "iteration: 229300 loss: 0.0013 lr: 0.02\n",
      "iteration: 229400 loss: 0.0013 lr: 0.02\n",
      "iteration: 229500 loss: 0.0013 lr: 0.02\n",
      "iteration: 229600 loss: 0.0013 lr: 0.02\n",
      "iteration: 229700 loss: 0.0012 lr: 0.02\n",
      "iteration: 229800 loss: 0.0011 lr: 0.02\n",
      "iteration: 229900 loss: 0.0014 lr: 0.02\n",
      "iteration: 230000 loss: 0.0012 lr: 0.02\n",
      "iteration: 230100 loss: 0.0013 lr: 0.02\n",
      "iteration: 230200 loss: 0.0012 lr: 0.02\n",
      "iteration: 230300 loss: 0.0013 lr: 0.02\n",
      "iteration: 230400 loss: 0.0013 lr: 0.02\n",
      "iteration: 230500 loss: 0.0014 lr: 0.02\n",
      "iteration: 230600 loss: 0.0013 lr: 0.02\n",
      "iteration: 230700 loss: 0.0013 lr: 0.02\n",
      "iteration: 230800 loss: 0.0013 lr: 0.02\n",
      "iteration: 230900 loss: 0.0012 lr: 0.02\n",
      "iteration: 231000 loss: 0.0011 lr: 0.02\n",
      "iteration: 231100 loss: 0.0013 lr: 0.02\n",
      "iteration: 231200 loss: 0.0012 lr: 0.02\n",
      "iteration: 231300 loss: 0.0012 lr: 0.02\n",
      "iteration: 231400 loss: 0.0012 lr: 0.02\n",
      "iteration: 231500 loss: 0.0012 lr: 0.02\n",
      "iteration: 231600 loss: 0.0012 lr: 0.02\n",
      "iteration: 231700 loss: 0.0013 lr: 0.02\n",
      "iteration: 231800 loss: 0.0012 lr: 0.02\n",
      "iteration: 231900 loss: 0.0013 lr: 0.02\n",
      "iteration: 232000 loss: 0.0014 lr: 0.02\n",
      "iteration: 232100 loss: 0.0014 lr: 0.02\n",
      "iteration: 232200 loss: 0.0013 lr: 0.02\n",
      "iteration: 232300 loss: 0.0013 lr: 0.02\n",
      "iteration: 232400 loss: 0.0014 lr: 0.02\n",
      "iteration: 232500 loss: 0.0013 lr: 0.02\n",
      "iteration: 232600 loss: 0.0013 lr: 0.02\n",
      "iteration: 232700 loss: 0.0012 lr: 0.02\n",
      "iteration: 232800 loss: 0.0012 lr: 0.02\n",
      "iteration: 232900 loss: 0.0012 lr: 0.02\n",
      "iteration: 233000 loss: 0.0014 lr: 0.02\n",
      "iteration: 233100 loss: 0.0013 lr: 0.02\n",
      "iteration: 233200 loss: 0.0013 lr: 0.02\n",
      "iteration: 233300 loss: 0.0013 lr: 0.02\n",
      "iteration: 233400 loss: 0.0012 lr: 0.02\n",
      "iteration: 233500 loss: 0.0013 lr: 0.02\n",
      "iteration: 233600 loss: 0.0012 lr: 0.02\n",
      "iteration: 233700 loss: 0.0013 lr: 0.02\n",
      "iteration: 233800 loss: 0.0013 lr: 0.02\n",
      "iteration: 233900 loss: 0.0013 lr: 0.02\n",
      "iteration: 234000 loss: 0.0012 lr: 0.02\n",
      "iteration: 234100 loss: 0.0012 lr: 0.02\n",
      "iteration: 234200 loss: 0.0012 lr: 0.02\n",
      "iteration: 234300 loss: 0.0012 lr: 0.02\n",
      "iteration: 234400 loss: 0.0012 lr: 0.02\n",
      "iteration: 234500 loss: 0.0013 lr: 0.02\n",
      "iteration: 234600 loss: 0.0013 lr: 0.02\n",
      "iteration: 234700 loss: 0.0013 lr: 0.02\n",
      "iteration: 234800 loss: 0.0012 lr: 0.02\n",
      "iteration: 234900 loss: 0.0012 lr: 0.02\n",
      "iteration: 235000 loss: 0.0012 lr: 0.02\n",
      "iteration: 235100 loss: 0.0013 lr: 0.02\n",
      "iteration: 235200 loss: 0.0011 lr: 0.02\n",
      "iteration: 235300 loss: 0.0012 lr: 0.02\n",
      "iteration: 235400 loss: 0.0012 lr: 0.02\n",
      "iteration: 235500 loss: 0.0012 lr: 0.02\n",
      "iteration: 235600 loss: 0.0014 lr: 0.02\n",
      "iteration: 235700 loss: 0.0012 lr: 0.02\n",
      "iteration: 235800 loss: 0.0012 lr: 0.02\n",
      "iteration: 235900 loss: 0.0014 lr: 0.02\n",
      "iteration: 236000 loss: 0.0012 lr: 0.02\n",
      "iteration: 236100 loss: 0.0012 lr: 0.02\n",
      "iteration: 236200 loss: 0.0014 lr: 0.02\n",
      "iteration: 236300 loss: 0.0013 lr: 0.02\n",
      "iteration: 236400 loss: 0.0013 lr: 0.02\n",
      "iteration: 236500 loss: 0.0012 lr: 0.02\n",
      "iteration: 236600 loss: 0.0012 lr: 0.02\n",
      "iteration: 236700 loss: 0.0013 lr: 0.02\n",
      "iteration: 236800 loss: 0.0012 lr: 0.02\n",
      "iteration: 236900 loss: 0.0012 lr: 0.02\n",
      "iteration: 237000 loss: 0.0012 lr: 0.02\n",
      "iteration: 237100 loss: 0.0012 lr: 0.02\n",
      "iteration: 237200 loss: 0.0013 lr: 0.02\n",
      "iteration: 237300 loss: 0.0013 lr: 0.02\n",
      "iteration: 237400 loss: 0.0013 lr: 0.02\n",
      "iteration: 237500 loss: 0.0014 lr: 0.02\n",
      "iteration: 237600 loss: 0.0012 lr: 0.02\n",
      "iteration: 237700 loss: 0.0013 lr: 0.02\n",
      "iteration: 237800 loss: 0.0012 lr: 0.02\n",
      "iteration: 237900 loss: 0.0014 lr: 0.02\n",
      "iteration: 238000 loss: 0.0014 lr: 0.02\n",
      "iteration: 238100 loss: 0.0012 lr: 0.02\n",
      "iteration: 238200 loss: 0.0014 lr: 0.02\n",
      "iteration: 238300 loss: 0.0013 lr: 0.02\n",
      "iteration: 238400 loss: 0.0013 lr: 0.02\n",
      "iteration: 238500 loss: 0.0012 lr: 0.02\n",
      "iteration: 238600 loss: 0.0014 lr: 0.02\n",
      "iteration: 238700 loss: 0.0012 lr: 0.02\n",
      "iteration: 238800 loss: 0.0014 lr: 0.02\n",
      "iteration: 238900 loss: 0.0013 lr: 0.02\n",
      "iteration: 239000 loss: 0.0014 lr: 0.02\n",
      "iteration: 239100 loss: 0.0012 lr: 0.02\n",
      "iteration: 239200 loss: 0.0012 lr: 0.02\n",
      "iteration: 239300 loss: 0.0011 lr: 0.02\n",
      "iteration: 239400 loss: 0.0013 lr: 0.02\n",
      "iteration: 239500 loss: 0.0012 lr: 0.02\n",
      "iteration: 239600 loss: 0.0013 lr: 0.02\n",
      "iteration: 239700 loss: 0.0012 lr: 0.02\n",
      "iteration: 239800 loss: 0.0012 lr: 0.02\n",
      "iteration: 239900 loss: 0.0012 lr: 0.02\n",
      "iteration: 240000 loss: 0.0012 lr: 0.02\n",
      "iteration: 240100 loss: 0.0012 lr: 0.02\n",
      "iteration: 240200 loss: 0.0012 lr: 0.02\n",
      "iteration: 240300 loss: 0.0011 lr: 0.02\n",
      "iteration: 240400 loss: 0.0013 lr: 0.02\n",
      "iteration: 240500 loss: 0.0013 lr: 0.02\n",
      "iteration: 240600 loss: 0.0012 lr: 0.02\n",
      "iteration: 240700 loss: 0.0014 lr: 0.02\n",
      "iteration: 240800 loss: 0.0013 lr: 0.02\n",
      "iteration: 240900 loss: 0.0013 lr: 0.02\n",
      "iteration: 241000 loss: 0.0013 lr: 0.02\n",
      "iteration: 241100 loss: 0.0014 lr: 0.02\n",
      "iteration: 241200 loss: 0.0013 lr: 0.02\n",
      "iteration: 241300 loss: 0.0012 lr: 0.02\n",
      "iteration: 241400 loss: 0.0013 lr: 0.02\n",
      "iteration: 241500 loss: 0.0013 lr: 0.02\n",
      "iteration: 241600 loss: 0.0013 lr: 0.02\n",
      "iteration: 241700 loss: 0.0013 lr: 0.02\n",
      "iteration: 241800 loss: 0.0012 lr: 0.02\n",
      "iteration: 241900 loss: 0.0012 lr: 0.02\n",
      "iteration: 242000 loss: 0.0014 lr: 0.02\n",
      "iteration: 242100 loss: 0.0013 lr: 0.02\n",
      "iteration: 242200 loss: 0.0013 lr: 0.02\n",
      "iteration: 242300 loss: 0.0011 lr: 0.02\n",
      "iteration: 242400 loss: 0.0014 lr: 0.02\n",
      "iteration: 242500 loss: 0.0013 lr: 0.02\n",
      "iteration: 242600 loss: 0.0012 lr: 0.02\n",
      "iteration: 242700 loss: 0.0012 lr: 0.02\n",
      "iteration: 242800 loss: 0.0011 lr: 0.02\n",
      "iteration: 242900 loss: 0.0012 lr: 0.02\n",
      "iteration: 243000 loss: 0.0014 lr: 0.02\n",
      "iteration: 243100 loss: 0.0013 lr: 0.02\n",
      "iteration: 243200 loss: 0.0014 lr: 0.02\n",
      "iteration: 243300 loss: 0.0013 lr: 0.02\n",
      "iteration: 243400 loss: 0.0011 lr: 0.02\n",
      "iteration: 243500 loss: 0.0014 lr: 0.02\n",
      "iteration: 243600 loss: 0.0014 lr: 0.02\n",
      "iteration: 243700 loss: 0.0012 lr: 0.02\n",
      "iteration: 243800 loss: 0.0011 lr: 0.02\n",
      "iteration: 243900 loss: 0.0011 lr: 0.02\n",
      "iteration: 244000 loss: 0.0012 lr: 0.02\n",
      "iteration: 244100 loss: 0.0013 lr: 0.02\n",
      "iteration: 244200 loss: 0.0011 lr: 0.02\n",
      "iteration: 244300 loss: 0.0012 lr: 0.02\n",
      "iteration: 244400 loss: 0.0013 lr: 0.02\n",
      "iteration: 244500 loss: 0.0013 lr: 0.02\n",
      "iteration: 244600 loss: 0.0012 lr: 0.02\n",
      "iteration: 244700 loss: 0.0013 lr: 0.02\n",
      "iteration: 244800 loss: 0.0013 lr: 0.02\n",
      "iteration: 244900 loss: 0.0013 lr: 0.02\n",
      "iteration: 245000 loss: 0.0013 lr: 0.02\n",
      "iteration: 245100 loss: 0.0012 lr: 0.02\n",
      "iteration: 245200 loss: 0.0015 lr: 0.02\n",
      "iteration: 245300 loss: 0.0013 lr: 0.02\n",
      "iteration: 245400 loss: 0.0013 lr: 0.02\n",
      "iteration: 245500 loss: 0.0013 lr: 0.02\n",
      "iteration: 245600 loss: 0.0012 lr: 0.02\n",
      "iteration: 245700 loss: 0.0013 lr: 0.02\n",
      "iteration: 245800 loss: 0.0013 lr: 0.02\n",
      "iteration: 245900 loss: 0.0013 lr: 0.02\n",
      "iteration: 246000 loss: 0.0012 lr: 0.02\n",
      "iteration: 246100 loss: 0.0013 lr: 0.02\n",
      "iteration: 246200 loss: 0.0012 lr: 0.02\n",
      "iteration: 246300 loss: 0.0013 lr: 0.02\n",
      "iteration: 246400 loss: 0.0012 lr: 0.02\n",
      "iteration: 246500 loss: 0.0013 lr: 0.02\n",
      "iteration: 246600 loss: 0.0013 lr: 0.02\n",
      "iteration: 246700 loss: 0.0013 lr: 0.02\n",
      "iteration: 246800 loss: 0.0012 lr: 0.02\n",
      "iteration: 246900 loss: 0.0011 lr: 0.02\n",
      "iteration: 247000 loss: 0.0013 lr: 0.02\n",
      "iteration: 247100 loss: 0.0013 lr: 0.02\n",
      "iteration: 247200 loss: 0.0014 lr: 0.02\n",
      "iteration: 247300 loss: 0.0012 lr: 0.02\n",
      "iteration: 247400 loss: 0.0014 lr: 0.02\n",
      "iteration: 247500 loss: 0.0013 lr: 0.02\n",
      "iteration: 247600 loss: 0.0013 lr: 0.02\n",
      "iteration: 247700 loss: 0.0013 lr: 0.02\n",
      "iteration: 247800 loss: 0.0013 lr: 0.02\n",
      "iteration: 247900 loss: 0.0013 lr: 0.02\n",
      "iteration: 248000 loss: 0.0012 lr: 0.02\n",
      "iteration: 248100 loss: 0.0011 lr: 0.02\n",
      "iteration: 248200 loss: 0.0012 lr: 0.02\n",
      "iteration: 248300 loss: 0.0014 lr: 0.02\n",
      "iteration: 248400 loss: 0.0013 lr: 0.02\n",
      "iteration: 248500 loss: 0.0013 lr: 0.02\n",
      "iteration: 248600 loss: 0.0010 lr: 0.02\n",
      "iteration: 248700 loss: 0.0013 lr: 0.02\n",
      "iteration: 248800 loss: 0.0013 lr: 0.02\n",
      "iteration: 248900 loss: 0.0013 lr: 0.02\n",
      "iteration: 249000 loss: 0.0014 lr: 0.02\n",
      "iteration: 249100 loss: 0.0013 lr: 0.02\n",
      "iteration: 249200 loss: 0.0013 lr: 0.02\n",
      "iteration: 249300 loss: 0.0012 lr: 0.02\n",
      "iteration: 249400 loss: 0.0014 lr: 0.02\n",
      "iteration: 249500 loss: 0.0012 lr: 0.02\n",
      "iteration: 249600 loss: 0.0011 lr: 0.02\n",
      "iteration: 249700 loss: 0.0012 lr: 0.02\n",
      "iteration: 249800 loss: 0.0011 lr: 0.02\n",
      "iteration: 249900 loss: 0.0012 lr: 0.02\n",
      "iteration: 250000 loss: 0.0012 lr: 0.02\n",
      "iteration: 250100 loss: 0.0014 lr: 0.02\n",
      "iteration: 250200 loss: 0.0013 lr: 0.02\n",
      "iteration: 250300 loss: 0.0012 lr: 0.02\n",
      "iteration: 250400 loss: 0.0013 lr: 0.02\n",
      "iteration: 250500 loss: 0.0012 lr: 0.02\n",
      "iteration: 250600 loss: 0.0012 lr: 0.02\n",
      "iteration: 250700 loss: 0.0013 lr: 0.02\n",
      "iteration: 250800 loss: 0.0012 lr: 0.02\n",
      "iteration: 250900 loss: 0.0011 lr: 0.02\n",
      "iteration: 251000 loss: 0.0013 lr: 0.02\n",
      "iteration: 251100 loss: 0.0012 lr: 0.02\n",
      "iteration: 251200 loss: 0.0013 lr: 0.02\n",
      "iteration: 251300 loss: 0.0013 lr: 0.02\n",
      "iteration: 251400 loss: 0.0013 lr: 0.02\n",
      "iteration: 251500 loss: 0.0012 lr: 0.02\n",
      "iteration: 251600 loss: 0.0013 lr: 0.02\n",
      "iteration: 251700 loss: 0.0013 lr: 0.02\n",
      "iteration: 251800 loss: 0.0013 lr: 0.02\n",
      "iteration: 251900 loss: 0.0013 lr: 0.02\n",
      "iteration: 252000 loss: 0.0012 lr: 0.02\n",
      "iteration: 252100 loss: 0.0013 lr: 0.02\n",
      "iteration: 252200 loss: 0.0012 lr: 0.02\n",
      "iteration: 252300 loss: 0.0011 lr: 0.02\n",
      "iteration: 252400 loss: 0.0012 lr: 0.02\n",
      "iteration: 252500 loss: 0.0013 lr: 0.02\n",
      "iteration: 252600 loss: 0.0014 lr: 0.02\n",
      "iteration: 252700 loss: 0.0013 lr: 0.02\n",
      "iteration: 252800 loss: 0.0012 lr: 0.02\n",
      "iteration: 252900 loss: 0.0013 lr: 0.02\n",
      "iteration: 253000 loss: 0.0012 lr: 0.02\n",
      "iteration: 253100 loss: 0.0012 lr: 0.02\n",
      "iteration: 253200 loss: 0.0012 lr: 0.02\n",
      "iteration: 253300 loss: 0.0011 lr: 0.02\n",
      "iteration: 253400 loss: 0.0013 lr: 0.02\n",
      "iteration: 253500 loss: 0.0013 lr: 0.02\n",
      "iteration: 253600 loss: 0.0012 lr: 0.02\n",
      "iteration: 253700 loss: 0.0012 lr: 0.02\n",
      "iteration: 253800 loss: 0.0012 lr: 0.02\n",
      "iteration: 253900 loss: 0.0012 lr: 0.02\n",
      "iteration: 254000 loss: 0.0013 lr: 0.02\n",
      "iteration: 254100 loss: 0.0013 lr: 0.02\n",
      "iteration: 254200 loss: 0.0013 lr: 0.02\n",
      "iteration: 254300 loss: 0.0013 lr: 0.02\n",
      "iteration: 254400 loss: 0.0013 lr: 0.02\n",
      "iteration: 254500 loss: 0.0012 lr: 0.02\n",
      "iteration: 254600 loss: 0.0013 lr: 0.02\n",
      "iteration: 254700 loss: 0.0014 lr: 0.02\n",
      "iteration: 254800 loss: 0.0013 lr: 0.02\n",
      "iteration: 254900 loss: 0.0013 lr: 0.02\n",
      "iteration: 255000 loss: 0.0014 lr: 0.02\n",
      "iteration: 255100 loss: 0.0012 lr: 0.02\n",
      "iteration: 255200 loss: 0.0013 lr: 0.02\n",
      "iteration: 255300 loss: 0.0013 lr: 0.02\n",
      "iteration: 255400 loss: 0.0011 lr: 0.02\n",
      "iteration: 255500 loss: 0.0014 lr: 0.02\n",
      "iteration: 255600 loss: 0.0012 lr: 0.02\n",
      "iteration: 255700 loss: 0.0011 lr: 0.02\n",
      "iteration: 255800 loss: 0.0013 lr: 0.02\n",
      "iteration: 255900 loss: 0.0012 lr: 0.02\n",
      "iteration: 256000 loss: 0.0013 lr: 0.02\n",
      "iteration: 256100 loss: 0.0012 lr: 0.02\n",
      "iteration: 256200 loss: 0.0013 lr: 0.02\n",
      "iteration: 256300 loss: 0.0013 lr: 0.02\n",
      "iteration: 256400 loss: 0.0011 lr: 0.02\n",
      "iteration: 256500 loss: 0.0013 lr: 0.02\n",
      "iteration: 256600 loss: 0.0013 lr: 0.02\n",
      "iteration: 256700 loss: 0.0013 lr: 0.02\n",
      "iteration: 256800 loss: 0.0012 lr: 0.02\n",
      "iteration: 256900 loss: 0.0012 lr: 0.02\n",
      "iteration: 257000 loss: 0.0013 lr: 0.02\n",
      "iteration: 257100 loss: 0.0014 lr: 0.02\n",
      "iteration: 257200 loss: 0.0014 lr: 0.02\n",
      "iteration: 257300 loss: 0.0013 lr: 0.02\n",
      "iteration: 257400 loss: 0.0012 lr: 0.02\n",
      "iteration: 257500 loss: 0.0012 lr: 0.02\n",
      "iteration: 257600 loss: 0.0012 lr: 0.02\n",
      "iteration: 257700 loss: 0.0013 lr: 0.02\n",
      "iteration: 257800 loss: 0.0014 lr: 0.02\n",
      "iteration: 257900 loss: 0.0011 lr: 0.02\n",
      "iteration: 258000 loss: 0.0013 lr: 0.02\n",
      "iteration: 258100 loss: 0.0013 lr: 0.02\n",
      "iteration: 258200 loss: 0.0013 lr: 0.02\n",
      "iteration: 258300 loss: 0.0012 lr: 0.02\n",
      "iteration: 258400 loss: 0.0011 lr: 0.02\n",
      "iteration: 258500 loss: 0.0012 lr: 0.02\n",
      "iteration: 258600 loss: 0.0012 lr: 0.02\n",
      "iteration: 258700 loss: 0.0013 lr: 0.02\n",
      "iteration: 258800 loss: 0.0012 lr: 0.02\n",
      "iteration: 258900 loss: 0.0011 lr: 0.02\n",
      "iteration: 259000 loss: 0.0012 lr: 0.02\n",
      "iteration: 259100 loss: 0.0011 lr: 0.02\n",
      "iteration: 259200 loss: 0.0013 lr: 0.02\n",
      "iteration: 259300 loss: 0.0013 lr: 0.02\n",
      "iteration: 259400 loss: 0.0011 lr: 0.02\n",
      "iteration: 259500 loss: 0.0012 lr: 0.02\n",
      "iteration: 259600 loss: 0.0013 lr: 0.02\n",
      "iteration: 259700 loss: 0.0013 lr: 0.02\n",
      "iteration: 259800 loss: 0.0011 lr: 0.02\n",
      "iteration: 259900 loss: 0.0012 lr: 0.02\n",
      "iteration: 260000 loss: 0.0012 lr: 0.02\n",
      "iteration: 260100 loss: 0.0012 lr: 0.02\n",
      "iteration: 260200 loss: 0.0012 lr: 0.02\n",
      "iteration: 260300 loss: 0.0011 lr: 0.02\n",
      "iteration: 260400 loss: 0.0013 lr: 0.02\n",
      "iteration: 260500 loss: 0.0012 lr: 0.02\n",
      "iteration: 260600 loss: 0.0013 lr: 0.02\n",
      "iteration: 260700 loss: 0.0013 lr: 0.02\n",
      "iteration: 260800 loss: 0.0014 lr: 0.02\n",
      "iteration: 260900 loss: 0.0012 lr: 0.02\n",
      "iteration: 261000 loss: 0.0012 lr: 0.02\n",
      "iteration: 261100 loss: 0.0013 lr: 0.02\n",
      "iteration: 261200 loss: 0.0013 lr: 0.02\n",
      "iteration: 261300 loss: 0.0013 lr: 0.02\n",
      "iteration: 261400 loss: 0.0012 lr: 0.02\n",
      "iteration: 261500 loss: 0.0013 lr: 0.02\n",
      "iteration: 261600 loss: 0.0012 lr: 0.02\n",
      "iteration: 261700 loss: 0.0012 lr: 0.02\n",
      "iteration: 261800 loss: 0.0011 lr: 0.02\n",
      "iteration: 261900 loss: 0.0013 lr: 0.02\n",
      "iteration: 262000 loss: 0.0013 lr: 0.02\n",
      "iteration: 262100 loss: 0.0012 lr: 0.02\n",
      "iteration: 262200 loss: 0.0012 lr: 0.02\n",
      "iteration: 262300 loss: 0.0012 lr: 0.02\n",
      "iteration: 262400 loss: 0.0012 lr: 0.02\n",
      "iteration: 262500 loss: 0.0011 lr: 0.02\n",
      "iteration: 262600 loss: 0.0013 lr: 0.02\n",
      "iteration: 262700 loss: 0.0012 lr: 0.02\n",
      "iteration: 262800 loss: 0.0013 lr: 0.02\n",
      "iteration: 262900 loss: 0.0012 lr: 0.02\n",
      "iteration: 263000 loss: 0.0013 lr: 0.02\n",
      "iteration: 263100 loss: 0.0013 lr: 0.02\n",
      "iteration: 263200 loss: 0.0013 lr: 0.02\n",
      "iteration: 263300 loss: 0.0013 lr: 0.02\n",
      "iteration: 263400 loss: 0.0011 lr: 0.02\n",
      "iteration: 263500 loss: 0.0014 lr: 0.02\n",
      "iteration: 263600 loss: 0.0012 lr: 0.02\n",
      "iteration: 263700 loss: 0.0012 lr: 0.02\n",
      "iteration: 263800 loss: 0.0012 lr: 0.02\n",
      "iteration: 263900 loss: 0.0012 lr: 0.02\n",
      "iteration: 264000 loss: 0.0011 lr: 0.02\n",
      "iteration: 264100 loss: 0.0014 lr: 0.02\n",
      "iteration: 264200 loss: 0.0014 lr: 0.02\n",
      "iteration: 264300 loss: 0.0013 lr: 0.02\n",
      "iteration: 264400 loss: 0.0014 lr: 0.02\n",
      "iteration: 264500 loss: 0.0013 lr: 0.02\n",
      "iteration: 264600 loss: 0.0012 lr: 0.02\n",
      "iteration: 264700 loss: 0.0011 lr: 0.02\n",
      "iteration: 264800 loss: 0.0013 lr: 0.02\n",
      "iteration: 264900 loss: 0.0013 lr: 0.02\n",
      "iteration: 265000 loss: 0.0013 lr: 0.02\n",
      "iteration: 265100 loss: 0.0012 lr: 0.02\n",
      "iteration: 265200 loss: 0.0013 lr: 0.02\n",
      "iteration: 265300 loss: 0.0013 lr: 0.02\n",
      "iteration: 265400 loss: 0.0012 lr: 0.02\n",
      "iteration: 265500 loss: 0.0012 lr: 0.02\n",
      "iteration: 265600 loss: 0.0013 lr: 0.02\n",
      "iteration: 265700 loss: 0.0012 lr: 0.02\n",
      "iteration: 265800 loss: 0.0013 lr: 0.02\n",
      "iteration: 265900 loss: 0.0012 lr: 0.02\n",
      "iteration: 266000 loss: 0.0012 lr: 0.02\n",
      "iteration: 266100 loss: 0.0014 lr: 0.02\n",
      "iteration: 266200 loss: 0.0011 lr: 0.02\n",
      "iteration: 266300 loss: 0.0012 lr: 0.02\n",
      "iteration: 266400 loss: 0.0013 lr: 0.02\n",
      "iteration: 266500 loss: 0.0012 lr: 0.02\n",
      "iteration: 266600 loss: 0.0014 lr: 0.02\n",
      "iteration: 266700 loss: 0.0013 lr: 0.02\n",
      "iteration: 266800 loss: 0.0012 lr: 0.02\n",
      "iteration: 266900 loss: 0.0013 lr: 0.02\n",
      "iteration: 267000 loss: 0.0012 lr: 0.02\n",
      "iteration: 267100 loss: 0.0013 lr: 0.02\n",
      "iteration: 267200 loss: 0.0012 lr: 0.02\n",
      "iteration: 267300 loss: 0.0013 lr: 0.02\n",
      "iteration: 267400 loss: 0.0014 lr: 0.02\n",
      "iteration: 267500 loss: 0.0014 lr: 0.02\n",
      "iteration: 267600 loss: 0.0011 lr: 0.02\n",
      "iteration: 267700 loss: 0.0012 lr: 0.02\n",
      "iteration: 267800 loss: 0.0012 lr: 0.02\n",
      "iteration: 267900 loss: 0.0012 lr: 0.02\n",
      "iteration: 268000 loss: 0.0012 lr: 0.02\n",
      "iteration: 268100 loss: 0.0012 lr: 0.02\n",
      "iteration: 268200 loss: 0.0014 lr: 0.02\n",
      "iteration: 268300 loss: 0.0013 lr: 0.02\n",
      "iteration: 268400 loss: 0.0012 lr: 0.02\n",
      "iteration: 268500 loss: 0.0013 lr: 0.02\n",
      "iteration: 268600 loss: 0.0012 lr: 0.02\n",
      "iteration: 268700 loss: 0.0012 lr: 0.02\n",
      "iteration: 268800 loss: 0.0011 lr: 0.02\n",
      "iteration: 268900 loss: 0.0013 lr: 0.02\n",
      "iteration: 269000 loss: 0.0011 lr: 0.02\n",
      "iteration: 269100 loss: 0.0014 lr: 0.02\n",
      "iteration: 269200 loss: 0.0012 lr: 0.02\n",
      "iteration: 269300 loss: 0.0013 lr: 0.02\n",
      "iteration: 269400 loss: 0.0012 lr: 0.02\n",
      "iteration: 269500 loss: 0.0011 lr: 0.02\n",
      "iteration: 269600 loss: 0.0012 lr: 0.02\n",
      "iteration: 269700 loss: 0.0012 lr: 0.02\n",
      "iteration: 269800 loss: 0.0013 lr: 0.02\n",
      "iteration: 269900 loss: 0.0012 lr: 0.02\n",
      "iteration: 270000 loss: 0.0013 lr: 0.02\n",
      "iteration: 270100 loss: 0.0011 lr: 0.02\n",
      "iteration: 270200 loss: 0.0013 lr: 0.02\n",
      "iteration: 270300 loss: 0.0012 lr: 0.02\n",
      "iteration: 270400 loss: 0.0011 lr: 0.02\n",
      "iteration: 270500 loss: 0.0012 lr: 0.02\n",
      "iteration: 270600 loss: 0.0013 lr: 0.02\n",
      "iteration: 270700 loss: 0.0011 lr: 0.02\n",
      "iteration: 270800 loss: 0.0012 lr: 0.02\n",
      "iteration: 270900 loss: 0.0012 lr: 0.02\n",
      "iteration: 271000 loss: 0.0012 lr: 0.02\n",
      "iteration: 271100 loss: 0.0012 lr: 0.02\n",
      "iteration: 271200 loss: 0.0011 lr: 0.02\n",
      "iteration: 271300 loss: 0.0013 lr: 0.02\n",
      "iteration: 271400 loss: 0.0011 lr: 0.02\n",
      "iteration: 271500 loss: 0.0013 lr: 0.02\n",
      "iteration: 271600 loss: 0.0012 lr: 0.02\n",
      "iteration: 271700 loss: 0.0011 lr: 0.02\n",
      "iteration: 271800 loss: 0.0013 lr: 0.02\n",
      "iteration: 271900 loss: 0.0012 lr: 0.02\n",
      "iteration: 272000 loss: 0.0012 lr: 0.02\n",
      "iteration: 272100 loss: 0.0013 lr: 0.02\n",
      "iteration: 272200 loss: 0.0013 lr: 0.02\n",
      "iteration: 272300 loss: 0.0012 lr: 0.02\n",
      "iteration: 272400 loss: 0.0012 lr: 0.02\n",
      "iteration: 272500 loss: 0.0012 lr: 0.02\n",
      "iteration: 272600 loss: 0.0012 lr: 0.02\n",
      "iteration: 272700 loss: 0.0012 lr: 0.02\n",
      "iteration: 272800 loss: 0.0012 lr: 0.02\n",
      "iteration: 272900 loss: 0.0013 lr: 0.02\n",
      "iteration: 273000 loss: 0.0013 lr: 0.02\n",
      "iteration: 273100 loss: 0.0012 lr: 0.02\n",
      "iteration: 273200 loss: 0.0011 lr: 0.02\n",
      "iteration: 273300 loss: 0.0012 lr: 0.02\n",
      "iteration: 273400 loss: 0.0014 lr: 0.02\n",
      "iteration: 273500 loss: 0.0012 lr: 0.02\n",
      "iteration: 273600 loss: 0.0013 lr: 0.02\n",
      "iteration: 273700 loss: 0.0013 lr: 0.02\n",
      "iteration: 273800 loss: 0.0013 lr: 0.02\n",
      "iteration: 273900 loss: 0.0012 lr: 0.02\n",
      "iteration: 274000 loss: 0.0013 lr: 0.02\n",
      "iteration: 274100 loss: 0.0012 lr: 0.02\n",
      "iteration: 274200 loss: 0.0013 lr: 0.02\n",
      "iteration: 274300 loss: 0.0012 lr: 0.02\n",
      "iteration: 274400 loss: 0.0011 lr: 0.02\n",
      "iteration: 274500 loss: 0.0013 lr: 0.02\n",
      "iteration: 274600 loss: 0.0013 lr: 0.02\n",
      "iteration: 274700 loss: 0.0013 lr: 0.02\n",
      "iteration: 274800 loss: 0.0014 lr: 0.02\n",
      "iteration: 274900 loss: 0.0013 lr: 0.02\n",
      "iteration: 275000 loss: 0.0013 lr: 0.02\n",
      "iteration: 275100 loss: 0.0013 lr: 0.02\n",
      "iteration: 275200 loss: 0.0011 lr: 0.02\n",
      "iteration: 275300 loss: 0.0012 lr: 0.02\n",
      "iteration: 275400 loss: 0.0011 lr: 0.02\n",
      "iteration: 275500 loss: 0.0012 lr: 0.02\n",
      "iteration: 275600 loss: 0.0012 lr: 0.02\n",
      "iteration: 275700 loss: 0.0013 lr: 0.02\n",
      "iteration: 275800 loss: 0.0014 lr: 0.02\n",
      "iteration: 275900 loss: 0.0012 lr: 0.02\n",
      "iteration: 276000 loss: 0.0012 lr: 0.02\n",
      "iteration: 276100 loss: 0.0011 lr: 0.02\n",
      "iteration: 276200 loss: 0.0011 lr: 0.02\n",
      "iteration: 276300 loss: 0.0012 lr: 0.02\n",
      "iteration: 276400 loss: 0.0012 lr: 0.02\n",
      "iteration: 276500 loss: 0.0013 lr: 0.02\n",
      "iteration: 276600 loss: 0.0011 lr: 0.02\n",
      "iteration: 276700 loss: 0.0013 lr: 0.02\n",
      "iteration: 276800 loss: 0.0013 lr: 0.02\n",
      "iteration: 276900 loss: 0.0011 lr: 0.02\n",
      "iteration: 277000 loss: 0.0012 lr: 0.02\n",
      "iteration: 277100 loss: 0.0012 lr: 0.02\n",
      "iteration: 277200 loss: 0.0013 lr: 0.02\n",
      "iteration: 277300 loss: 0.0012 lr: 0.02\n",
      "iteration: 277400 loss: 0.0012 lr: 0.02\n",
      "iteration: 277500 loss: 0.0012 lr: 0.02\n",
      "iteration: 277600 loss: 0.0013 lr: 0.02\n",
      "iteration: 277700 loss: 0.0013 lr: 0.02\n",
      "iteration: 277800 loss: 0.0014 lr: 0.02\n",
      "iteration: 277900 loss: 0.0012 lr: 0.02\n",
      "iteration: 278000 loss: 0.0012 lr: 0.02\n",
      "iteration: 278100 loss: 0.0012 lr: 0.02\n",
      "iteration: 278200 loss: 0.0012 lr: 0.02\n",
      "iteration: 278300 loss: 0.0012 lr: 0.02\n",
      "iteration: 278400 loss: 0.0011 lr: 0.02\n",
      "iteration: 278500 loss: 0.0012 lr: 0.02\n",
      "iteration: 278600 loss: 0.0011 lr: 0.02\n",
      "iteration: 278700 loss: 0.0011 lr: 0.02\n",
      "iteration: 278800 loss: 0.0012 lr: 0.02\n",
      "iteration: 278900 loss: 0.0012 lr: 0.02\n",
      "iteration: 279000 loss: 0.0011 lr: 0.02\n",
      "iteration: 279100 loss: 0.0012 lr: 0.02\n",
      "iteration: 279200 loss: 0.0013 lr: 0.02\n",
      "iteration: 279300 loss: 0.0013 lr: 0.02\n",
      "iteration: 279400 loss: 0.0011 lr: 0.02\n",
      "iteration: 279500 loss: 0.0012 lr: 0.02\n",
      "iteration: 279600 loss: 0.0011 lr: 0.02\n",
      "iteration: 279700 loss: 0.0012 lr: 0.02\n",
      "iteration: 279800 loss: 0.0012 lr: 0.02\n",
      "iteration: 279900 loss: 0.0013 lr: 0.02\n",
      "iteration: 280000 loss: 0.0012 lr: 0.02\n",
      "iteration: 280100 loss: 0.0012 lr: 0.02\n",
      "iteration: 280200 loss: 0.0011 lr: 0.02\n",
      "iteration: 280300 loss: 0.0012 lr: 0.02\n",
      "iteration: 280400 loss: 0.0013 lr: 0.02\n",
      "iteration: 280500 loss: 0.0013 lr: 0.02\n",
      "iteration: 280600 loss: 0.0012 lr: 0.02\n",
      "iteration: 280700 loss: 0.0011 lr: 0.02\n",
      "iteration: 280800 loss: 0.0012 lr: 0.02\n",
      "iteration: 280900 loss: 0.0012 lr: 0.02\n",
      "iteration: 281000 loss: 0.0012 lr: 0.02\n",
      "iteration: 281100 loss: 0.0012 lr: 0.02\n",
      "iteration: 281200 loss: 0.0012 lr: 0.02\n",
      "iteration: 281300 loss: 0.0013 lr: 0.02\n",
      "iteration: 281400 loss: 0.0013 lr: 0.02\n",
      "iteration: 281500 loss: 0.0011 lr: 0.02\n",
      "iteration: 281600 loss: 0.0013 lr: 0.02\n",
      "iteration: 281700 loss: 0.0012 lr: 0.02\n",
      "iteration: 281800 loss: 0.0012 lr: 0.02\n",
      "iteration: 281900 loss: 0.0013 lr: 0.02\n",
      "iteration: 282000 loss: 0.0011 lr: 0.02\n",
      "iteration: 282100 loss: 0.0011 lr: 0.02\n",
      "iteration: 282200 loss: 0.0013 lr: 0.02\n",
      "iteration: 282300 loss: 0.0013 lr: 0.02\n",
      "iteration: 282400 loss: 0.0012 lr: 0.02\n",
      "iteration: 282500 loss: 0.0012 lr: 0.02\n",
      "iteration: 282600 loss: 0.0011 lr: 0.02\n",
      "iteration: 282700 loss: 0.0011 lr: 0.02\n",
      "iteration: 282800 loss: 0.0013 lr: 0.02\n",
      "iteration: 282900 loss: 0.0012 lr: 0.02\n",
      "iteration: 283000 loss: 0.0012 lr: 0.02\n",
      "iteration: 283100 loss: 0.0011 lr: 0.02\n",
      "iteration: 283200 loss: 0.0013 lr: 0.02\n",
      "iteration: 283300 loss: 0.0012 lr: 0.02\n",
      "iteration: 283400 loss: 0.0012 lr: 0.02\n",
      "iteration: 283500 loss: 0.0012 lr: 0.02\n",
      "iteration: 283600 loss: 0.0012 lr: 0.02\n",
      "iteration: 283700 loss: 0.0012 lr: 0.02\n",
      "iteration: 283800 loss: 0.0012 lr: 0.02\n",
      "iteration: 283900 loss: 0.0012 lr: 0.02\n",
      "iteration: 284000 loss: 0.0011 lr: 0.02\n",
      "iteration: 284100 loss: 0.0011 lr: 0.02\n",
      "iteration: 284200 loss: 0.0011 lr: 0.02\n",
      "iteration: 284300 loss: 0.0012 lr: 0.02\n",
      "iteration: 284400 loss: 0.0012 lr: 0.02\n",
      "iteration: 284500 loss: 0.0013 lr: 0.02\n",
      "iteration: 284600 loss: 0.0014 lr: 0.02\n",
      "iteration: 284700 loss: 0.0013 lr: 0.02\n",
      "iteration: 284800 loss: 0.0014 lr: 0.02\n",
      "iteration: 284900 loss: 0.0012 lr: 0.02\n",
      "iteration: 285000 loss: 0.0011 lr: 0.02\n",
      "iteration: 285100 loss: 0.0011 lr: 0.02\n",
      "iteration: 285200 loss: 0.0013 lr: 0.02\n",
      "iteration: 285300 loss: 0.0012 lr: 0.02\n",
      "iteration: 285400 loss: 0.0012 lr: 0.02\n",
      "iteration: 285500 loss: 0.0010 lr: 0.02\n",
      "iteration: 285600 loss: 0.0014 lr: 0.02\n",
      "iteration: 285700 loss: 0.0012 lr: 0.02\n",
      "iteration: 285800 loss: 0.0012 lr: 0.02\n",
      "iteration: 285900 loss: 0.0012 lr: 0.02\n",
      "iteration: 286000 loss: 0.0013 lr: 0.02\n",
      "iteration: 286100 loss: 0.0014 lr: 0.02\n",
      "iteration: 286200 loss: 0.0013 lr: 0.02\n",
      "iteration: 286300 loss: 0.0011 lr: 0.02\n",
      "iteration: 286400 loss: 0.0013 lr: 0.02\n",
      "iteration: 286500 loss: 0.0013 lr: 0.02\n",
      "iteration: 286600 loss: 0.0012 lr: 0.02\n",
      "iteration: 286700 loss: 0.0013 lr: 0.02\n",
      "iteration: 286800 loss: 0.0013 lr: 0.02\n",
      "iteration: 286900 loss: 0.0012 lr: 0.02\n",
      "iteration: 287000 loss: 0.0013 lr: 0.02\n",
      "iteration: 287100 loss: 0.0013 lr: 0.02\n",
      "iteration: 287200 loss: 0.0012 lr: 0.02\n",
      "iteration: 287300 loss: 0.0010 lr: 0.02\n",
      "iteration: 287400 loss: 0.0012 lr: 0.02\n",
      "iteration: 287500 loss: 0.0011 lr: 0.02\n",
      "iteration: 287600 loss: 0.0013 lr: 0.02\n",
      "iteration: 287700 loss: 0.0011 lr: 0.02\n",
      "iteration: 287800 loss: 0.0012 lr: 0.02\n",
      "iteration: 287900 loss: 0.0011 lr: 0.02\n",
      "iteration: 288000 loss: 0.0013 lr: 0.02\n",
      "iteration: 288100 loss: 0.0011 lr: 0.02\n",
      "iteration: 288200 loss: 0.0012 lr: 0.02\n",
      "iteration: 288300 loss: 0.0012 lr: 0.02\n",
      "iteration: 288400 loss: 0.0012 lr: 0.02\n",
      "iteration: 288500 loss: 0.0010 lr: 0.02\n",
      "iteration: 288600 loss: 0.0011 lr: 0.02\n",
      "iteration: 288700 loss: 0.0013 lr: 0.02\n",
      "iteration: 288800 loss: 0.0013 lr: 0.02\n",
      "iteration: 288900 loss: 0.0013 lr: 0.02\n",
      "iteration: 289000 loss: 0.0013 lr: 0.02\n",
      "iteration: 289100 loss: 0.0012 lr: 0.02\n",
      "iteration: 289200 loss: 0.0013 lr: 0.02\n",
      "iteration: 289300 loss: 0.0012 lr: 0.02\n",
      "iteration: 289400 loss: 0.0012 lr: 0.02\n",
      "iteration: 289500 loss: 0.0011 lr: 0.02\n",
      "iteration: 289600 loss: 0.0010 lr: 0.02\n",
      "iteration: 289700 loss: 0.0013 lr: 0.02\n",
      "iteration: 289800 loss: 0.0012 lr: 0.02\n",
      "iteration: 289900 loss: 0.0012 lr: 0.02\n",
      "iteration: 290000 loss: 0.0012 lr: 0.02\n",
      "iteration: 290100 loss: 0.0012 lr: 0.02\n",
      "iteration: 290200 loss: 0.0012 lr: 0.02\n",
      "iteration: 290300 loss: 0.0012 lr: 0.02\n",
      "iteration: 290400 loss: 0.0011 lr: 0.02\n",
      "iteration: 290500 loss: 0.0012 lr: 0.02\n",
      "iteration: 290600 loss: 0.0012 lr: 0.02\n",
      "iteration: 290700 loss: 0.0012 lr: 0.02\n",
      "iteration: 290800 loss: 0.0013 lr: 0.02\n",
      "iteration: 290900 loss: 0.0013 lr: 0.02\n",
      "iteration: 291000 loss: 0.0013 lr: 0.02\n",
      "iteration: 291100 loss: 0.0012 lr: 0.02\n",
      "iteration: 291200 loss: 0.0011 lr: 0.02\n",
      "iteration: 291300 loss: 0.0012 lr: 0.02\n",
      "iteration: 291400 loss: 0.0012 lr: 0.02\n",
      "iteration: 291500 loss: 0.0010 lr: 0.02\n",
      "iteration: 291600 loss: 0.0013 lr: 0.02\n",
      "iteration: 291700 loss: 0.0013 lr: 0.02\n",
      "iteration: 291800 loss: 0.0012 lr: 0.02\n",
      "iteration: 291900 loss: 0.0013 lr: 0.02\n",
      "iteration: 292000 loss: 0.0012 lr: 0.02\n",
      "iteration: 292100 loss: 0.0011 lr: 0.02\n",
      "iteration: 292200 loss: 0.0012 lr: 0.02\n",
      "iteration: 292300 loss: 0.0014 lr: 0.02\n",
      "iteration: 292400 loss: 0.0012 lr: 0.02\n",
      "iteration: 292500 loss: 0.0012 lr: 0.02\n",
      "iteration: 292600 loss: 0.0011 lr: 0.02\n",
      "iteration: 292700 loss: 0.0011 lr: 0.02\n",
      "iteration: 292800 loss: 0.0012 lr: 0.02\n",
      "iteration: 292900 loss: 0.0013 lr: 0.02\n",
      "iteration: 293000 loss: 0.0012 lr: 0.02\n",
      "iteration: 293100 loss: 0.0012 lr: 0.02\n",
      "iteration: 293200 loss: 0.0015 lr: 0.02\n",
      "iteration: 293300 loss: 0.0013 lr: 0.02\n",
      "iteration: 293400 loss: 0.0012 lr: 0.02\n",
      "iteration: 293500 loss: 0.0012 lr: 0.02\n",
      "iteration: 293600 loss: 0.0012 lr: 0.02\n",
      "iteration: 293700 loss: 0.0012 lr: 0.02\n",
      "iteration: 293800 loss: 0.0012 lr: 0.02\n",
      "iteration: 293900 loss: 0.0012 lr: 0.02\n",
      "iteration: 294000 loss: 0.0012 lr: 0.02\n",
      "iteration: 294100 loss: 0.0012 lr: 0.02\n",
      "iteration: 294200 loss: 0.0012 lr: 0.02\n",
      "iteration: 294300 loss: 0.0010 lr: 0.02\n",
      "iteration: 294400 loss: 0.0011 lr: 0.02\n",
      "iteration: 294500 loss: 0.0012 lr: 0.02\n",
      "iteration: 294600 loss: 0.0011 lr: 0.02\n",
      "iteration: 294700 loss: 0.0012 lr: 0.02\n",
      "iteration: 294800 loss: 0.0013 lr: 0.02\n",
      "iteration: 294900 loss: 0.0011 lr: 0.02\n",
      "iteration: 295000 loss: 0.0011 lr: 0.02\n",
      "iteration: 295100 loss: 0.0011 lr: 0.02\n",
      "iteration: 295200 loss: 0.0012 lr: 0.02\n",
      "iteration: 295300 loss: 0.0012 lr: 0.02\n",
      "iteration: 295400 loss: 0.0012 lr: 0.02\n",
      "iteration: 295500 loss: 0.0012 lr: 0.02\n",
      "iteration: 295600 loss: 0.0013 lr: 0.02\n",
      "iteration: 295700 loss: 0.0011 lr: 0.02\n",
      "iteration: 295800 loss: 0.0012 lr: 0.02\n",
      "iteration: 295900 loss: 0.0012 lr: 0.02\n",
      "iteration: 296000 loss: 0.0012 lr: 0.02\n",
      "iteration: 296100 loss: 0.0011 lr: 0.02\n",
      "iteration: 296200 loss: 0.0012 lr: 0.02\n",
      "iteration: 296300 loss: 0.0011 lr: 0.02\n",
      "iteration: 296400 loss: 0.0012 lr: 0.02\n",
      "iteration: 296500 loss: 0.0012 lr: 0.02\n",
      "iteration: 296600 loss: 0.0012 lr: 0.02\n",
      "iteration: 296700 loss: 0.0013 lr: 0.02\n",
      "iteration: 296800 loss: 0.0013 lr: 0.02\n",
      "iteration: 296900 loss: 0.0013 lr: 0.02\n",
      "iteration: 297000 loss: 0.0013 lr: 0.02\n",
      "iteration: 297100 loss: 0.0011 lr: 0.02\n",
      "iteration: 297200 loss: 0.0010 lr: 0.02\n",
      "iteration: 297300 loss: 0.0012 lr: 0.02\n",
      "iteration: 297400 loss: 0.0012 lr: 0.02\n",
      "iteration: 297500 loss: 0.0012 lr: 0.02\n",
      "iteration: 297600 loss: 0.0012 lr: 0.02\n",
      "iteration: 297700 loss: 0.0012 lr: 0.02\n",
      "iteration: 297800 loss: 0.0011 lr: 0.02\n",
      "iteration: 297900 loss: 0.0012 lr: 0.02\n",
      "iteration: 298000 loss: 0.0013 lr: 0.02\n",
      "iteration: 298100 loss: 0.0012 lr: 0.02\n",
      "iteration: 298200 loss: 0.0011 lr: 0.02\n",
      "iteration: 298300 loss: 0.0013 lr: 0.02\n",
      "iteration: 298400 loss: 0.0013 lr: 0.02\n",
      "iteration: 298500 loss: 0.0013 lr: 0.02\n",
      "iteration: 298600 loss: 0.0011 lr: 0.02\n",
      "iteration: 298700 loss: 0.0012 lr: 0.02\n",
      "iteration: 298800 loss: 0.0011 lr: 0.02\n",
      "iteration: 298900 loss: 0.0012 lr: 0.02\n",
      "iteration: 299000 loss: 0.0011 lr: 0.02\n",
      "iteration: 299100 loss: 0.0012 lr: 0.02\n",
      "iteration: 299200 loss: 0.0011 lr: 0.02\n",
      "iteration: 299300 loss: 0.0012 lr: 0.02\n",
      "iteration: 299400 loss: 0.0011 lr: 0.02\n",
      "iteration: 299500 loss: 0.0013 lr: 0.02\n",
      "iteration: 299600 loss: 0.0011 lr: 0.02\n",
      "iteration: 299700 loss: 0.0013 lr: 0.02\n",
      "iteration: 299800 loss: 0.0011 lr: 0.02\n",
      "iteration: 299900 loss: 0.0011 lr: 0.02\n",
      "iteration: 300000 loss: 0.0011 lr: 0.02\n",
      "iteration: 300100 loss: 0.0012 lr: 0.02\n",
      "iteration: 300200 loss: 0.0012 lr: 0.02\n",
      "iteration: 300300 loss: 0.0011 lr: 0.02\n",
      "iteration: 300400 loss: 0.0012 lr: 0.02\n",
      "iteration: 300500 loss: 0.0012 lr: 0.02\n",
      "iteration: 300600 loss: 0.0012 lr: 0.02\n",
      "iteration: 300700 loss: 0.0012 lr: 0.02\n",
      "iteration: 300800 loss: 0.0011 lr: 0.02\n",
      "iteration: 300900 loss: 0.0013 lr: 0.02\n",
      "iteration: 301000 loss: 0.0011 lr: 0.02\n",
      "iteration: 301100 loss: 0.0012 lr: 0.02\n",
      "iteration: 301200 loss: 0.0012 lr: 0.02\n",
      "iteration: 301300 loss: 0.0012 lr: 0.02\n",
      "iteration: 301400 loss: 0.0012 lr: 0.02\n",
      "iteration: 301500 loss: 0.0012 lr: 0.02\n",
      "iteration: 301600 loss: 0.0013 lr: 0.02\n",
      "iteration: 301700 loss: 0.0012 lr: 0.02\n",
      "iteration: 301800 loss: 0.0011 lr: 0.02\n",
      "iteration: 301900 loss: 0.0013 lr: 0.02\n",
      "iteration: 302000 loss: 0.0012 lr: 0.02\n",
      "iteration: 302100 loss: 0.0011 lr: 0.02\n",
      "iteration: 302200 loss: 0.0012 lr: 0.02\n",
      "iteration: 302300 loss: 0.0012 lr: 0.02\n",
      "iteration: 302400 loss: 0.0013 lr: 0.02\n",
      "iteration: 302500 loss: 0.0012 lr: 0.02\n",
      "iteration: 302600 loss: 0.0012 lr: 0.02\n",
      "iteration: 302700 loss: 0.0013 lr: 0.02\n",
      "iteration: 302800 loss: 0.0011 lr: 0.02\n",
      "iteration: 302900 loss: 0.0012 lr: 0.02\n",
      "iteration: 303000 loss: 0.0011 lr: 0.02\n",
      "iteration: 303100 loss: 0.0013 lr: 0.02\n",
      "iteration: 303200 loss: 0.0011 lr: 0.02\n",
      "iteration: 303300 loss: 0.0012 lr: 0.02\n",
      "iteration: 303400 loss: 0.0013 lr: 0.02\n",
      "iteration: 303500 loss: 0.0012 lr: 0.02\n",
      "iteration: 303600 loss: 0.0013 lr: 0.02\n",
      "iteration: 303700 loss: 0.0011 lr: 0.02\n",
      "iteration: 303800 loss: 0.0011 lr: 0.02\n",
      "iteration: 303900 loss: 0.0012 lr: 0.02\n",
      "iteration: 304000 loss: 0.0012 lr: 0.02\n",
      "iteration: 304100 loss: 0.0012 lr: 0.02\n",
      "iteration: 304200 loss: 0.0014 lr: 0.02\n",
      "iteration: 304300 loss: 0.0013 lr: 0.02\n",
      "iteration: 304400 loss: 0.0012 lr: 0.02\n",
      "iteration: 304500 loss: 0.0011 lr: 0.02\n",
      "iteration: 304600 loss: 0.0011 lr: 0.02\n",
      "iteration: 304700 loss: 0.0013 lr: 0.02\n",
      "iteration: 304800 loss: 0.0013 lr: 0.02\n",
      "iteration: 304900 loss: 0.0012 lr: 0.02\n",
      "iteration: 305000 loss: 0.0013 lr: 0.02\n",
      "iteration: 305100 loss: 0.0012 lr: 0.02\n",
      "iteration: 305200 loss: 0.0011 lr: 0.02\n",
      "iteration: 305300 loss: 0.0012 lr: 0.02\n",
      "iteration: 305400 loss: 0.0014 lr: 0.02\n",
      "iteration: 305500 loss: 0.0011 lr: 0.02\n",
      "iteration: 305600 loss: 0.0012 lr: 0.02\n",
      "iteration: 305700 loss: 0.0011 lr: 0.02\n",
      "iteration: 305800 loss: 0.0012 lr: 0.02\n",
      "iteration: 305900 loss: 0.0011 lr: 0.02\n",
      "iteration: 306000 loss: 0.0011 lr: 0.02\n",
      "iteration: 306100 loss: 0.0012 lr: 0.02\n",
      "iteration: 306200 loss: 0.0011 lr: 0.02\n",
      "iteration: 306300 loss: 0.0010 lr: 0.02\n",
      "iteration: 306400 loss: 0.0012 lr: 0.02\n",
      "iteration: 306500 loss: 0.0011 lr: 0.02\n",
      "iteration: 306600 loss: 0.0012 lr: 0.02\n",
      "iteration: 306700 loss: 0.0013 lr: 0.02\n",
      "iteration: 306800 loss: 0.0012 lr: 0.02\n",
      "iteration: 306900 loss: 0.0013 lr: 0.02\n",
      "iteration: 307000 loss: 0.0012 lr: 0.02\n",
      "iteration: 307100 loss: 0.0012 lr: 0.02\n",
      "iteration: 307200 loss: 0.0012 lr: 0.02\n",
      "iteration: 307300 loss: 0.0011 lr: 0.02\n",
      "iteration: 307400 loss: 0.0012 lr: 0.02\n",
      "iteration: 307500 loss: 0.0012 lr: 0.02\n",
      "iteration: 307600 loss: 0.0011 lr: 0.02\n",
      "iteration: 307700 loss: 0.0013 lr: 0.02\n",
      "iteration: 307800 loss: 0.0012 lr: 0.02\n",
      "iteration: 307900 loss: 0.0012 lr: 0.02\n",
      "iteration: 308000 loss: 0.0012 lr: 0.02\n",
      "iteration: 308100 loss: 0.0011 lr: 0.02\n",
      "iteration: 308200 loss: 0.0011 lr: 0.02\n",
      "iteration: 308300 loss: 0.0012 lr: 0.02\n",
      "iteration: 308400 loss: 0.0013 lr: 0.02\n",
      "iteration: 308500 loss: 0.0013 lr: 0.02\n",
      "iteration: 308600 loss: 0.0013 lr: 0.02\n",
      "iteration: 308700 loss: 0.0012 lr: 0.02\n",
      "iteration: 308800 loss: 0.0012 lr: 0.02\n",
      "iteration: 308900 loss: 0.0011 lr: 0.02\n",
      "iteration: 309000 loss: 0.0012 lr: 0.02\n",
      "iteration: 309100 loss: 0.0011 lr: 0.02\n",
      "iteration: 309200 loss: 0.0010 lr: 0.02\n",
      "iteration: 309300 loss: 0.0012 lr: 0.02\n",
      "iteration: 309400 loss: 0.0012 lr: 0.02\n",
      "iteration: 309500 loss: 0.0011 lr: 0.02\n",
      "iteration: 309600 loss: 0.0012 lr: 0.02\n",
      "iteration: 309700 loss: 0.0012 lr: 0.02\n",
      "iteration: 309800 loss: 0.0012 lr: 0.02\n",
      "iteration: 309900 loss: 0.0012 lr: 0.02\n",
      "iteration: 310000 loss: 0.0011 lr: 0.02\n",
      "iteration: 310100 loss: 0.0012 lr: 0.02\n",
      "iteration: 310200 loss: 0.0013 lr: 0.02\n",
      "iteration: 310300 loss: 0.0011 lr: 0.02\n",
      "iteration: 310400 loss: 0.0013 lr: 0.02\n",
      "iteration: 310500 loss: 0.0012 lr: 0.02\n",
      "iteration: 310600 loss: 0.0015 lr: 0.02\n",
      "iteration: 310700 loss: 0.0012 lr: 0.02\n",
      "iteration: 310800 loss: 0.0010 lr: 0.02\n",
      "iteration: 310900 loss: 0.0011 lr: 0.02\n",
      "iteration: 311000 loss: 0.0011 lr: 0.02\n",
      "iteration: 311100 loss: 0.0013 lr: 0.02\n",
      "iteration: 311200 loss: 0.0013 lr: 0.02\n",
      "iteration: 311300 loss: 0.0010 lr: 0.02\n",
      "iteration: 311400 loss: 0.0012 lr: 0.02\n",
      "iteration: 311500 loss: 0.0012 lr: 0.02\n",
      "iteration: 311600 loss: 0.0012 lr: 0.02\n",
      "iteration: 311700 loss: 0.0012 lr: 0.02\n",
      "iteration: 311800 loss: 0.0012 lr: 0.02\n",
      "iteration: 311900 loss: 0.0011 lr: 0.02\n",
      "iteration: 312000 loss: 0.0011 lr: 0.02\n",
      "iteration: 312100 loss: 0.0012 lr: 0.02\n",
      "iteration: 312200 loss: 0.0011 lr: 0.02\n",
      "iteration: 312300 loss: 0.0011 lr: 0.02\n",
      "iteration: 312400 loss: 0.0012 lr: 0.02\n",
      "iteration: 312500 loss: 0.0012 lr: 0.02\n",
      "iteration: 312600 loss: 0.0012 lr: 0.02\n",
      "iteration: 312700 loss: 0.0011 lr: 0.02\n",
      "iteration: 312800 loss: 0.0011 lr: 0.02\n",
      "iteration: 312900 loss: 0.0012 lr: 0.02\n",
      "iteration: 313000 loss: 0.0011 lr: 0.02\n",
      "iteration: 313100 loss: 0.0011 lr: 0.02\n",
      "iteration: 313200 loss: 0.0012 lr: 0.02\n",
      "iteration: 313300 loss: 0.0013 lr: 0.02\n",
      "iteration: 313400 loss: 0.0012 lr: 0.02\n",
      "iteration: 313500 loss: 0.0012 lr: 0.02\n",
      "iteration: 313600 loss: 0.0011 lr: 0.02\n",
      "iteration: 313700 loss: 0.0011 lr: 0.02\n",
      "iteration: 313800 loss: 0.0012 lr: 0.02\n",
      "iteration: 313900 loss: 0.0012 lr: 0.02\n",
      "iteration: 314000 loss: 0.0010 lr: 0.02\n",
      "iteration: 314100 loss: 0.0011 lr: 0.02\n",
      "iteration: 314200 loss: 0.0012 lr: 0.02\n",
      "iteration: 314300 loss: 0.0014 lr: 0.02\n",
      "iteration: 314400 loss: 0.0010 lr: 0.02\n",
      "iteration: 314500 loss: 0.0011 lr: 0.02\n",
      "iteration: 314600 loss: 0.0011 lr: 0.02\n",
      "iteration: 314700 loss: 0.0011 lr: 0.02\n",
      "iteration: 314800 loss: 0.0011 lr: 0.02\n",
      "iteration: 314900 loss: 0.0012 lr: 0.02\n",
      "iteration: 315000 loss: 0.0012 lr: 0.02\n",
      "iteration: 315100 loss: 0.0011 lr: 0.02\n",
      "iteration: 315200 loss: 0.0011 lr: 0.02\n",
      "iteration: 315300 loss: 0.0012 lr: 0.02\n",
      "iteration: 315400 loss: 0.0013 lr: 0.02\n",
      "iteration: 315500 loss: 0.0011 lr: 0.02\n",
      "iteration: 315600 loss: 0.0012 lr: 0.02\n",
      "iteration: 315700 loss: 0.0011 lr: 0.02\n",
      "iteration: 315800 loss: 0.0011 lr: 0.02\n",
      "iteration: 315900 loss: 0.0013 lr: 0.02\n",
      "iteration: 316000 loss: 0.0013 lr: 0.02\n",
      "iteration: 316100 loss: 0.0012 lr: 0.02\n",
      "iteration: 316200 loss: 0.0013 lr: 0.02\n",
      "iteration: 316300 loss: 0.0012 lr: 0.02\n",
      "iteration: 316400 loss: 0.0012 lr: 0.02\n",
      "iteration: 316500 loss: 0.0012 lr: 0.02\n",
      "iteration: 316600 loss: 0.0012 lr: 0.02\n",
      "iteration: 316700 loss: 0.0013 lr: 0.02\n",
      "iteration: 316800 loss: 0.0012 lr: 0.02\n",
      "iteration: 316900 loss: 0.0012 lr: 0.02\n",
      "iteration: 317000 loss: 0.0012 lr: 0.02\n",
      "iteration: 317100 loss: 0.0012 lr: 0.02\n",
      "iteration: 317200 loss: 0.0011 lr: 0.02\n",
      "iteration: 317300 loss: 0.0011 lr: 0.02\n",
      "iteration: 317400 loss: 0.0012 lr: 0.02\n",
      "iteration: 317500 loss: 0.0013 lr: 0.02\n",
      "iteration: 317600 loss: 0.0013 lr: 0.02\n",
      "iteration: 317700 loss: 0.0010 lr: 0.02\n",
      "iteration: 317800 loss: 0.0011 lr: 0.02\n",
      "iteration: 317900 loss: 0.0011 lr: 0.02\n",
      "iteration: 318000 loss: 0.0012 lr: 0.02\n",
      "iteration: 318100 loss: 0.0011 lr: 0.02\n",
      "iteration: 318200 loss: 0.0011 lr: 0.02\n",
      "iteration: 318300 loss: 0.0013 lr: 0.02\n",
      "iteration: 318400 loss: 0.0013 lr: 0.02\n",
      "iteration: 318500 loss: 0.0011 lr: 0.02\n",
      "iteration: 318600 loss: 0.0013 lr: 0.02\n",
      "iteration: 318700 loss: 0.0011 lr: 0.02\n",
      "iteration: 318800 loss: 0.0012 lr: 0.02\n",
      "iteration: 318900 loss: 0.0012 lr: 0.02\n",
      "iteration: 319000 loss: 0.0013 lr: 0.02\n",
      "iteration: 319100 loss: 0.0011 lr: 0.02\n",
      "iteration: 319200 loss: 0.0013 lr: 0.02\n",
      "iteration: 319300 loss: 0.0011 lr: 0.02\n",
      "iteration: 319400 loss: 0.0012 lr: 0.02\n",
      "iteration: 319500 loss: 0.0012 lr: 0.02\n",
      "iteration: 319600 loss: 0.0013 lr: 0.02\n",
      "iteration: 319700 loss: 0.0011 lr: 0.02\n",
      "iteration: 319800 loss: 0.0013 lr: 0.02\n",
      "iteration: 319900 loss: 0.0011 lr: 0.02\n",
      "iteration: 320000 loss: 0.0012 lr: 0.02\n",
      "iteration: 320100 loss: 0.0012 lr: 0.02\n",
      "iteration: 320200 loss: 0.0012 lr: 0.02\n",
      "iteration: 320300 loss: 0.0012 lr: 0.02\n",
      "iteration: 320400 loss: 0.0013 lr: 0.02\n",
      "iteration: 320500 loss: 0.0013 lr: 0.02\n",
      "iteration: 320600 loss: 0.0012 lr: 0.02\n",
      "iteration: 320700 loss: 0.0011 lr: 0.02\n",
      "iteration: 320800 loss: 0.0012 lr: 0.02\n",
      "iteration: 320900 loss: 0.0011 lr: 0.02\n",
      "iteration: 321000 loss: 0.0013 lr: 0.02\n",
      "iteration: 321100 loss: 0.0011 lr: 0.02\n",
      "iteration: 321200 loss: 0.0010 lr: 0.02\n",
      "iteration: 321300 loss: 0.0012 lr: 0.02\n",
      "iteration: 321400 loss: 0.0013 lr: 0.02\n",
      "iteration: 321500 loss: 0.0013 lr: 0.02\n",
      "iteration: 321600 loss: 0.0011 lr: 0.02\n",
      "iteration: 321700 loss: 0.0011 lr: 0.02\n",
      "iteration: 321800 loss: 0.0011 lr: 0.02\n",
      "iteration: 321900 loss: 0.0012 lr: 0.02\n",
      "iteration: 322000 loss: 0.0012 lr: 0.02\n",
      "iteration: 322100 loss: 0.0012 lr: 0.02\n",
      "iteration: 322200 loss: 0.0012 lr: 0.02\n",
      "iteration: 322300 loss: 0.0012 lr: 0.02\n",
      "iteration: 322400 loss: 0.0010 lr: 0.02\n",
      "iteration: 322500 loss: 0.0011 lr: 0.02\n",
      "iteration: 322600 loss: 0.0012 lr: 0.02\n",
      "iteration: 322700 loss: 0.0011 lr: 0.02\n",
      "iteration: 322800 loss: 0.0012 lr: 0.02\n",
      "iteration: 322900 loss: 0.0013 lr: 0.02\n",
      "iteration: 323000 loss: 0.0011 lr: 0.02\n",
      "iteration: 323100 loss: 0.0011 lr: 0.02\n",
      "iteration: 323200 loss: 0.0013 lr: 0.02\n",
      "iteration: 323300 loss: 0.0012 lr: 0.02\n",
      "iteration: 323400 loss: 0.0012 lr: 0.02\n",
      "iteration: 323500 loss: 0.0011 lr: 0.02\n",
      "iteration: 323600 loss: 0.0012 lr: 0.02\n",
      "iteration: 323700 loss: 0.0012 lr: 0.02\n",
      "iteration: 323800 loss: 0.0011 lr: 0.02\n",
      "iteration: 323900 loss: 0.0012 lr: 0.02\n",
      "iteration: 324000 loss: 0.0011 lr: 0.02\n",
      "iteration: 324100 loss: 0.0013 lr: 0.02\n",
      "iteration: 324200 loss: 0.0011 lr: 0.02\n",
      "iteration: 324300 loss: 0.0012 lr: 0.02\n",
      "iteration: 324400 loss: 0.0011 lr: 0.02\n",
      "iteration: 324500 loss: 0.0013 lr: 0.02\n",
      "iteration: 324600 loss: 0.0012 lr: 0.02\n",
      "iteration: 324700 loss: 0.0013 lr: 0.02\n",
      "iteration: 324800 loss: 0.0012 lr: 0.02\n",
      "iteration: 324900 loss: 0.0011 lr: 0.02\n",
      "iteration: 325000 loss: 0.0013 lr: 0.02\n",
      "iteration: 325100 loss: 0.0011 lr: 0.02\n",
      "iteration: 325200 loss: 0.0011 lr: 0.02\n",
      "iteration: 325300 loss: 0.0012 lr: 0.02\n",
      "iteration: 325400 loss: 0.0011 lr: 0.02\n",
      "iteration: 325500 loss: 0.0012 lr: 0.02\n",
      "iteration: 325600 loss: 0.0011 lr: 0.02\n",
      "iteration: 325700 loss: 0.0011 lr: 0.02\n",
      "iteration: 325800 loss: 0.0012 lr: 0.02\n",
      "iteration: 325900 loss: 0.0013 lr: 0.02\n",
      "iteration: 326000 loss: 0.0013 lr: 0.02\n",
      "iteration: 326100 loss: 0.0012 lr: 0.02\n",
      "iteration: 326200 loss: 0.0011 lr: 0.02\n",
      "iteration: 326300 loss: 0.0011 lr: 0.02\n",
      "iteration: 326400 loss: 0.0013 lr: 0.02\n",
      "iteration: 326500 loss: 0.0012 lr: 0.02\n",
      "iteration: 326600 loss: 0.0011 lr: 0.02\n",
      "iteration: 326700 loss: 0.0012 lr: 0.02\n",
      "iteration: 326800 loss: 0.0012 lr: 0.02\n",
      "iteration: 326900 loss: 0.0013 lr: 0.02\n",
      "iteration: 327000 loss: 0.0011 lr: 0.02\n",
      "iteration: 327100 loss: 0.0011 lr: 0.02\n",
      "iteration: 327200 loss: 0.0010 lr: 0.02\n",
      "iteration: 327300 loss: 0.0012 lr: 0.02\n",
      "iteration: 327400 loss: 0.0013 lr: 0.02\n",
      "iteration: 327500 loss: 0.0012 lr: 0.02\n",
      "iteration: 327600 loss: 0.0011 lr: 0.02\n",
      "iteration: 327700 loss: 0.0010 lr: 0.02\n",
      "iteration: 327800 loss: 0.0010 lr: 0.02\n",
      "iteration: 327900 loss: 0.0012 lr: 0.02\n",
      "iteration: 328000 loss: 0.0012 lr: 0.02\n",
      "iteration: 328100 loss: 0.0011 lr: 0.02\n",
      "iteration: 328200 loss: 0.0012 lr: 0.02\n",
      "iteration: 328300 loss: 0.0013 lr: 0.02\n",
      "iteration: 328400 loss: 0.0012 lr: 0.02\n",
      "iteration: 328500 loss: 0.0013 lr: 0.02\n",
      "iteration: 328600 loss: 0.0012 lr: 0.02\n",
      "iteration: 328700 loss: 0.0013 lr: 0.02\n",
      "iteration: 328800 loss: 0.0011 lr: 0.02\n",
      "iteration: 328900 loss: 0.0013 lr: 0.02\n",
      "iteration: 329000 loss: 0.0012 lr: 0.02\n",
      "iteration: 329100 loss: 0.0012 lr: 0.02\n",
      "iteration: 329200 loss: 0.0013 lr: 0.02\n",
      "iteration: 329300 loss: 0.0012 lr: 0.02\n",
      "iteration: 329400 loss: 0.0012 lr: 0.02\n",
      "iteration: 329500 loss: 0.0012 lr: 0.02\n",
      "iteration: 329600 loss: 0.0011 lr: 0.02\n",
      "iteration: 329700 loss: 0.0012 lr: 0.02\n",
      "iteration: 329800 loss: 0.0012 lr: 0.02\n",
      "iteration: 329900 loss: 0.0012 lr: 0.02\n",
      "iteration: 330000 loss: 0.0011 lr: 0.02\n",
      "iteration: 330100 loss: 0.0011 lr: 0.02\n",
      "iteration: 330200 loss: 0.0013 lr: 0.02\n",
      "iteration: 330300 loss: 0.0012 lr: 0.02\n",
      "iteration: 330400 loss: 0.0013 lr: 0.02\n",
      "iteration: 330500 loss: 0.0010 lr: 0.02\n",
      "iteration: 330600 loss: 0.0012 lr: 0.02\n",
      "iteration: 330700 loss: 0.0010 lr: 0.02\n",
      "iteration: 330800 loss: 0.0012 lr: 0.02\n",
      "iteration: 330900 loss: 0.0011 lr: 0.02\n",
      "iteration: 331000 loss: 0.0011 lr: 0.02\n",
      "iteration: 331100 loss: 0.0011 lr: 0.02\n",
      "iteration: 331200 loss: 0.0012 lr: 0.02\n",
      "iteration: 331300 loss: 0.0010 lr: 0.02\n",
      "iteration: 331400 loss: 0.0011 lr: 0.02\n",
      "iteration: 331500 loss: 0.0012 lr: 0.02\n",
      "iteration: 331600 loss: 0.0010 lr: 0.02\n",
      "iteration: 331700 loss: 0.0011 lr: 0.02\n",
      "iteration: 331800 loss: 0.0011 lr: 0.02\n",
      "iteration: 331900 loss: 0.0011 lr: 0.02\n",
      "iteration: 332000 loss: 0.0011 lr: 0.02\n",
      "iteration: 332100 loss: 0.0012 lr: 0.02\n",
      "iteration: 332200 loss: 0.0010 lr: 0.02\n",
      "iteration: 332300 loss: 0.0012 lr: 0.02\n",
      "iteration: 332400 loss: 0.0011 lr: 0.02\n",
      "iteration: 332500 loss: 0.0013 lr: 0.02\n",
      "iteration: 332600 loss: 0.0010 lr: 0.02\n",
      "iteration: 332700 loss: 0.0010 lr: 0.02\n",
      "iteration: 332800 loss: 0.0011 lr: 0.02\n",
      "iteration: 332900 loss: 0.0012 lr: 0.02\n",
      "iteration: 333000 loss: 0.0012 lr: 0.02\n",
      "iteration: 333100 loss: 0.0012 lr: 0.02\n",
      "iteration: 333200 loss: 0.0012 lr: 0.02\n",
      "iteration: 333300 loss: 0.0011 lr: 0.02\n",
      "iteration: 333400 loss: 0.0012 lr: 0.02\n",
      "iteration: 333500 loss: 0.0012 lr: 0.02\n",
      "iteration: 333600 loss: 0.0011 lr: 0.02\n",
      "iteration: 333700 loss: 0.0011 lr: 0.02\n",
      "iteration: 333800 loss: 0.0011 lr: 0.02\n",
      "iteration: 333900 loss: 0.0011 lr: 0.02\n",
      "iteration: 334000 loss: 0.0010 lr: 0.02\n",
      "iteration: 334100 loss: 0.0012 lr: 0.02\n",
      "iteration: 334200 loss: 0.0013 lr: 0.02\n",
      "iteration: 334300 loss: 0.0011 lr: 0.02\n",
      "iteration: 334400 loss: 0.0011 lr: 0.02\n",
      "iteration: 334500 loss: 0.0012 lr: 0.02\n",
      "iteration: 334600 loss: 0.0012 lr: 0.02\n",
      "iteration: 334700 loss: 0.0011 lr: 0.02\n",
      "iteration: 334800 loss: 0.0012 lr: 0.02\n",
      "iteration: 334900 loss: 0.0012 lr: 0.02\n",
      "iteration: 335000 loss: 0.0012 lr: 0.02\n",
      "iteration: 335100 loss: 0.0012 lr: 0.02\n",
      "iteration: 335200 loss: 0.0011 lr: 0.02\n",
      "iteration: 335300 loss: 0.0011 lr: 0.02\n",
      "iteration: 335400 loss: 0.0011 lr: 0.02\n",
      "iteration: 335500 loss: 0.0011 lr: 0.02\n",
      "iteration: 335600 loss: 0.0011 lr: 0.02\n",
      "iteration: 335700 loss: 0.0012 lr: 0.02\n",
      "iteration: 335800 loss: 0.0012 lr: 0.02\n",
      "iteration: 335900 loss: 0.0011 lr: 0.02\n",
      "iteration: 336000 loss: 0.0013 lr: 0.02\n",
      "iteration: 336100 loss: 0.0013 lr: 0.02\n",
      "iteration: 336200 loss: 0.0012 lr: 0.02\n",
      "iteration: 336300 loss: 0.0013 lr: 0.02\n",
      "iteration: 336400 loss: 0.0010 lr: 0.02\n",
      "iteration: 336500 loss: 0.0010 lr: 0.02\n",
      "iteration: 336600 loss: 0.0011 lr: 0.02\n",
      "iteration: 336700 loss: 0.0013 lr: 0.02\n",
      "iteration: 336800 loss: 0.0011 lr: 0.02\n",
      "iteration: 336900 loss: 0.0011 lr: 0.02\n",
      "iteration: 337000 loss: 0.0011 lr: 0.02\n",
      "iteration: 337100 loss: 0.0011 lr: 0.02\n",
      "iteration: 337200 loss: 0.0012 lr: 0.02\n",
      "iteration: 337300 loss: 0.0013 lr: 0.02\n",
      "iteration: 337400 loss: 0.0012 lr: 0.02\n",
      "iteration: 337500 loss: 0.0011 lr: 0.02\n",
      "iteration: 337600 loss: 0.0011 lr: 0.02\n",
      "iteration: 337700 loss: 0.0012 lr: 0.02\n",
      "iteration: 337800 loss: 0.0011 lr: 0.02\n",
      "iteration: 337900 loss: 0.0012 lr: 0.02\n",
      "iteration: 338000 loss: 0.0011 lr: 0.02\n",
      "iteration: 338100 loss: 0.0011 lr: 0.02\n",
      "iteration: 338200 loss: 0.0011 lr: 0.02\n",
      "iteration: 338300 loss: 0.0009 lr: 0.02\n",
      "iteration: 338400 loss: 0.0012 lr: 0.02\n",
      "iteration: 338500 loss: 0.0010 lr: 0.02\n",
      "iteration: 338600 loss: 0.0012 lr: 0.02\n",
      "iteration: 338700 loss: 0.0011 lr: 0.02\n",
      "iteration: 338800 loss: 0.0012 lr: 0.02\n",
      "iteration: 338900 loss: 0.0011 lr: 0.02\n",
      "iteration: 339000 loss: 0.0012 lr: 0.02\n",
      "iteration: 339100 loss: 0.0011 lr: 0.02\n",
      "iteration: 339200 loss: 0.0014 lr: 0.02\n",
      "iteration: 339300 loss: 0.0011 lr: 0.02\n",
      "iteration: 339400 loss: 0.0012 lr: 0.02\n",
      "iteration: 339500 loss: 0.0012 lr: 0.02\n",
      "iteration: 339600 loss: 0.0013 lr: 0.02\n",
      "iteration: 339700 loss: 0.0012 lr: 0.02\n",
      "iteration: 339800 loss: 0.0011 lr: 0.02\n",
      "iteration: 339900 loss: 0.0011 lr: 0.02\n",
      "iteration: 340000 loss: 0.0012 lr: 0.02\n",
      "iteration: 340100 loss: 0.0011 lr: 0.02\n",
      "iteration: 340200 loss: 0.0011 lr: 0.02\n",
      "iteration: 340300 loss: 0.0013 lr: 0.02\n",
      "iteration: 340400 loss: 0.0011 lr: 0.02\n",
      "iteration: 340500 loss: 0.0012 lr: 0.02\n",
      "iteration: 340600 loss: 0.0012 lr: 0.02\n",
      "iteration: 340700 loss: 0.0011 lr: 0.02\n",
      "iteration: 340800 loss: 0.0012 lr: 0.02\n",
      "iteration: 340900 loss: 0.0011 lr: 0.02\n",
      "iteration: 341000 loss: 0.0012 lr: 0.02\n",
      "iteration: 341100 loss: 0.0012 lr: 0.02\n",
      "iteration: 341200 loss: 0.0011 lr: 0.02\n",
      "iteration: 341300 loss: 0.0011 lr: 0.02\n",
      "iteration: 341400 loss: 0.0011 lr: 0.02\n",
      "iteration: 341500 loss: 0.0011 lr: 0.02\n",
      "iteration: 341600 loss: 0.0011 lr: 0.02\n",
      "iteration: 341700 loss: 0.0011 lr: 0.02\n",
      "iteration: 341800 loss: 0.0012 lr: 0.02\n",
      "iteration: 341900 loss: 0.0011 lr: 0.02\n",
      "iteration: 342000 loss: 0.0012 lr: 0.02\n",
      "iteration: 342100 loss: 0.0011 lr: 0.02\n",
      "iteration: 342200 loss: 0.0011 lr: 0.02\n",
      "iteration: 342300 loss: 0.0010 lr: 0.02\n",
      "iteration: 342400 loss: 0.0011 lr: 0.02\n",
      "iteration: 342500 loss: 0.0012 lr: 0.02\n",
      "iteration: 342600 loss: 0.0012 lr: 0.02\n",
      "iteration: 342700 loss: 0.0011 lr: 0.02\n",
      "iteration: 342800 loss: 0.0011 lr: 0.02\n",
      "iteration: 342900 loss: 0.0012 lr: 0.02\n",
      "iteration: 343000 loss: 0.0010 lr: 0.02\n",
      "iteration: 343100 loss: 0.0012 lr: 0.02\n",
      "iteration: 343200 loss: 0.0012 lr: 0.02\n",
      "iteration: 343300 loss: 0.0011 lr: 0.02\n",
      "iteration: 343400 loss: 0.0011 lr: 0.02\n",
      "iteration: 343500 loss: 0.0012 lr: 0.02\n",
      "iteration: 343600 loss: 0.0012 lr: 0.02\n",
      "iteration: 343700 loss: 0.0011 lr: 0.02\n",
      "iteration: 343800 loss: 0.0011 lr: 0.02\n",
      "iteration: 343900 loss: 0.0011 lr: 0.02\n",
      "iteration: 344000 loss: 0.0013 lr: 0.02\n",
      "iteration: 344100 loss: 0.0011 lr: 0.02\n",
      "iteration: 344200 loss: 0.0012 lr: 0.02\n",
      "iteration: 344300 loss: 0.0011 lr: 0.02\n",
      "iteration: 344400 loss: 0.0010 lr: 0.02\n",
      "iteration: 344500 loss: 0.0012 lr: 0.02\n",
      "iteration: 344600 loss: 0.0012 lr: 0.02\n",
      "iteration: 344700 loss: 0.0011 lr: 0.02\n",
      "iteration: 344800 loss: 0.0011 lr: 0.02\n",
      "iteration: 344900 loss: 0.0012 lr: 0.02\n",
      "iteration: 345000 loss: 0.0012 lr: 0.02\n",
      "iteration: 345100 loss: 0.0011 lr: 0.02\n",
      "iteration: 345200 loss: 0.0011 lr: 0.02\n",
      "iteration: 345300 loss: 0.0011 lr: 0.02\n",
      "iteration: 345400 loss: 0.0012 lr: 0.02\n",
      "iteration: 345500 loss: 0.0011 lr: 0.02\n",
      "iteration: 345600 loss: 0.0011 lr: 0.02\n",
      "iteration: 345700 loss: 0.0012 lr: 0.02\n",
      "iteration: 345800 loss: 0.0010 lr: 0.02\n",
      "iteration: 345900 loss: 0.0013 lr: 0.02\n",
      "iteration: 346000 loss: 0.0011 lr: 0.02\n",
      "iteration: 346100 loss: 0.0011 lr: 0.02\n",
      "iteration: 346200 loss: 0.0010 lr: 0.02\n",
      "iteration: 346300 loss: 0.0013 lr: 0.02\n",
      "iteration: 346400 loss: 0.0012 lr: 0.02\n",
      "iteration: 346500 loss: 0.0010 lr: 0.02\n",
      "iteration: 346600 loss: 0.0011 lr: 0.02\n",
      "iteration: 346700 loss: 0.0012 lr: 0.02\n",
      "iteration: 346800 loss: 0.0011 lr: 0.02\n",
      "iteration: 346900 loss: 0.0013 lr: 0.02\n",
      "iteration: 347000 loss: 0.0012 lr: 0.02\n",
      "iteration: 347100 loss: 0.0012 lr: 0.02\n",
      "iteration: 347200 loss: 0.0011 lr: 0.02\n",
      "iteration: 347300 loss: 0.0013 lr: 0.02\n",
      "iteration: 347400 loss: 0.0012 lr: 0.02\n",
      "iteration: 347500 loss: 0.0012 lr: 0.02\n",
      "iteration: 347600 loss: 0.0011 lr: 0.02\n",
      "iteration: 347700 loss: 0.0011 lr: 0.02\n",
      "iteration: 347800 loss: 0.0011 lr: 0.02\n",
      "iteration: 347900 loss: 0.0011 lr: 0.02\n",
      "iteration: 348000 loss: 0.0012 lr: 0.02\n",
      "iteration: 348100 loss: 0.0012 lr: 0.02\n",
      "iteration: 348200 loss: 0.0010 lr: 0.02\n",
      "iteration: 348300 loss: 0.0012 lr: 0.02\n",
      "iteration: 348400 loss: 0.0012 lr: 0.02\n",
      "iteration: 348500 loss: 0.0010 lr: 0.02\n",
      "iteration: 348600 loss: 0.0011 lr: 0.02\n",
      "iteration: 348700 loss: 0.0011 lr: 0.02\n",
      "iteration: 348800 loss: 0.0011 lr: 0.02\n",
      "iteration: 348900 loss: 0.0011 lr: 0.02\n",
      "iteration: 349000 loss: 0.0012 lr: 0.02\n",
      "iteration: 349100 loss: 0.0013 lr: 0.02\n",
      "iteration: 349200 loss: 0.0010 lr: 0.02\n",
      "iteration: 349300 loss: 0.0011 lr: 0.02\n",
      "iteration: 349400 loss: 0.0011 lr: 0.02\n",
      "iteration: 349500 loss: 0.0011 lr: 0.02\n",
      "iteration: 349600 loss: 0.0011 lr: 0.02\n",
      "iteration: 349700 loss: 0.0011 lr: 0.02\n",
      "iteration: 349800 loss: 0.0011 lr: 0.02\n",
      "iteration: 349900 loss: 0.0011 lr: 0.02\n",
      "iteration: 350000 loss: 0.0011 lr: 0.02\n",
      "iteration: 350100 loss: 0.0011 lr: 0.02\n",
      "iteration: 350200 loss: 0.0013 lr: 0.02\n",
      "iteration: 350300 loss: 0.0011 lr: 0.02\n",
      "iteration: 350400 loss: 0.0011 lr: 0.02\n",
      "iteration: 350500 loss: 0.0011 lr: 0.02\n",
      "iteration: 350600 loss: 0.0012 lr: 0.02\n",
      "iteration: 350700 loss: 0.0011 lr: 0.02\n",
      "iteration: 350800 loss: 0.0011 lr: 0.02\n",
      "iteration: 350900 loss: 0.0011 lr: 0.02\n",
      "iteration: 351000 loss: 0.0010 lr: 0.02\n",
      "iteration: 351100 loss: 0.0010 lr: 0.02\n",
      "iteration: 351200 loss: 0.0012 lr: 0.02\n",
      "iteration: 351300 loss: 0.0011 lr: 0.02\n",
      "iteration: 351400 loss: 0.0011 lr: 0.02\n",
      "iteration: 351500 loss: 0.0010 lr: 0.02\n",
      "iteration: 351600 loss: 0.0013 lr: 0.02\n",
      "iteration: 351700 loss: 0.0012 lr: 0.02\n",
      "iteration: 351800 loss: 0.0010 lr: 0.02\n",
      "iteration: 351900 loss: 0.0011 lr: 0.02\n",
      "iteration: 352000 loss: 0.0011 lr: 0.02\n",
      "iteration: 352100 loss: 0.0011 lr: 0.02\n",
      "iteration: 352200 loss: 0.0012 lr: 0.02\n",
      "iteration: 352300 loss: 0.0011 lr: 0.02\n",
      "iteration: 352400 loss: 0.0011 lr: 0.02\n",
      "iteration: 352500 loss: 0.0012 lr: 0.02\n",
      "iteration: 352600 loss: 0.0012 lr: 0.02\n",
      "iteration: 352700 loss: 0.0011 lr: 0.02\n",
      "iteration: 352800 loss: 0.0012 lr: 0.02\n",
      "iteration: 352900 loss: 0.0012 lr: 0.02\n",
      "iteration: 353000 loss: 0.0011 lr: 0.02\n",
      "iteration: 353100 loss: 0.0013 lr: 0.02\n",
      "iteration: 353200 loss: 0.0011 lr: 0.02\n",
      "iteration: 353300 loss: 0.0012 lr: 0.02\n",
      "iteration: 353400 loss: 0.0012 lr: 0.02\n",
      "iteration: 353500 loss: 0.0011 lr: 0.02\n",
      "iteration: 353600 loss: 0.0012 lr: 0.02\n",
      "iteration: 353700 loss: 0.0012 lr: 0.02\n",
      "iteration: 353800 loss: 0.0012 lr: 0.02\n",
      "iteration: 353900 loss: 0.0012 lr: 0.02\n",
      "iteration: 354000 loss: 0.0011 lr: 0.02\n",
      "iteration: 354100 loss: 0.0011 lr: 0.02\n",
      "iteration: 354200 loss: 0.0011 lr: 0.02\n",
      "iteration: 354300 loss: 0.0011 lr: 0.02\n",
      "iteration: 354400 loss: 0.0010 lr: 0.02\n",
      "iteration: 354500 loss: 0.0012 lr: 0.02\n",
      "iteration: 354600 loss: 0.0011 lr: 0.02\n",
      "iteration: 354700 loss: 0.0011 lr: 0.02\n",
      "iteration: 354800 loss: 0.0012 lr: 0.02\n",
      "iteration: 354900 loss: 0.0012 lr: 0.02\n",
      "iteration: 355000 loss: 0.0012 lr: 0.02\n",
      "iteration: 355100 loss: 0.0010 lr: 0.02\n",
      "iteration: 355200 loss: 0.0012 lr: 0.02\n",
      "iteration: 355300 loss: 0.0012 lr: 0.02\n",
      "iteration: 355400 loss: 0.0010 lr: 0.02\n",
      "iteration: 355500 loss: 0.0011 lr: 0.02\n",
      "iteration: 355600 loss: 0.0011 lr: 0.02\n",
      "iteration: 355700 loss: 0.0012 lr: 0.02\n",
      "iteration: 355800 loss: 0.0011 lr: 0.02\n",
      "iteration: 355900 loss: 0.0011 lr: 0.02\n",
      "iteration: 356000 loss: 0.0011 lr: 0.02\n",
      "iteration: 356100 loss: 0.0011 lr: 0.02\n",
      "iteration: 356200 loss: 0.0011 lr: 0.02\n",
      "iteration: 356300 loss: 0.0012 lr: 0.02\n",
      "iteration: 356400 loss: 0.0011 lr: 0.02\n",
      "iteration: 356500 loss: 0.0010 lr: 0.02\n",
      "iteration: 356600 loss: 0.0011 lr: 0.02\n",
      "iteration: 356700 loss: 0.0012 lr: 0.02\n",
      "iteration: 356800 loss: 0.0010 lr: 0.02\n",
      "iteration: 356900 loss: 0.0013 lr: 0.02\n",
      "iteration: 357000 loss: 0.0011 lr: 0.02\n",
      "iteration: 357100 loss: 0.0011 lr: 0.02\n",
      "iteration: 357200 loss: 0.0011 lr: 0.02\n",
      "iteration: 357300 loss: 0.0012 lr: 0.02\n",
      "iteration: 357400 loss: 0.0012 lr: 0.02\n",
      "iteration: 357500 loss: 0.0011 lr: 0.02\n",
      "iteration: 357600 loss: 0.0012 lr: 0.02\n",
      "iteration: 357700 loss: 0.0011 lr: 0.02\n",
      "iteration: 357800 loss: 0.0011 lr: 0.02\n",
      "iteration: 357900 loss: 0.0013 lr: 0.02\n",
      "iteration: 358000 loss: 0.0012 lr: 0.02\n",
      "iteration: 358100 loss: 0.0013 lr: 0.02\n",
      "iteration: 358200 loss: 0.0011 lr: 0.02\n",
      "iteration: 358300 loss: 0.0011 lr: 0.02\n",
      "iteration: 358400 loss: 0.0010 lr: 0.02\n",
      "iteration: 358500 loss: 0.0013 lr: 0.02\n",
      "iteration: 358600 loss: 0.0011 lr: 0.02\n",
      "iteration: 358700 loss: 0.0013 lr: 0.02\n",
      "iteration: 358800 loss: 0.0011 lr: 0.02\n",
      "iteration: 358900 loss: 0.0012 lr: 0.02\n",
      "iteration: 359000 loss: 0.0011 lr: 0.02\n",
      "iteration: 359100 loss: 0.0011 lr: 0.02\n",
      "iteration: 359200 loss: 0.0013 lr: 0.02\n",
      "iteration: 359300 loss: 0.0012 lr: 0.02\n",
      "iteration: 359400 loss: 0.0011 lr: 0.02\n",
      "iteration: 359500 loss: 0.0011 lr: 0.02\n",
      "iteration: 359600 loss: 0.0011 lr: 0.02\n",
      "iteration: 359700 loss: 0.0013 lr: 0.02\n",
      "iteration: 359800 loss: 0.0011 lr: 0.02\n",
      "iteration: 359900 loss: 0.0010 lr: 0.02\n",
      "iteration: 360000 loss: 0.0011 lr: 0.02\n",
      "iteration: 360100 loss: 0.0011 lr: 0.02\n",
      "iteration: 360200 loss: 0.0011 lr: 0.02\n",
      "iteration: 360300 loss: 0.0012 lr: 0.02\n",
      "iteration: 360400 loss: 0.0011 lr: 0.02\n",
      "iteration: 360500 loss: 0.0011 lr: 0.02\n",
      "iteration: 360600 loss: 0.0011 lr: 0.02\n",
      "iteration: 360700 loss: 0.0013 lr: 0.02\n",
      "iteration: 360800 loss: 0.0011 lr: 0.02\n",
      "iteration: 360900 loss: 0.0011 lr: 0.02\n",
      "iteration: 361000 loss: 0.0011 lr: 0.02\n",
      "iteration: 361100 loss: 0.0011 lr: 0.02\n",
      "iteration: 361200 loss: 0.0012 lr: 0.02\n",
      "iteration: 361300 loss: 0.0010 lr: 0.02\n",
      "iteration: 361400 loss: 0.0012 lr: 0.02\n",
      "iteration: 361500 loss: 0.0011 lr: 0.02\n",
      "iteration: 361600 loss: 0.0012 lr: 0.02\n",
      "iteration: 361700 loss: 0.0013 lr: 0.02\n",
      "iteration: 361800 loss: 0.0011 lr: 0.02\n",
      "iteration: 361900 loss: 0.0011 lr: 0.02\n",
      "iteration: 362000 loss: 0.0011 lr: 0.02\n",
      "iteration: 362100 loss: 0.0014 lr: 0.02\n",
      "iteration: 362200 loss: 0.0012 lr: 0.02\n",
      "iteration: 362300 loss: 0.0011 lr: 0.02\n",
      "iteration: 362400 loss: 0.0010 lr: 0.02\n",
      "iteration: 362500 loss: 0.0011 lr: 0.02\n",
      "iteration: 362600 loss: 0.0011 lr: 0.02\n",
      "iteration: 362700 loss: 0.0010 lr: 0.02\n",
      "iteration: 362800 loss: 0.0012 lr: 0.02\n",
      "iteration: 362900 loss: 0.0011 lr: 0.02\n",
      "iteration: 363000 loss: 0.0011 lr: 0.02\n",
      "iteration: 363100 loss: 0.0011 lr: 0.02\n",
      "iteration: 363200 loss: 0.0012 lr: 0.02\n",
      "iteration: 363300 loss: 0.0012 lr: 0.02\n",
      "iteration: 363400 loss: 0.0011 lr: 0.02\n",
      "iteration: 363500 loss: 0.0011 lr: 0.02\n",
      "iteration: 363600 loss: 0.0011 lr: 0.02\n",
      "iteration: 363700 loss: 0.0011 lr: 0.02\n",
      "iteration: 363800 loss: 0.0011 lr: 0.02\n",
      "iteration: 363900 loss: 0.0011 lr: 0.02\n",
      "iteration: 364000 loss: 0.0011 lr: 0.02\n",
      "iteration: 364100 loss: 0.0013 lr: 0.02\n",
      "iteration: 364200 loss: 0.0012 lr: 0.02\n",
      "iteration: 364300 loss: 0.0011 lr: 0.02\n",
      "iteration: 364400 loss: 0.0013 lr: 0.02\n",
      "iteration: 364500 loss: 0.0012 lr: 0.02\n",
      "iteration: 364600 loss: 0.0012 lr: 0.02\n",
      "iteration: 364700 loss: 0.0011 lr: 0.02\n",
      "iteration: 364800 loss: 0.0010 lr: 0.02\n",
      "iteration: 364900 loss: 0.0011 lr: 0.02\n",
      "iteration: 365000 loss: 0.0013 lr: 0.02\n",
      "iteration: 365100 loss: 0.0011 lr: 0.02\n",
      "iteration: 365200 loss: 0.0012 lr: 0.02\n",
      "iteration: 365300 loss: 0.0011 lr: 0.02\n",
      "iteration: 365400 loss: 0.0011 lr: 0.02\n",
      "iteration: 365500 loss: 0.0011 lr: 0.02\n",
      "iteration: 365600 loss: 0.0011 lr: 0.02\n",
      "iteration: 365700 loss: 0.0011 lr: 0.02\n",
      "iteration: 365800 loss: 0.0011 lr: 0.02\n",
      "iteration: 365900 loss: 0.0012 lr: 0.02\n",
      "iteration: 366000 loss: 0.0011 lr: 0.02\n",
      "iteration: 366100 loss: 0.0010 lr: 0.02\n",
      "iteration: 366200 loss: 0.0010 lr: 0.02\n",
      "iteration: 366300 loss: 0.0010 lr: 0.02\n",
      "iteration: 366400 loss: 0.0012 lr: 0.02\n",
      "iteration: 366500 loss: 0.0010 lr: 0.02\n",
      "iteration: 366600 loss: 0.0012 lr: 0.02\n",
      "iteration: 366700 loss: 0.0010 lr: 0.02\n",
      "iteration: 366800 loss: 0.0011 lr: 0.02\n",
      "iteration: 366900 loss: 0.0011 lr: 0.02\n",
      "iteration: 367000 loss: 0.0011 lr: 0.02\n",
      "iteration: 367100 loss: 0.0011 lr: 0.02\n",
      "iteration: 367200 loss: 0.0012 lr: 0.02\n",
      "iteration: 367300 loss: 0.0012 lr: 0.02\n",
      "iteration: 367400 loss: 0.0011 lr: 0.02\n",
      "iteration: 367500 loss: 0.0012 lr: 0.02\n",
      "iteration: 367600 loss: 0.0011 lr: 0.02\n",
      "iteration: 367700 loss: 0.0010 lr: 0.02\n",
      "iteration: 367800 loss: 0.0011 lr: 0.02\n",
      "iteration: 367900 loss: 0.0012 lr: 0.02\n",
      "iteration: 368000 loss: 0.0012 lr: 0.02\n",
      "iteration: 368100 loss: 0.0012 lr: 0.02\n",
      "iteration: 368200 loss: 0.0012 lr: 0.02\n",
      "iteration: 368300 loss: 0.0010 lr: 0.02\n",
      "iteration: 368400 loss: 0.0011 lr: 0.02\n",
      "iteration: 368500 loss: 0.0011 lr: 0.02\n",
      "iteration: 368600 loss: 0.0011 lr: 0.02\n",
      "iteration: 368700 loss: 0.0011 lr: 0.02\n",
      "iteration: 368800 loss: 0.0011 lr: 0.02\n",
      "iteration: 368900 loss: 0.0011 lr: 0.02\n",
      "iteration: 369000 loss: 0.0011 lr: 0.02\n",
      "iteration: 369100 loss: 0.0012 lr: 0.02\n",
      "iteration: 369200 loss: 0.0010 lr: 0.02\n",
      "iteration: 369300 loss: 0.0011 lr: 0.02\n",
      "iteration: 369400 loss: 0.0012 lr: 0.02\n",
      "iteration: 369500 loss: 0.0011 lr: 0.02\n",
      "iteration: 369600 loss: 0.0011 lr: 0.02\n",
      "iteration: 369700 loss: 0.0012 lr: 0.02\n",
      "iteration: 369800 loss: 0.0012 lr: 0.02\n",
      "iteration: 369900 loss: 0.0012 lr: 0.02\n",
      "iteration: 370000 loss: 0.0011 lr: 0.02\n",
      "iteration: 370100 loss: 0.0010 lr: 0.02\n",
      "iteration: 370200 loss: 0.0010 lr: 0.02\n",
      "iteration: 370300 loss: 0.0011 lr: 0.02\n",
      "iteration: 370400 loss: 0.0011 lr: 0.02\n",
      "iteration: 370500 loss: 0.0011 lr: 0.02\n",
      "iteration: 370600 loss: 0.0012 lr: 0.02\n",
      "iteration: 370700 loss: 0.0013 lr: 0.02\n",
      "iteration: 370800 loss: 0.0012 lr: 0.02\n",
      "iteration: 370900 loss: 0.0011 lr: 0.02\n",
      "iteration: 371000 loss: 0.0011 lr: 0.02\n",
      "iteration: 371100 loss: 0.0011 lr: 0.02\n",
      "iteration: 371200 loss: 0.0012 lr: 0.02\n",
      "iteration: 371300 loss: 0.0012 lr: 0.02\n",
      "iteration: 371400 loss: 0.0010 lr: 0.02\n",
      "iteration: 371500 loss: 0.0012 lr: 0.02\n",
      "iteration: 371600 loss: 0.0011 lr: 0.02\n",
      "iteration: 371700 loss: 0.0011 lr: 0.02\n",
      "iteration: 371800 loss: 0.0011 lr: 0.02\n",
      "iteration: 371900 loss: 0.0011 lr: 0.02\n",
      "iteration: 372000 loss: 0.0011 lr: 0.02\n",
      "iteration: 372100 loss: 0.0011 lr: 0.02\n",
      "iteration: 372200 loss: 0.0011 lr: 0.02\n",
      "iteration: 372300 loss: 0.0012 lr: 0.02\n",
      "iteration: 372400 loss: 0.0010 lr: 0.02\n",
      "iteration: 372500 loss: 0.0010 lr: 0.02\n",
      "iteration: 372600 loss: 0.0012 lr: 0.02\n",
      "iteration: 372700 loss: 0.0011 lr: 0.02\n",
      "iteration: 372800 loss: 0.0011 lr: 0.02\n",
      "iteration: 372900 loss: 0.0012 lr: 0.02\n",
      "iteration: 373000 loss: 0.0011 lr: 0.02\n",
      "iteration: 373100 loss: 0.0010 lr: 0.02\n",
      "iteration: 373200 loss: 0.0011 lr: 0.02\n",
      "iteration: 373300 loss: 0.0011 lr: 0.02\n",
      "iteration: 373400 loss: 0.0011 lr: 0.02\n",
      "iteration: 373500 loss: 0.0011 lr: 0.02\n",
      "iteration: 373600 loss: 0.0011 lr: 0.02\n",
      "iteration: 373700 loss: 0.0011 lr: 0.02\n",
      "iteration: 373800 loss: 0.0012 lr: 0.02\n",
      "iteration: 373900 loss: 0.0011 lr: 0.02\n",
      "iteration: 374000 loss: 0.0010 lr: 0.02\n",
      "iteration: 374100 loss: 0.0012 lr: 0.02\n",
      "iteration: 374200 loss: 0.0011 lr: 0.02\n",
      "iteration: 374300 loss: 0.0012 lr: 0.02\n",
      "iteration: 374400 loss: 0.0011 lr: 0.02\n",
      "iteration: 374500 loss: 0.0011 lr: 0.02\n",
      "iteration: 374600 loss: 0.0012 lr: 0.02\n",
      "iteration: 374700 loss: 0.0011 lr: 0.02\n",
      "iteration: 374800 loss: 0.0011 lr: 0.02\n",
      "iteration: 374900 loss: 0.0011 lr: 0.02\n",
      "iteration: 375000 loss: 0.0011 lr: 0.02\n",
      "iteration: 375100 loss: 0.0011 lr: 0.02\n",
      "iteration: 375200 loss: 0.0012 lr: 0.02\n",
      "iteration: 375300 loss: 0.0011 lr: 0.02\n",
      "iteration: 375400 loss: 0.0011 lr: 0.02\n",
      "iteration: 375500 loss: 0.0011 lr: 0.02\n",
      "iteration: 375600 loss: 0.0011 lr: 0.02\n",
      "iteration: 375700 loss: 0.0012 lr: 0.02\n",
      "iteration: 375800 loss: 0.0010 lr: 0.02\n",
      "iteration: 375900 loss: 0.0012 lr: 0.02\n",
      "iteration: 376000 loss: 0.0011 lr: 0.02\n",
      "iteration: 376100 loss: 0.0011 lr: 0.02\n",
      "iteration: 376200 loss: 0.0011 lr: 0.02\n",
      "iteration: 376300 loss: 0.0011 lr: 0.02\n",
      "iteration: 376400 loss: 0.0012 lr: 0.02\n",
      "iteration: 376500 loss: 0.0011 lr: 0.02\n",
      "iteration: 376600 loss: 0.0012 lr: 0.02\n",
      "iteration: 376700 loss: 0.0012 lr: 0.02\n",
      "iteration: 376800 loss: 0.0010 lr: 0.02\n",
      "iteration: 376900 loss: 0.0012 lr: 0.02\n",
      "iteration: 377000 loss: 0.0013 lr: 0.02\n",
      "iteration: 377100 loss: 0.0010 lr: 0.02\n",
      "iteration: 377200 loss: 0.0011 lr: 0.02\n",
      "iteration: 377300 loss: 0.0011 lr: 0.02\n",
      "iteration: 377400 loss: 0.0011 lr: 0.02\n",
      "iteration: 377500 loss: 0.0010 lr: 0.02\n",
      "iteration: 377600 loss: 0.0010 lr: 0.02\n",
      "iteration: 377700 loss: 0.0010 lr: 0.02\n",
      "iteration: 377800 loss: 0.0011 lr: 0.02\n",
      "iteration: 377900 loss: 0.0012 lr: 0.02\n",
      "iteration: 378000 loss: 0.0011 lr: 0.02\n",
      "iteration: 378100 loss: 0.0012 lr: 0.02\n",
      "iteration: 378200 loss: 0.0011 lr: 0.02\n",
      "iteration: 378300 loss: 0.0011 lr: 0.02\n",
      "iteration: 378400 loss: 0.0010 lr: 0.02\n",
      "iteration: 378500 loss: 0.0013 lr: 0.02\n",
      "iteration: 378600 loss: 0.0012 lr: 0.02\n",
      "iteration: 378700 loss: 0.0011 lr: 0.02\n",
      "iteration: 378800 loss: 0.0011 lr: 0.02\n",
      "iteration: 378900 loss: 0.0012 lr: 0.02\n",
      "iteration: 379000 loss: 0.0012 lr: 0.02\n",
      "iteration: 379100 loss: 0.0011 lr: 0.02\n",
      "iteration: 379200 loss: 0.0011 lr: 0.02\n",
      "iteration: 379300 loss: 0.0011 lr: 0.02\n",
      "iteration: 379400 loss: 0.0010 lr: 0.02\n",
      "iteration: 379500 loss: 0.0011 lr: 0.02\n",
      "iteration: 379600 loss: 0.0011 lr: 0.02\n",
      "iteration: 379700 loss: 0.0010 lr: 0.02\n",
      "iteration: 379800 loss: 0.0012 lr: 0.02\n",
      "iteration: 379900 loss: 0.0012 lr: 0.02\n",
      "iteration: 380000 loss: 0.0011 lr: 0.02\n",
      "iteration: 380100 loss: 0.0011 lr: 0.02\n",
      "iteration: 380200 loss: 0.0011 lr: 0.02\n",
      "iteration: 380300 loss: 0.0011 lr: 0.02\n",
      "iteration: 380400 loss: 0.0010 lr: 0.02\n",
      "iteration: 380500 loss: 0.0011 lr: 0.02\n",
      "iteration: 380600 loss: 0.0011 lr: 0.02\n",
      "iteration: 380700 loss: 0.0011 lr: 0.02\n",
      "iteration: 380800 loss: 0.0011 lr: 0.02\n",
      "iteration: 380900 loss: 0.0012 lr: 0.02\n",
      "iteration: 381000 loss: 0.0013 lr: 0.02\n",
      "iteration: 381100 loss: 0.0011 lr: 0.02\n",
      "iteration: 381200 loss: 0.0012 lr: 0.02\n",
      "iteration: 381300 loss: 0.0011 lr: 0.02\n",
      "iteration: 381400 loss: 0.0012 lr: 0.02\n",
      "iteration: 381500 loss: 0.0012 lr: 0.02\n",
      "iteration: 381600 loss: 0.0013 lr: 0.02\n",
      "iteration: 381700 loss: 0.0011 lr: 0.02\n",
      "iteration: 381800 loss: 0.0012 lr: 0.02\n",
      "iteration: 381900 loss: 0.0011 lr: 0.02\n",
      "iteration: 382000 loss: 0.0011 lr: 0.02\n",
      "iteration: 382100 loss: 0.0012 lr: 0.02\n",
      "iteration: 382200 loss: 0.0012 lr: 0.02\n",
      "iteration: 382300 loss: 0.0011 lr: 0.02\n",
      "iteration: 382400 loss: 0.0011 lr: 0.02\n",
      "iteration: 382500 loss: 0.0011 lr: 0.02\n",
      "iteration: 382600 loss: 0.0011 lr: 0.02\n",
      "iteration: 382700 loss: 0.0011 lr: 0.02\n",
      "iteration: 382800 loss: 0.0012 lr: 0.02\n",
      "iteration: 382900 loss: 0.0012 lr: 0.02\n",
      "iteration: 383000 loss: 0.0013 lr: 0.02\n",
      "iteration: 383100 loss: 0.0012 lr: 0.02\n",
      "iteration: 383200 loss: 0.0011 lr: 0.02\n",
      "iteration: 383300 loss: 0.0011 lr: 0.02\n",
      "iteration: 383400 loss: 0.0013 lr: 0.02\n",
      "iteration: 383500 loss: 0.0013 lr: 0.02\n",
      "iteration: 383600 loss: 0.0011 lr: 0.02\n",
      "iteration: 383700 loss: 0.0013 lr: 0.02\n",
      "iteration: 383800 loss: 0.0011 lr: 0.02\n",
      "iteration: 383900 loss: 0.0010 lr: 0.02\n",
      "iteration: 384000 loss: 0.0010 lr: 0.02\n",
      "iteration: 384100 loss: 0.0012 lr: 0.02\n",
      "iteration: 384200 loss: 0.0011 lr: 0.02\n",
      "iteration: 384300 loss: 0.0010 lr: 0.02\n",
      "iteration: 384400 loss: 0.0011 lr: 0.02\n",
      "iteration: 384500 loss: 0.0010 lr: 0.02\n",
      "iteration: 384600 loss: 0.0011 lr: 0.02\n",
      "iteration: 384700 loss: 0.0013 lr: 0.02\n",
      "iteration: 384800 loss: 0.0011 lr: 0.02\n",
      "iteration: 384900 loss: 0.0011 lr: 0.02\n",
      "iteration: 385000 loss: 0.0012 lr: 0.02\n",
      "iteration: 385100 loss: 0.0011 lr: 0.02\n",
      "iteration: 385200 loss: 0.0011 lr: 0.02\n",
      "iteration: 385300 loss: 0.0011 lr: 0.02\n",
      "iteration: 385400 loss: 0.0010 lr: 0.02\n",
      "iteration: 385500 loss: 0.0012 lr: 0.02\n",
      "iteration: 385600 loss: 0.0012 lr: 0.02\n",
      "iteration: 385700 loss: 0.0012 lr: 0.02\n",
      "iteration: 385800 loss: 0.0013 lr: 0.02\n",
      "iteration: 385900 loss: 0.0012 lr: 0.02\n",
      "iteration: 386000 loss: 0.0013 lr: 0.02\n",
      "iteration: 386100 loss: 0.0010 lr: 0.02\n",
      "iteration: 386200 loss: 0.0010 lr: 0.02\n",
      "iteration: 386300 loss: 0.0012 lr: 0.02\n",
      "iteration: 386400 loss: 0.0012 lr: 0.02\n",
      "iteration: 386500 loss: 0.0010 lr: 0.02\n",
      "iteration: 386600 loss: 0.0012 lr: 0.02\n",
      "iteration: 386700 loss: 0.0010 lr: 0.02\n",
      "iteration: 386800 loss: 0.0014 lr: 0.02\n",
      "iteration: 386900 loss: 0.0012 lr: 0.02\n",
      "iteration: 387000 loss: 0.0011 lr: 0.02\n",
      "iteration: 387100 loss: 0.0011 lr: 0.02\n",
      "iteration: 387200 loss: 0.0012 lr: 0.02\n",
      "iteration: 387300 loss: 0.0011 lr: 0.02\n",
      "iteration: 387400 loss: 0.0010 lr: 0.02\n",
      "iteration: 387500 loss: 0.0011 lr: 0.02\n",
      "iteration: 387600 loss: 0.0012 lr: 0.02\n",
      "iteration: 387700 loss: 0.0012 lr: 0.02\n",
      "iteration: 387800 loss: 0.0011 lr: 0.02\n",
      "iteration: 387900 loss: 0.0013 lr: 0.02\n",
      "iteration: 388000 loss: 0.0012 lr: 0.02\n",
      "iteration: 388100 loss: 0.0011 lr: 0.02\n",
      "iteration: 388200 loss: 0.0011 lr: 0.02\n",
      "iteration: 388300 loss: 0.0011 lr: 0.02\n",
      "iteration: 388400 loss: 0.0011 lr: 0.02\n",
      "iteration: 388500 loss: 0.0011 lr: 0.02\n",
      "iteration: 388600 loss: 0.0012 lr: 0.02\n",
      "iteration: 388700 loss: 0.0011 lr: 0.02\n",
      "iteration: 388800 loss: 0.0012 lr: 0.02\n",
      "iteration: 388900 loss: 0.0011 lr: 0.02\n",
      "iteration: 389000 loss: 0.0011 lr: 0.02\n",
      "iteration: 389100 loss: 0.0012 lr: 0.02\n",
      "iteration: 389200 loss: 0.0011 lr: 0.02\n",
      "iteration: 389300 loss: 0.0013 lr: 0.02\n",
      "iteration: 389400 loss: 0.0011 lr: 0.02\n",
      "iteration: 389500 loss: 0.0012 lr: 0.02\n",
      "iteration: 389600 loss: 0.0011 lr: 0.02\n",
      "iteration: 389700 loss: 0.0011 lr: 0.02\n",
      "iteration: 389800 loss: 0.0010 lr: 0.02\n",
      "iteration: 389900 loss: 0.0011 lr: 0.02\n",
      "iteration: 390000 loss: 0.0010 lr: 0.02\n",
      "iteration: 390100 loss: 0.0011 lr: 0.02\n",
      "iteration: 390200 loss: 0.0011 lr: 0.02\n",
      "iteration: 390300 loss: 0.0010 lr: 0.02\n",
      "iteration: 390400 loss: 0.0011 lr: 0.02\n",
      "iteration: 390500 loss: 0.0011 lr: 0.02\n",
      "iteration: 390600 loss: 0.0011 lr: 0.02\n",
      "iteration: 390700 loss: 0.0012 lr: 0.02\n",
      "iteration: 390800 loss: 0.0010 lr: 0.02\n",
      "iteration: 390900 loss: 0.0011 lr: 0.02\n",
      "iteration: 391000 loss: 0.0012 lr: 0.02\n",
      "iteration: 391100 loss: 0.0010 lr: 0.02\n",
      "iteration: 391200 loss: 0.0011 lr: 0.02\n",
      "iteration: 391300 loss: 0.0011 lr: 0.02\n",
      "iteration: 391400 loss: 0.0011 lr: 0.02\n",
      "iteration: 391500 loss: 0.0010 lr: 0.02\n",
      "iteration: 391600 loss: 0.0010 lr: 0.02\n",
      "iteration: 391700 loss: 0.0010 lr: 0.02\n",
      "iteration: 391800 loss: 0.0010 lr: 0.02\n",
      "iteration: 391900 loss: 0.0011 lr: 0.02\n",
      "iteration: 392000 loss: 0.0010 lr: 0.02\n",
      "iteration: 392100 loss: 0.0011 lr: 0.02\n",
      "iteration: 392200 loss: 0.0010 lr: 0.02\n",
      "iteration: 392300 loss: 0.0010 lr: 0.02\n",
      "iteration: 392400 loss: 0.0012 lr: 0.02\n",
      "iteration: 392500 loss: 0.0012 lr: 0.02\n",
      "iteration: 392600 loss: 0.0012 lr: 0.02\n",
      "iteration: 392700 loss: 0.0011 lr: 0.02\n",
      "iteration: 392800 loss: 0.0010 lr: 0.02\n",
      "iteration: 392900 loss: 0.0012 lr: 0.02\n",
      "iteration: 393000 loss: 0.0013 lr: 0.02\n",
      "iteration: 393100 loss: 0.0012 lr: 0.02\n",
      "iteration: 393200 loss: 0.0012 lr: 0.02\n",
      "iteration: 393300 loss: 0.0011 lr: 0.02\n",
      "iteration: 393400 loss: 0.0012 lr: 0.02\n",
      "iteration: 393500 loss: 0.0012 lr: 0.02\n",
      "iteration: 393600 loss: 0.0011 lr: 0.02\n",
      "iteration: 393700 loss: 0.0011 lr: 0.02\n",
      "iteration: 393800 loss: 0.0012 lr: 0.02\n",
      "iteration: 393900 loss: 0.0011 lr: 0.02\n",
      "iteration: 394000 loss: 0.0011 lr: 0.02\n",
      "iteration: 394100 loss: 0.0011 lr: 0.02\n",
      "iteration: 394200 loss: 0.0010 lr: 0.02\n",
      "iteration: 394300 loss: 0.0010 lr: 0.02\n",
      "iteration: 394400 loss: 0.0011 lr: 0.02\n",
      "iteration: 394500 loss: 0.0011 lr: 0.02\n",
      "iteration: 394600 loss: 0.0011 lr: 0.02\n",
      "iteration: 394700 loss: 0.0010 lr: 0.02\n",
      "iteration: 394800 loss: 0.0011 lr: 0.02\n",
      "iteration: 394900 loss: 0.0012 lr: 0.02\n",
      "iteration: 395000 loss: 0.0012 lr: 0.02\n",
      "iteration: 395100 loss: 0.0011 lr: 0.02\n",
      "iteration: 395200 loss: 0.0012 lr: 0.02\n",
      "iteration: 395300 loss: 0.0012 lr: 0.02\n",
      "iteration: 395400 loss: 0.0011 lr: 0.02\n",
      "iteration: 395500 loss: 0.0011 lr: 0.02\n",
      "iteration: 395600 loss: 0.0011 lr: 0.02\n",
      "iteration: 395700 loss: 0.0012 lr: 0.02\n",
      "iteration: 395800 loss: 0.0012 lr: 0.02\n",
      "iteration: 395900 loss: 0.0011 lr: 0.02\n",
      "iteration: 396000 loss: 0.0012 lr: 0.02\n",
      "iteration: 396100 loss: 0.0011 lr: 0.02\n",
      "iteration: 396200 loss: 0.0010 lr: 0.02\n",
      "iteration: 396300 loss: 0.0011 lr: 0.02\n",
      "iteration: 396400 loss: 0.0011 lr: 0.02\n",
      "iteration: 396500 loss: 0.0011 lr: 0.02\n",
      "iteration: 396600 loss: 0.0011 lr: 0.02\n",
      "iteration: 396700 loss: 0.0010 lr: 0.02\n",
      "iteration: 396800 loss: 0.0012 lr: 0.02\n",
      "iteration: 396900 loss: 0.0011 lr: 0.02\n",
      "iteration: 397000 loss: 0.0010 lr: 0.02\n",
      "iteration: 397100 loss: 0.0010 lr: 0.02\n",
      "iteration: 397200 loss: 0.0010 lr: 0.02\n",
      "iteration: 397300 loss: 0.0011 lr: 0.02\n",
      "iteration: 397400 loss: 0.0011 lr: 0.02\n",
      "iteration: 397500 loss: 0.0011 lr: 0.02\n",
      "iteration: 397600 loss: 0.0009 lr: 0.02\n",
      "iteration: 397700 loss: 0.0011 lr: 0.02\n",
      "iteration: 397800 loss: 0.0011 lr: 0.02\n",
      "iteration: 397900 loss: 0.0012 lr: 0.02\n",
      "iteration: 398000 loss: 0.0011 lr: 0.02\n",
      "iteration: 398100 loss: 0.0010 lr: 0.02\n",
      "iteration: 398200 loss: 0.0012 lr: 0.02\n",
      "iteration: 398300 loss: 0.0010 lr: 0.02\n",
      "iteration: 398400 loss: 0.0012 lr: 0.02\n",
      "iteration: 398500 loss: 0.0012 lr: 0.02\n",
      "iteration: 398600 loss: 0.0012 lr: 0.02\n",
      "iteration: 398700 loss: 0.0012 lr: 0.02\n",
      "iteration: 398800 loss: 0.0011 lr: 0.02\n",
      "iteration: 398900 loss: 0.0011 lr: 0.02\n",
      "iteration: 399000 loss: 0.0011 lr: 0.02\n",
      "iteration: 399100 loss: 0.0012 lr: 0.02\n",
      "iteration: 399200 loss: 0.0012 lr: 0.02\n",
      "iteration: 399300 loss: 0.0011 lr: 0.02\n",
      "iteration: 399400 loss: 0.0011 lr: 0.02\n",
      "iteration: 399500 loss: 0.0012 lr: 0.02\n",
      "iteration: 399600 loss: 0.0012 lr: 0.02\n",
      "iteration: 399700 loss: 0.0011 lr: 0.02\n",
      "iteration: 399800 loss: 0.0010 lr: 0.02\n",
      "iteration: 399900 loss: 0.0011 lr: 0.02\n",
      "iteration: 400000 loss: 0.0013 lr: 0.02\n",
      "iteration: 400100 loss: 0.0011 lr: 0.02\n",
      "iteration: 400200 loss: 0.0012 lr: 0.02\n",
      "iteration: 400300 loss: 0.0010 lr: 0.02\n",
      "iteration: 400400 loss: 0.0013 lr: 0.02\n",
      "iteration: 400500 loss: 0.0012 lr: 0.02\n",
      "iteration: 400600 loss: 0.0010 lr: 0.02\n",
      "iteration: 400700 loss: 0.0011 lr: 0.02\n",
      "iteration: 400800 loss: 0.0010 lr: 0.02\n",
      "iteration: 400900 loss: 0.0011 lr: 0.02\n",
      "iteration: 401000 loss: 0.0013 lr: 0.02\n",
      "iteration: 401100 loss: 0.0011 lr: 0.02\n",
      "iteration: 401200 loss: 0.0010 lr: 0.02\n",
      "iteration: 401300 loss: 0.0010 lr: 0.02\n",
      "iteration: 401400 loss: 0.0011 lr: 0.02\n",
      "iteration: 401500 loss: 0.0012 lr: 0.02\n",
      "iteration: 401600 loss: 0.0011 lr: 0.02\n",
      "iteration: 401700 loss: 0.0010 lr: 0.02\n",
      "iteration: 401800 loss: 0.0011 lr: 0.02\n",
      "iteration: 401900 loss: 0.0011 lr: 0.02\n",
      "iteration: 402000 loss: 0.0011 lr: 0.02\n",
      "iteration: 402100 loss: 0.0011 lr: 0.02\n",
      "iteration: 402200 loss: 0.0011 lr: 0.02\n",
      "iteration: 402300 loss: 0.0011 lr: 0.02\n",
      "iteration: 402400 loss: 0.0011 lr: 0.02\n",
      "iteration: 402500 loss: 0.0011 lr: 0.02\n",
      "iteration: 402600 loss: 0.0011 lr: 0.02\n",
      "iteration: 402700 loss: 0.0011 lr: 0.02\n",
      "iteration: 402800 loss: 0.0010 lr: 0.02\n",
      "iteration: 402900 loss: 0.0013 lr: 0.02\n",
      "iteration: 403000 loss: 0.0011 lr: 0.02\n",
      "iteration: 403100 loss: 0.0011 lr: 0.02\n",
      "iteration: 403200 loss: 0.0011 lr: 0.02\n",
      "iteration: 403300 loss: 0.0011 lr: 0.02\n",
      "iteration: 403400 loss: 0.0012 lr: 0.02\n",
      "iteration: 403500 loss: 0.0012 lr: 0.02\n",
      "iteration: 403600 loss: 0.0011 lr: 0.02\n",
      "iteration: 403700 loss: 0.0011 lr: 0.02\n",
      "iteration: 403800 loss: 0.0013 lr: 0.02\n",
      "iteration: 403900 loss: 0.0010 lr: 0.02\n",
      "iteration: 404000 loss: 0.0011 lr: 0.02\n",
      "iteration: 404100 loss: 0.0011 lr: 0.02\n",
      "iteration: 404200 loss: 0.0011 lr: 0.02\n",
      "iteration: 404300 loss: 0.0013 lr: 0.02\n",
      "iteration: 404400 loss: 0.0012 lr: 0.02\n",
      "iteration: 404500 loss: 0.0011 lr: 0.02\n",
      "iteration: 404600 loss: 0.0011 lr: 0.02\n",
      "iteration: 404700 loss: 0.0011 lr: 0.02\n",
      "iteration: 404800 loss: 0.0011 lr: 0.02\n",
      "iteration: 404900 loss: 0.0010 lr: 0.02\n",
      "iteration: 405000 loss: 0.0011 lr: 0.02\n",
      "iteration: 405100 loss: 0.0012 lr: 0.02\n",
      "iteration: 405200 loss: 0.0011 lr: 0.02\n",
      "iteration: 405300 loss: 0.0010 lr: 0.02\n",
      "iteration: 405400 loss: 0.0011 lr: 0.02\n",
      "iteration: 405500 loss: 0.0011 lr: 0.02\n",
      "iteration: 405600 loss: 0.0011 lr: 0.02\n",
      "iteration: 405700 loss: 0.0011 lr: 0.02\n",
      "iteration: 405800 loss: 0.0011 lr: 0.02\n",
      "iteration: 405900 loss: 0.0011 lr: 0.02\n",
      "iteration: 406000 loss: 0.0011 lr: 0.02\n",
      "iteration: 406100 loss: 0.0011 lr: 0.02\n",
      "iteration: 406200 loss: 0.0012 lr: 0.02\n",
      "iteration: 406300 loss: 0.0012 lr: 0.02\n",
      "iteration: 406400 loss: 0.0011 lr: 0.02\n",
      "iteration: 406500 loss: 0.0012 lr: 0.02\n",
      "iteration: 406600 loss: 0.0010 lr: 0.02\n",
      "iteration: 406700 loss: 0.0011 lr: 0.02\n",
      "iteration: 406800 loss: 0.0012 lr: 0.02\n",
      "iteration: 406900 loss: 0.0013 lr: 0.02\n",
      "iteration: 407000 loss: 0.0011 lr: 0.02\n",
      "iteration: 407100 loss: 0.0011 lr: 0.02\n",
      "iteration: 407200 loss: 0.0011 lr: 0.02\n",
      "iteration: 407300 loss: 0.0011 lr: 0.02\n",
      "iteration: 407400 loss: 0.0011 lr: 0.02\n",
      "iteration: 407500 loss: 0.0011 lr: 0.02\n",
      "iteration: 407600 loss: 0.0011 lr: 0.02\n",
      "iteration: 407700 loss: 0.0010 lr: 0.02\n",
      "iteration: 407800 loss: 0.0012 lr: 0.02\n",
      "iteration: 407900 loss: 0.0010 lr: 0.02\n",
      "iteration: 408000 loss: 0.0011 lr: 0.02\n",
      "iteration: 408100 loss: 0.0011 lr: 0.02\n",
      "iteration: 408200 loss: 0.0011 lr: 0.02\n",
      "iteration: 408300 loss: 0.0011 lr: 0.02\n",
      "iteration: 408400 loss: 0.0011 lr: 0.02\n",
      "iteration: 408500 loss: 0.0012 lr: 0.02\n",
      "iteration: 408600 loss: 0.0011 lr: 0.02\n",
      "iteration: 408700 loss: 0.0011 lr: 0.02\n",
      "iteration: 408800 loss: 0.0010 lr: 0.02\n",
      "iteration: 408900 loss: 0.0012 lr: 0.02\n",
      "iteration: 409000 loss: 0.0011 lr: 0.02\n",
      "iteration: 409100 loss: 0.0012 lr: 0.02\n",
      "iteration: 409200 loss: 0.0012 lr: 0.02\n",
      "iteration: 409300 loss: 0.0012 lr: 0.02\n",
      "iteration: 409400 loss: 0.0011 lr: 0.02\n",
      "iteration: 409500 loss: 0.0011 lr: 0.02\n",
      "iteration: 409600 loss: 0.0011 lr: 0.02\n",
      "iteration: 409700 loss: 0.0011 lr: 0.02\n",
      "iteration: 409800 loss: 0.0010 lr: 0.02\n",
      "iteration: 409900 loss: 0.0011 lr: 0.02\n",
      "iteration: 410000 loss: 0.0010 lr: 0.02\n",
      "iteration: 410100 loss: 0.0012 lr: 0.02\n",
      "iteration: 410200 loss: 0.0011 lr: 0.02\n",
      "iteration: 410300 loss: 0.0011 lr: 0.02\n",
      "iteration: 410400 loss: 0.0010 lr: 0.02\n",
      "iteration: 410500 loss: 0.0011 lr: 0.02\n",
      "iteration: 410600 loss: 0.0011 lr: 0.02\n",
      "iteration: 410700 loss: 0.0011 lr: 0.02\n",
      "iteration: 410800 loss: 0.0011 lr: 0.02\n",
      "iteration: 410900 loss: 0.0010 lr: 0.02\n",
      "iteration: 411000 loss: 0.0010 lr: 0.02\n",
      "iteration: 411100 loss: 0.0010 lr: 0.02\n",
      "iteration: 411200 loss: 0.0010 lr: 0.02\n",
      "iteration: 411300 loss: 0.0011 lr: 0.02\n",
      "iteration: 411400 loss: 0.0011 lr: 0.02\n",
      "iteration: 411500 loss: 0.0012 lr: 0.02\n",
      "iteration: 411600 loss: 0.0011 lr: 0.02\n",
      "iteration: 411700 loss: 0.0011 lr: 0.02\n",
      "iteration: 411800 loss: 0.0010 lr: 0.02\n",
      "iteration: 411900 loss: 0.0011 lr: 0.02\n",
      "iteration: 412000 loss: 0.0013 lr: 0.02\n",
      "iteration: 412100 loss: 0.0011 lr: 0.02\n",
      "iteration: 412200 loss: 0.0011 lr: 0.02\n",
      "iteration: 412300 loss: 0.0011 lr: 0.02\n",
      "iteration: 412400 loss: 0.0010 lr: 0.02\n",
      "iteration: 412500 loss: 0.0012 lr: 0.02\n",
      "iteration: 412600 loss: 0.0012 lr: 0.02\n",
      "iteration: 412700 loss: 0.0012 lr: 0.02\n",
      "iteration: 412800 loss: 0.0012 lr: 0.02\n",
      "iteration: 412900 loss: 0.0012 lr: 0.02\n",
      "iteration: 413000 loss: 0.0010 lr: 0.02\n",
      "iteration: 413100 loss: 0.0011 lr: 0.02\n",
      "iteration: 413200 loss: 0.0012 lr: 0.02\n",
      "iteration: 413300 loss: 0.0011 lr: 0.02\n",
      "iteration: 413400 loss: 0.0010 lr: 0.02\n",
      "iteration: 413500 loss: 0.0011 lr: 0.02\n",
      "iteration: 413600 loss: 0.0011 lr: 0.02\n",
      "iteration: 413700 loss: 0.0012 lr: 0.02\n",
      "iteration: 413800 loss: 0.0010 lr: 0.02\n",
      "iteration: 413900 loss: 0.0010 lr: 0.02\n",
      "iteration: 414000 loss: 0.0012 lr: 0.02\n",
      "iteration: 414100 loss: 0.0010 lr: 0.02\n",
      "iteration: 414200 loss: 0.0010 lr: 0.02\n",
      "iteration: 414300 loss: 0.0012 lr: 0.02\n",
      "iteration: 414400 loss: 0.0011 lr: 0.02\n",
      "iteration: 414500 loss: 0.0011 lr: 0.02\n",
      "iteration: 414600 loss: 0.0011 lr: 0.02\n",
      "iteration: 414700 loss: 0.0011 lr: 0.02\n",
      "iteration: 414800 loss: 0.0011 lr: 0.02\n",
      "iteration: 414900 loss: 0.0011 lr: 0.02\n",
      "iteration: 415000 loss: 0.0012 lr: 0.02\n",
      "iteration: 415100 loss: 0.0011 lr: 0.02\n",
      "iteration: 415200 loss: 0.0011 lr: 0.02\n",
      "iteration: 415300 loss: 0.0012 lr: 0.02\n",
      "iteration: 415400 loss: 0.0012 lr: 0.02\n",
      "iteration: 415500 loss: 0.0012 lr: 0.02\n",
      "iteration: 415600 loss: 0.0013 lr: 0.02\n",
      "iteration: 415700 loss: 0.0011 lr: 0.02\n",
      "iteration: 415800 loss: 0.0013 lr: 0.02\n",
      "iteration: 415900 loss: 0.0011 lr: 0.02\n",
      "iteration: 416000 loss: 0.0010 lr: 0.02\n",
      "iteration: 416100 loss: 0.0011 lr: 0.02\n",
      "iteration: 416200 loss: 0.0010 lr: 0.02\n",
      "iteration: 416300 loss: 0.0011 lr: 0.02\n",
      "iteration: 416400 loss: 0.0011 lr: 0.02\n",
      "iteration: 416500 loss: 0.0010 lr: 0.02\n",
      "iteration: 416600 loss: 0.0011 lr: 0.02\n",
      "iteration: 416700 loss: 0.0011 lr: 0.02\n",
      "iteration: 416800 loss: 0.0011 lr: 0.02\n",
      "iteration: 416900 loss: 0.0011 lr: 0.02\n",
      "iteration: 417000 loss: 0.0011 lr: 0.02\n",
      "iteration: 417100 loss: 0.0010 lr: 0.02\n",
      "iteration: 417200 loss: 0.0011 lr: 0.02\n",
      "iteration: 417300 loss: 0.0011 lr: 0.02\n",
      "iteration: 417400 loss: 0.0011 lr: 0.02\n",
      "iteration: 417500 loss: 0.0011 lr: 0.02\n",
      "iteration: 417600 loss: 0.0010 lr: 0.02\n",
      "iteration: 417700 loss: 0.0012 lr: 0.02\n",
      "iteration: 417800 loss: 0.0012 lr: 0.02\n",
      "iteration: 417900 loss: 0.0011 lr: 0.02\n",
      "iteration: 418000 loss: 0.0011 lr: 0.02\n",
      "iteration: 418100 loss: 0.0010 lr: 0.02\n",
      "iteration: 418200 loss: 0.0012 lr: 0.02\n",
      "iteration: 418300 loss: 0.0011 lr: 0.02\n",
      "iteration: 418400 loss: 0.0011 lr: 0.02\n",
      "iteration: 418500 loss: 0.0012 lr: 0.02\n",
      "iteration: 418600 loss: 0.0012 lr: 0.02\n",
      "iteration: 418700 loss: 0.0011 lr: 0.02\n",
      "iteration: 418800 loss: 0.0012 lr: 0.02\n",
      "iteration: 418900 loss: 0.0010 lr: 0.02\n",
      "iteration: 419000 loss: 0.0012 lr: 0.02\n",
      "iteration: 419100 loss: 0.0011 lr: 0.02\n",
      "iteration: 419200 loss: 0.0011 lr: 0.02\n",
      "iteration: 419300 loss: 0.0010 lr: 0.02\n",
      "iteration: 419400 loss: 0.0012 lr: 0.02\n",
      "iteration: 419500 loss: 0.0012 lr: 0.02\n",
      "iteration: 419600 loss: 0.0012 lr: 0.02\n",
      "iteration: 419700 loss: 0.0011 lr: 0.02\n",
      "iteration: 419800 loss: 0.0012 lr: 0.02\n",
      "iteration: 419900 loss: 0.0010 lr: 0.02\n",
      "iteration: 420000 loss: 0.0011 lr: 0.02\n",
      "iteration: 420100 loss: 0.0011 lr: 0.02\n",
      "iteration: 420200 loss: 0.0010 lr: 0.02\n",
      "iteration: 420300 loss: 0.0010 lr: 0.02\n",
      "iteration: 420400 loss: 0.0012 lr: 0.02\n",
      "iteration: 420500 loss: 0.0012 lr: 0.02\n",
      "iteration: 420600 loss: 0.0012 lr: 0.02\n",
      "iteration: 420700 loss: 0.0011 lr: 0.02\n",
      "iteration: 420800 loss: 0.0011 lr: 0.02\n",
      "iteration: 420900 loss: 0.0011 lr: 0.02\n",
      "iteration: 421000 loss: 0.0010 lr: 0.02\n",
      "iteration: 421100 loss: 0.0011 lr: 0.02\n",
      "iteration: 421200 loss: 0.0012 lr: 0.02\n",
      "iteration: 421300 loss: 0.0011 lr: 0.02\n",
      "iteration: 421400 loss: 0.0010 lr: 0.02\n",
      "iteration: 421500 loss: 0.0012 lr: 0.02\n",
      "iteration: 421600 loss: 0.0010 lr: 0.02\n",
      "iteration: 421700 loss: 0.0011 lr: 0.02\n",
      "iteration: 421800 loss: 0.0011 lr: 0.02\n",
      "iteration: 421900 loss: 0.0011 lr: 0.02\n",
      "iteration: 422000 loss: 0.0011 lr: 0.02\n",
      "iteration: 422100 loss: 0.0011 lr: 0.02\n",
      "iteration: 422200 loss: 0.0011 lr: 0.02\n",
      "iteration: 422300 loss: 0.0011 lr: 0.02\n",
      "iteration: 422400 loss: 0.0010 lr: 0.02\n",
      "iteration: 422500 loss: 0.0011 lr: 0.02\n",
      "iteration: 422600 loss: 0.0012 lr: 0.02\n",
      "iteration: 422700 loss: 0.0010 lr: 0.02\n",
      "iteration: 422800 loss: 0.0012 lr: 0.02\n",
      "iteration: 422900 loss: 0.0011 lr: 0.02\n",
      "iteration: 423000 loss: 0.0010 lr: 0.02\n",
      "iteration: 423100 loss: 0.0010 lr: 0.02\n",
      "iteration: 423200 loss: 0.0013 lr: 0.02\n",
      "iteration: 423300 loss: 0.0012 lr: 0.02\n",
      "iteration: 423400 loss: 0.0010 lr: 0.02\n",
      "iteration: 423500 loss: 0.0009 lr: 0.02\n",
      "iteration: 423600 loss: 0.0011 lr: 0.02\n",
      "iteration: 423700 loss: 0.0010 lr: 0.02\n",
      "iteration: 423800 loss: 0.0011 lr: 0.02\n",
      "iteration: 423900 loss: 0.0011 lr: 0.02\n",
      "iteration: 424000 loss: 0.0011 lr: 0.02\n",
      "iteration: 424100 loss: 0.0011 lr: 0.02\n",
      "iteration: 424200 loss: 0.0011 lr: 0.02\n",
      "iteration: 424300 loss: 0.0011 lr: 0.02\n",
      "iteration: 424400 loss: 0.0010 lr: 0.02\n",
      "iteration: 424500 loss: 0.0010 lr: 0.02\n",
      "iteration: 424600 loss: 0.0011 lr: 0.02\n",
      "iteration: 424700 loss: 0.0011 lr: 0.02\n",
      "iteration: 424800 loss: 0.0010 lr: 0.02\n",
      "iteration: 424900 loss: 0.0010 lr: 0.02\n",
      "iteration: 425000 loss: 0.0011 lr: 0.02\n",
      "iteration: 425100 loss: 0.0011 lr: 0.02\n",
      "iteration: 425200 loss: 0.0011 lr: 0.02\n",
      "iteration: 425300 loss: 0.0011 lr: 0.02\n",
      "iteration: 425400 loss: 0.0011 lr: 0.02\n",
      "iteration: 425500 loss: 0.0011 lr: 0.02\n",
      "iteration: 425600 loss: 0.0011 lr: 0.02\n",
      "iteration: 425700 loss: 0.0011 lr: 0.02\n",
      "iteration: 425800 loss: 0.0011 lr: 0.02\n",
      "iteration: 425900 loss: 0.0010 lr: 0.02\n",
      "iteration: 426000 loss: 0.0011 lr: 0.02\n",
      "iteration: 426100 loss: 0.0011 lr: 0.02\n",
      "iteration: 426200 loss: 0.0010 lr: 0.02\n",
      "iteration: 426300 loss: 0.0012 lr: 0.02\n",
      "iteration: 426400 loss: 0.0010 lr: 0.02\n",
      "iteration: 426500 loss: 0.0011 lr: 0.02\n",
      "iteration: 426600 loss: 0.0010 lr: 0.02\n",
      "iteration: 426700 loss: 0.0010 lr: 0.02\n",
      "iteration: 426800 loss: 0.0011 lr: 0.02\n",
      "iteration: 426900 loss: 0.0011 lr: 0.02\n",
      "iteration: 427000 loss: 0.0012 lr: 0.02\n",
      "iteration: 427100 loss: 0.0010 lr: 0.02\n",
      "iteration: 427200 loss: 0.0011 lr: 0.02\n",
      "iteration: 427300 loss: 0.0010 lr: 0.02\n",
      "iteration: 427400 loss: 0.0011 lr: 0.02\n",
      "iteration: 427500 loss: 0.0012 lr: 0.02\n",
      "iteration: 427600 loss: 0.0011 lr: 0.02\n",
      "iteration: 427700 loss: 0.0011 lr: 0.02\n",
      "iteration: 427800 loss: 0.0013 lr: 0.02\n",
      "iteration: 427900 loss: 0.0012 lr: 0.02\n",
      "iteration: 428000 loss: 0.0012 lr: 0.02\n",
      "iteration: 428100 loss: 0.0011 lr: 0.02\n",
      "iteration: 428200 loss: 0.0011 lr: 0.02\n",
      "iteration: 428300 loss: 0.0011 lr: 0.02\n",
      "iteration: 428400 loss: 0.0010 lr: 0.02\n",
      "iteration: 428500 loss: 0.0010 lr: 0.02\n",
      "iteration: 428600 loss: 0.0011 lr: 0.02\n",
      "iteration: 428700 loss: 0.0010 lr: 0.02\n",
      "iteration: 428800 loss: 0.0010 lr: 0.02\n",
      "iteration: 428900 loss: 0.0012 lr: 0.02\n",
      "iteration: 429000 loss: 0.0011 lr: 0.02\n",
      "iteration: 429100 loss: 0.0011 lr: 0.02\n",
      "iteration: 429200 loss: 0.0010 lr: 0.02\n",
      "iteration: 429300 loss: 0.0010 lr: 0.02\n",
      "iteration: 429400 loss: 0.0010 lr: 0.02\n",
      "iteration: 429500 loss: 0.0012 lr: 0.02\n",
      "iteration: 429600 loss: 0.0011 lr: 0.02\n",
      "iteration: 429700 loss: 0.0011 lr: 0.02\n",
      "iteration: 429800 loss: 0.0011 lr: 0.02\n",
      "iteration: 429900 loss: 0.0011 lr: 0.02\n",
      "iteration: 430000 loss: 0.0011 lr: 0.02\n",
      "iteration: 430100 loss: 0.0011 lr: 0.002\n",
      "iteration: 430200 loss: 0.0011 lr: 0.002\n",
      "iteration: 430300 loss: 0.0010 lr: 0.002\n",
      "iteration: 430400 loss: 0.0010 lr: 0.002\n",
      "iteration: 430500 loss: 0.0010 lr: 0.002\n",
      "iteration: 430600 loss: 0.0011 lr: 0.002\n",
      "iteration: 430700 loss: 0.0011 lr: 0.002\n",
      "iteration: 430800 loss: 0.0009 lr: 0.002\n",
      "iteration: 430900 loss: 0.0009 lr: 0.002\n",
      "iteration: 431000 loss: 0.0009 lr: 0.002\n",
      "iteration: 431100 loss: 0.0010 lr: 0.002\n",
      "iteration: 431200 loss: 0.0011 lr: 0.002\n",
      "iteration: 431300 loss: 0.0009 lr: 0.002\n",
      "iteration: 431400 loss: 0.0009 lr: 0.002\n",
      "iteration: 431500 loss: 0.0010 lr: 0.002\n",
      "iteration: 431600 loss: 0.0010 lr: 0.002\n",
      "iteration: 431700 loss: 0.0010 lr: 0.002\n",
      "iteration: 431800 loss: 0.0010 lr: 0.002\n",
      "iteration: 431900 loss: 0.0010 lr: 0.002\n",
      "iteration: 432000 loss: 0.0010 lr: 0.002\n",
      "iteration: 432100 loss: 0.0009 lr: 0.002\n",
      "iteration: 432200 loss: 0.0010 lr: 0.002\n",
      "iteration: 432300 loss: 0.0009 lr: 0.002\n",
      "iteration: 432400 loss: 0.0010 lr: 0.002\n",
      "iteration: 432500 loss: 0.0009 lr: 0.002\n",
      "iteration: 432600 loss: 0.0010 lr: 0.002\n",
      "iteration: 432700 loss: 0.0010 lr: 0.002\n",
      "iteration: 432800 loss: 0.0010 lr: 0.002\n",
      "iteration: 432900 loss: 0.0010 lr: 0.002\n",
      "iteration: 433000 loss: 0.0010 lr: 0.002\n",
      "iteration: 433100 loss: 0.0010 lr: 0.002\n",
      "iteration: 433200 loss: 0.0010 lr: 0.002\n",
      "iteration: 433300 loss: 0.0008 lr: 0.002\n",
      "iteration: 433400 loss: 0.0011 lr: 0.002\n",
      "iteration: 433500 loss: 0.0008 lr: 0.002\n",
      "iteration: 433600 loss: 0.0008 lr: 0.002\n",
      "iteration: 433700 loss: 0.0009 lr: 0.002\n",
      "iteration: 433800 loss: 0.0009 lr: 0.002\n",
      "iteration: 433900 loss: 0.0010 lr: 0.002\n",
      "iteration: 434000 loss: 0.0009 lr: 0.002\n",
      "iteration: 434100 loss: 0.0010 lr: 0.002\n",
      "iteration: 434200 loss: 0.0010 lr: 0.002\n",
      "iteration: 434300 loss: 0.0009 lr: 0.002\n",
      "iteration: 434400 loss: 0.0009 lr: 0.002\n",
      "iteration: 434500 loss: 0.0009 lr: 0.002\n",
      "iteration: 434600 loss: 0.0010 lr: 0.002\n",
      "iteration: 434700 loss: 0.0009 lr: 0.002\n",
      "iteration: 434800 loss: 0.0009 lr: 0.002\n",
      "iteration: 434900 loss: 0.0008 lr: 0.002\n",
      "iteration: 435000 loss: 0.0010 lr: 0.002\n",
      "iteration: 435100 loss: 0.0008 lr: 0.002\n",
      "iteration: 435200 loss: 0.0009 lr: 0.002\n",
      "iteration: 435300 loss: 0.0011 lr: 0.002\n",
      "iteration: 435400 loss: 0.0010 lr: 0.002\n",
      "iteration: 435500 loss: 0.0008 lr: 0.002\n",
      "iteration: 435600 loss: 0.0010 lr: 0.002\n",
      "iteration: 435700 loss: 0.0009 lr: 0.002\n",
      "iteration: 435800 loss: 0.0009 lr: 0.002\n",
      "iteration: 435900 loss: 0.0009 lr: 0.002\n",
      "iteration: 436000 loss: 0.0009 lr: 0.002\n",
      "iteration: 436100 loss: 0.0009 lr: 0.002\n",
      "iteration: 436200 loss: 0.0010 lr: 0.002\n",
      "iteration: 436300 loss: 0.0010 lr: 0.002\n",
      "iteration: 436400 loss: 0.0009 lr: 0.002\n",
      "iteration: 436500 loss: 0.0009 lr: 0.002\n",
      "iteration: 436600 loss: 0.0010 lr: 0.002\n",
      "iteration: 436700 loss: 0.0009 lr: 0.002\n",
      "iteration: 436800 loss: 0.0010 lr: 0.002\n",
      "iteration: 436900 loss: 0.0009 lr: 0.002\n",
      "iteration: 437000 loss: 0.0009 lr: 0.002\n",
      "iteration: 437100 loss: 0.0010 lr: 0.002\n",
      "iteration: 437200 loss: 0.0009 lr: 0.002\n",
      "iteration: 437300 loss: 0.0009 lr: 0.002\n",
      "iteration: 437400 loss: 0.0008 lr: 0.002\n",
      "iteration: 437500 loss: 0.0009 lr: 0.002\n",
      "iteration: 437600 loss: 0.0010 lr: 0.002\n",
      "iteration: 437700 loss: 0.0010 lr: 0.002\n",
      "iteration: 437800 loss: 0.0009 lr: 0.002\n",
      "iteration: 437900 loss: 0.0009 lr: 0.002\n",
      "iteration: 438000 loss: 0.0010 lr: 0.002\n",
      "iteration: 438100 loss: 0.0009 lr: 0.002\n",
      "iteration: 438200 loss: 0.0010 lr: 0.002\n",
      "iteration: 438300 loss: 0.0008 lr: 0.002\n",
      "iteration: 438400 loss: 0.0010 lr: 0.002\n",
      "iteration: 438500 loss: 0.0009 lr: 0.002\n",
      "iteration: 438600 loss: 0.0009 lr: 0.002\n",
      "iteration: 438700 loss: 0.0010 lr: 0.002\n",
      "iteration: 438800 loss: 0.0010 lr: 0.002\n",
      "iteration: 438900 loss: 0.0009 lr: 0.002\n",
      "iteration: 439000 loss: 0.0009 lr: 0.002\n",
      "iteration: 439100 loss: 0.0009 lr: 0.002\n",
      "iteration: 439200 loss: 0.0011 lr: 0.002\n",
      "iteration: 439300 loss: 0.0011 lr: 0.002\n",
      "iteration: 439400 loss: 0.0009 lr: 0.002\n",
      "iteration: 439500 loss: 0.0009 lr: 0.002\n",
      "iteration: 439600 loss: 0.0009 lr: 0.002\n",
      "iteration: 439700 loss: 0.0010 lr: 0.002\n",
      "iteration: 439800 loss: 0.0008 lr: 0.002\n",
      "iteration: 439900 loss: 0.0010 lr: 0.002\n",
      "iteration: 440000 loss: 0.0009 lr: 0.002\n",
      "iteration: 440100 loss: 0.0010 lr: 0.002\n",
      "iteration: 440200 loss: 0.0009 lr: 0.002\n",
      "iteration: 440300 loss: 0.0010 lr: 0.002\n",
      "iteration: 440400 loss: 0.0009 lr: 0.002\n",
      "iteration: 440500 loss: 0.0010 lr: 0.002\n",
      "iteration: 440600 loss: 0.0010 lr: 0.002\n",
      "iteration: 440700 loss: 0.0008 lr: 0.002\n",
      "iteration: 440800 loss: 0.0009 lr: 0.002\n",
      "iteration: 440900 loss: 0.0008 lr: 0.002\n",
      "iteration: 441000 loss: 0.0010 lr: 0.002\n",
      "iteration: 441100 loss: 0.0010 lr: 0.002\n",
      "iteration: 441200 loss: 0.0010 lr: 0.002\n",
      "iteration: 441300 loss: 0.0010 lr: 0.002\n",
      "iteration: 441400 loss: 0.0010 lr: 0.002\n",
      "iteration: 441500 loss: 0.0009 lr: 0.002\n",
      "iteration: 441600 loss: 0.0010 lr: 0.002\n",
      "iteration: 441700 loss: 0.0009 lr: 0.002\n",
      "iteration: 441800 loss: 0.0008 lr: 0.002\n",
      "iteration: 441900 loss: 0.0009 lr: 0.002\n",
      "iteration: 442000 loss: 0.0010 lr: 0.002\n",
      "iteration: 442100 loss: 0.0010 lr: 0.002\n",
      "iteration: 442200 loss: 0.0010 lr: 0.002\n",
      "iteration: 442300 loss: 0.0009 lr: 0.002\n",
      "iteration: 442400 loss: 0.0008 lr: 0.002\n",
      "iteration: 442500 loss: 0.0009 lr: 0.002\n",
      "iteration: 442600 loss: 0.0010 lr: 0.002\n",
      "iteration: 442700 loss: 0.0009 lr: 0.002\n",
      "iteration: 442800 loss: 0.0010 lr: 0.002\n",
      "iteration: 442900 loss: 0.0009 lr: 0.002\n",
      "iteration: 443000 loss: 0.0009 lr: 0.002\n",
      "iteration: 443100 loss: 0.0009 lr: 0.002\n",
      "iteration: 443200 loss: 0.0010 lr: 0.002\n",
      "iteration: 443300 loss: 0.0009 lr: 0.002\n",
      "iteration: 443400 loss: 0.0009 lr: 0.002\n",
      "iteration: 443500 loss: 0.0010 lr: 0.002\n",
      "iteration: 443600 loss: 0.0009 lr: 0.002\n",
      "iteration: 443700 loss: 0.0008 lr: 0.002\n",
      "iteration: 443800 loss: 0.0009 lr: 0.002\n",
      "iteration: 443900 loss: 0.0009 lr: 0.002\n",
      "iteration: 444000 loss: 0.0009 lr: 0.002\n",
      "iteration: 444100 loss: 0.0010 lr: 0.002\n",
      "iteration: 444200 loss: 0.0009 lr: 0.002\n",
      "iteration: 444300 loss: 0.0010 lr: 0.002\n",
      "iteration: 444400 loss: 0.0009 lr: 0.002\n",
      "iteration: 444500 loss: 0.0008 lr: 0.002\n",
      "iteration: 444600 loss: 0.0009 lr: 0.002\n",
      "iteration: 444700 loss: 0.0009 lr: 0.002\n",
      "iteration: 444800 loss: 0.0009 lr: 0.002\n",
      "iteration: 444900 loss: 0.0010 lr: 0.002\n",
      "iteration: 445000 loss: 0.0009 lr: 0.002\n",
      "iteration: 445100 loss: 0.0010 lr: 0.002\n",
      "iteration: 445200 loss: 0.0010 lr: 0.002\n",
      "iteration: 445300 loss: 0.0008 lr: 0.002\n",
      "iteration: 445400 loss: 0.0009 lr: 0.002\n",
      "iteration: 445500 loss: 0.0010 lr: 0.002\n",
      "iteration: 445600 loss: 0.0009 lr: 0.002\n",
      "iteration: 445700 loss: 0.0009 lr: 0.002\n",
      "iteration: 445800 loss: 0.0010 lr: 0.002\n",
      "iteration: 445900 loss: 0.0009 lr: 0.002\n",
      "iteration: 446000 loss: 0.0008 lr: 0.002\n",
      "iteration: 446100 loss: 0.0009 lr: 0.002\n",
      "iteration: 446200 loss: 0.0009 lr: 0.002\n",
      "iteration: 446300 loss: 0.0009 lr: 0.002\n",
      "iteration: 446400 loss: 0.0009 lr: 0.002\n",
      "iteration: 446500 loss: 0.0010 lr: 0.002\n",
      "iteration: 446600 loss: 0.0009 lr: 0.002\n",
      "iteration: 446700 loss: 0.0008 lr: 0.002\n",
      "iteration: 446800 loss: 0.0009 lr: 0.002\n",
      "iteration: 446900 loss: 0.0009 lr: 0.002\n",
      "iteration: 447000 loss: 0.0009 lr: 0.002\n",
      "iteration: 447100 loss: 0.0008 lr: 0.002\n",
      "iteration: 447200 loss: 0.0010 lr: 0.002\n",
      "iteration: 447300 loss: 0.0010 lr: 0.002\n",
      "iteration: 447400 loss: 0.0009 lr: 0.002\n",
      "iteration: 447500 loss: 0.0009 lr: 0.002\n",
      "iteration: 447600 loss: 0.0010 lr: 0.002\n",
      "iteration: 447700 loss: 0.0009 lr: 0.002\n",
      "iteration: 447800 loss: 0.0010 lr: 0.002\n",
      "iteration: 447900 loss: 0.0010 lr: 0.002\n",
      "iteration: 448000 loss: 0.0009 lr: 0.002\n",
      "iteration: 448100 loss: 0.0008 lr: 0.002\n",
      "iteration: 448200 loss: 0.0010 lr: 0.002\n",
      "iteration: 448300 loss: 0.0009 lr: 0.002\n",
      "iteration: 448400 loss: 0.0009 lr: 0.002\n",
      "iteration: 448500 loss: 0.0009 lr: 0.002\n",
      "iteration: 448600 loss: 0.0010 lr: 0.002\n",
      "iteration: 448700 loss: 0.0009 lr: 0.002\n",
      "iteration: 448800 loss: 0.0010 lr: 0.002\n",
      "iteration: 448900 loss: 0.0010 lr: 0.002\n",
      "iteration: 449000 loss: 0.0009 lr: 0.002\n",
      "iteration: 449100 loss: 0.0009 lr: 0.002\n",
      "iteration: 449200 loss: 0.0009 lr: 0.002\n",
      "iteration: 449300 loss: 0.0010 lr: 0.002\n",
      "iteration: 449400 loss: 0.0009 lr: 0.002\n",
      "iteration: 449500 loss: 0.0010 lr: 0.002\n",
      "iteration: 449600 loss: 0.0009 lr: 0.002\n",
      "iteration: 449700 loss: 0.0009 lr: 0.002\n",
      "iteration: 449800 loss: 0.0009 lr: 0.002\n",
      "iteration: 449900 loss: 0.0010 lr: 0.002\n",
      "iteration: 450000 loss: 0.0009 lr: 0.002\n",
      "iteration: 450100 loss: 0.0010 lr: 0.002\n",
      "iteration: 450200 loss: 0.0009 lr: 0.002\n",
      "iteration: 450300 loss: 0.0009 lr: 0.002\n",
      "iteration: 450400 loss: 0.0009 lr: 0.002\n",
      "iteration: 450500 loss: 0.0010 lr: 0.002\n",
      "iteration: 450600 loss: 0.0008 lr: 0.002\n",
      "iteration: 450700 loss: 0.0009 lr: 0.002\n",
      "iteration: 450800 loss: 0.0010 lr: 0.002\n",
      "iteration: 450900 loss: 0.0009 lr: 0.002\n",
      "iteration: 451000 loss: 0.0009 lr: 0.002\n",
      "iteration: 451100 loss: 0.0010 lr: 0.002\n",
      "iteration: 451200 loss: 0.0009 lr: 0.002\n",
      "iteration: 451300 loss: 0.0008 lr: 0.002\n",
      "iteration: 451400 loss: 0.0008 lr: 0.002\n",
      "iteration: 451500 loss: 0.0010 lr: 0.002\n",
      "iteration: 451600 loss: 0.0009 lr: 0.002\n",
      "iteration: 451700 loss: 0.0010 lr: 0.002\n",
      "iteration: 451800 loss: 0.0009 lr: 0.002\n",
      "iteration: 451900 loss: 0.0009 lr: 0.002\n",
      "iteration: 452000 loss: 0.0009 lr: 0.002\n",
      "iteration: 452100 loss: 0.0009 lr: 0.002\n",
      "iteration: 452200 loss: 0.0009 lr: 0.002\n",
      "iteration: 452300 loss: 0.0010 lr: 0.002\n",
      "iteration: 452400 loss: 0.0011 lr: 0.002\n",
      "iteration: 452500 loss: 0.0010 lr: 0.002\n",
      "iteration: 452600 loss: 0.0009 lr: 0.002\n",
      "iteration: 452700 loss: 0.0008 lr: 0.002\n",
      "iteration: 452800 loss: 0.0009 lr: 0.002\n",
      "iteration: 452900 loss: 0.0009 lr: 0.002\n",
      "iteration: 453000 loss: 0.0009 lr: 0.002\n",
      "iteration: 453100 loss: 0.0009 lr: 0.002\n",
      "iteration: 453200 loss: 0.0010 lr: 0.002\n",
      "iteration: 453300 loss: 0.0009 lr: 0.002\n",
      "iteration: 453400 loss: 0.0009 lr: 0.002\n",
      "iteration: 453500 loss: 0.0010 lr: 0.002\n",
      "iteration: 453600 loss: 0.0009 lr: 0.002\n",
      "iteration: 453700 loss: 0.0010 lr: 0.002\n",
      "iteration: 453800 loss: 0.0009 lr: 0.002\n",
      "iteration: 453900 loss: 0.0008 lr: 0.002\n",
      "iteration: 454000 loss: 0.0010 lr: 0.002\n",
      "iteration: 454100 loss: 0.0010 lr: 0.002\n",
      "iteration: 454200 loss: 0.0009 lr: 0.002\n",
      "iteration: 454300 loss: 0.0010 lr: 0.002\n",
      "iteration: 454400 loss: 0.0009 lr: 0.002\n",
      "iteration: 454500 loss: 0.0009 lr: 0.002\n",
      "iteration: 454600 loss: 0.0009 lr: 0.002\n",
      "iteration: 454700 loss: 0.0009 lr: 0.002\n",
      "iteration: 454800 loss: 0.0010 lr: 0.002\n",
      "iteration: 454900 loss: 0.0010 lr: 0.002\n",
      "iteration: 455000 loss: 0.0009 lr: 0.002\n",
      "iteration: 455100 loss: 0.0010 lr: 0.002\n",
      "iteration: 455200 loss: 0.0009 lr: 0.002\n",
      "iteration: 455300 loss: 0.0011 lr: 0.002\n",
      "iteration: 455400 loss: 0.0010 lr: 0.002\n",
      "iteration: 455500 loss: 0.0009 lr: 0.002\n",
      "iteration: 455600 loss: 0.0009 lr: 0.002\n",
      "iteration: 455700 loss: 0.0010 lr: 0.002\n",
      "iteration: 455800 loss: 0.0010 lr: 0.002\n",
      "iteration: 455900 loss: 0.0010 lr: 0.002\n",
      "iteration: 456000 loss: 0.0008 lr: 0.002\n",
      "iteration: 456100 loss: 0.0009 lr: 0.002\n",
      "iteration: 456200 loss: 0.0010 lr: 0.002\n",
      "iteration: 456300 loss: 0.0009 lr: 0.002\n",
      "iteration: 456400 loss: 0.0010 lr: 0.002\n",
      "iteration: 456500 loss: 0.0009 lr: 0.002\n",
      "iteration: 456600 loss: 0.0009 lr: 0.002\n",
      "iteration: 456700 loss: 0.0008 lr: 0.002\n",
      "iteration: 456800 loss: 0.0009 lr: 0.002\n",
      "iteration: 456900 loss: 0.0009 lr: 0.002\n",
      "iteration: 457000 loss: 0.0010 lr: 0.002\n",
      "iteration: 457100 loss: 0.0010 lr: 0.002\n",
      "iteration: 457200 loss: 0.0009 lr: 0.002\n",
      "iteration: 457300 loss: 0.0010 lr: 0.002\n",
      "iteration: 457400 loss: 0.0009 lr: 0.002\n",
      "iteration: 457500 loss: 0.0009 lr: 0.002\n",
      "iteration: 457600 loss: 0.0009 lr: 0.002\n",
      "iteration: 457700 loss: 0.0009 lr: 0.002\n",
      "iteration: 457800 loss: 0.0009 lr: 0.002\n",
      "iteration: 457900 loss: 0.0010 lr: 0.002\n",
      "iteration: 458000 loss: 0.0010 lr: 0.002\n",
      "iteration: 458100 loss: 0.0010 lr: 0.002\n",
      "iteration: 458200 loss: 0.0009 lr: 0.002\n",
      "iteration: 458300 loss: 0.0009 lr: 0.002\n",
      "iteration: 458400 loss: 0.0010 lr: 0.002\n",
      "iteration: 458500 loss: 0.0011 lr: 0.002\n",
      "iteration: 458600 loss: 0.0009 lr: 0.002\n",
      "iteration: 458700 loss: 0.0008 lr: 0.002\n",
      "iteration: 458800 loss: 0.0009 lr: 0.002\n",
      "iteration: 458900 loss: 0.0010 lr: 0.002\n",
      "iteration: 459000 loss: 0.0008 lr: 0.002\n",
      "iteration: 459100 loss: 0.0009 lr: 0.002\n",
      "iteration: 459200 loss: 0.0010 lr: 0.002\n",
      "iteration: 459300 loss: 0.0009 lr: 0.002\n",
      "iteration: 459400 loss: 0.0009 lr: 0.002\n",
      "iteration: 459500 loss: 0.0010 lr: 0.002\n",
      "iteration: 459600 loss: 0.0011 lr: 0.002\n",
      "iteration: 459700 loss: 0.0009 lr: 0.002\n",
      "iteration: 459800 loss: 0.0009 lr: 0.002\n",
      "iteration: 459900 loss: 0.0010 lr: 0.002\n",
      "iteration: 460000 loss: 0.0008 lr: 0.002\n",
      "iteration: 460100 loss: 0.0010 lr: 0.002\n",
      "iteration: 460200 loss: 0.0009 lr: 0.002\n",
      "iteration: 460300 loss: 0.0009 lr: 0.002\n",
      "iteration: 460400 loss: 0.0009 lr: 0.002\n",
      "iteration: 460500 loss: 0.0009 lr: 0.002\n",
      "iteration: 460600 loss: 0.0008 lr: 0.002\n",
      "iteration: 460700 loss: 0.0010 lr: 0.002\n",
      "iteration: 460800 loss: 0.0009 lr: 0.002\n",
      "iteration: 460900 loss: 0.0010 lr: 0.002\n",
      "iteration: 461000 loss: 0.0009 lr: 0.002\n",
      "iteration: 461100 loss: 0.0009 lr: 0.002\n",
      "iteration: 461200 loss: 0.0009 lr: 0.002\n",
      "iteration: 461300 loss: 0.0009 lr: 0.002\n",
      "iteration: 461400 loss: 0.0008 lr: 0.002\n",
      "iteration: 461500 loss: 0.0010 lr: 0.002\n",
      "iteration: 461600 loss: 0.0010 lr: 0.002\n",
      "iteration: 461700 loss: 0.0010 lr: 0.002\n",
      "iteration: 461800 loss: 0.0011 lr: 0.002\n",
      "iteration: 461900 loss: 0.0010 lr: 0.002\n",
      "iteration: 462000 loss: 0.0009 lr: 0.002\n",
      "iteration: 462100 loss: 0.0009 lr: 0.002\n",
      "iteration: 462200 loss: 0.0009 lr: 0.002\n",
      "iteration: 462300 loss: 0.0009 lr: 0.002\n",
      "iteration: 462400 loss: 0.0010 lr: 0.002\n",
      "iteration: 462500 loss: 0.0009 lr: 0.002\n",
      "iteration: 462600 loss: 0.0010 lr: 0.002\n",
      "iteration: 462700 loss: 0.0009 lr: 0.002\n",
      "iteration: 462800 loss: 0.0009 lr: 0.002\n",
      "iteration: 462900 loss: 0.0009 lr: 0.002\n",
      "iteration: 463000 loss: 0.0009 lr: 0.002\n",
      "iteration: 463100 loss: 0.0009 lr: 0.002\n",
      "iteration: 463200 loss: 0.0010 lr: 0.002\n",
      "iteration: 463300 loss: 0.0009 lr: 0.002\n",
      "iteration: 463400 loss: 0.0009 lr: 0.002\n",
      "iteration: 463500 loss: 0.0009 lr: 0.002\n",
      "iteration: 463600 loss: 0.0009 lr: 0.002\n",
      "iteration: 463700 loss: 0.0009 lr: 0.002\n",
      "iteration: 463800 loss: 0.0009 lr: 0.002\n",
      "iteration: 463900 loss: 0.0010 lr: 0.002\n",
      "iteration: 464000 loss: 0.0008 lr: 0.002\n",
      "iteration: 464100 loss: 0.0009 lr: 0.002\n",
      "iteration: 464200 loss: 0.0010 lr: 0.002\n",
      "iteration: 464300 loss: 0.0010 lr: 0.002\n",
      "iteration: 464400 loss: 0.0010 lr: 0.002\n",
      "iteration: 464500 loss: 0.0009 lr: 0.002\n",
      "iteration: 464600 loss: 0.0009 lr: 0.002\n",
      "iteration: 464700 loss: 0.0009 lr: 0.002\n",
      "iteration: 464800 loss: 0.0009 lr: 0.002\n",
      "iteration: 464900 loss: 0.0008 lr: 0.002\n",
      "iteration: 465000 loss: 0.0009 lr: 0.002\n",
      "iteration: 465100 loss: 0.0009 lr: 0.002\n",
      "iteration: 465200 loss: 0.0009 lr: 0.002\n",
      "iteration: 465300 loss: 0.0009 lr: 0.002\n",
      "iteration: 465400 loss: 0.0010 lr: 0.002\n",
      "iteration: 465500 loss: 0.0009 lr: 0.002\n",
      "iteration: 465600 loss: 0.0010 lr: 0.002\n",
      "iteration: 465700 loss: 0.0010 lr: 0.002\n",
      "iteration: 465800 loss: 0.0010 lr: 0.002\n",
      "iteration: 465900 loss: 0.0008 lr: 0.002\n",
      "iteration: 466000 loss: 0.0009 lr: 0.002\n",
      "iteration: 466100 loss: 0.0008 lr: 0.002\n",
      "iteration: 466200 loss: 0.0010 lr: 0.002\n",
      "iteration: 466300 loss: 0.0009 lr: 0.002\n",
      "iteration: 466400 loss: 0.0010 lr: 0.002\n",
      "iteration: 466500 loss: 0.0009 lr: 0.002\n",
      "iteration: 466600 loss: 0.0009 lr: 0.002\n",
      "iteration: 466700 loss: 0.0009 lr: 0.002\n",
      "iteration: 466800 loss: 0.0010 lr: 0.002\n",
      "iteration: 466900 loss: 0.0009 lr: 0.002\n",
      "iteration: 467000 loss: 0.0009 lr: 0.002\n",
      "iteration: 467100 loss: 0.0009 lr: 0.002\n",
      "iteration: 467200 loss: 0.0009 lr: 0.002\n",
      "iteration: 467300 loss: 0.0010 lr: 0.002\n",
      "iteration: 467400 loss: 0.0010 lr: 0.002\n",
      "iteration: 467500 loss: 0.0009 lr: 0.002\n",
      "iteration: 467600 loss: 0.0010 lr: 0.002\n",
      "iteration: 467700 loss: 0.0010 lr: 0.002\n",
      "iteration: 467800 loss: 0.0010 lr: 0.002\n",
      "iteration: 467900 loss: 0.0009 lr: 0.002\n",
      "iteration: 468000 loss: 0.0008 lr: 0.002\n",
      "iteration: 468100 loss: 0.0010 lr: 0.002\n",
      "iteration: 468200 loss: 0.0009 lr: 0.002\n",
      "iteration: 468300 loss: 0.0010 lr: 0.002\n",
      "iteration: 468400 loss: 0.0010 lr: 0.002\n",
      "iteration: 468500 loss: 0.0009 lr: 0.002\n",
      "iteration: 468600 loss: 0.0010 lr: 0.002\n",
      "iteration: 468700 loss: 0.0011 lr: 0.002\n",
      "iteration: 468800 loss: 0.0010 lr: 0.002\n",
      "iteration: 468900 loss: 0.0010 lr: 0.002\n",
      "iteration: 469000 loss: 0.0009 lr: 0.002\n",
      "iteration: 469100 loss: 0.0009 lr: 0.002\n",
      "iteration: 469200 loss: 0.0009 lr: 0.002\n",
      "iteration: 469300 loss: 0.0010 lr: 0.002\n",
      "iteration: 469400 loss: 0.0008 lr: 0.002\n",
      "iteration: 469500 loss: 0.0010 lr: 0.002\n",
      "iteration: 469600 loss: 0.0010 lr: 0.002\n",
      "iteration: 469700 loss: 0.0008 lr: 0.002\n",
      "iteration: 469800 loss: 0.0009 lr: 0.002\n",
      "iteration: 469900 loss: 0.0010 lr: 0.002\n",
      "iteration: 470000 loss: 0.0008 lr: 0.002\n",
      "iteration: 470100 loss: 0.0009 lr: 0.002\n",
      "iteration: 470200 loss: 0.0009 lr: 0.002\n",
      "iteration: 470300 loss: 0.0010 lr: 0.002\n",
      "iteration: 470400 loss: 0.0008 lr: 0.002\n",
      "iteration: 470500 loss: 0.0010 lr: 0.002\n",
      "iteration: 470600 loss: 0.0009 lr: 0.002\n",
      "iteration: 470700 loss: 0.0010 lr: 0.002\n",
      "iteration: 470800 loss: 0.0009 lr: 0.002\n",
      "iteration: 470900 loss: 0.0010 lr: 0.002\n",
      "iteration: 471000 loss: 0.0009 lr: 0.002\n",
      "iteration: 471100 loss: 0.0010 lr: 0.002\n",
      "iteration: 471200 loss: 0.0009 lr: 0.002\n",
      "iteration: 471300 loss: 0.0010 lr: 0.002\n",
      "iteration: 471400 loss: 0.0009 lr: 0.002\n",
      "iteration: 471500 loss: 0.0010 lr: 0.002\n",
      "iteration: 471600 loss: 0.0009 lr: 0.002\n",
      "iteration: 471700 loss: 0.0008 lr: 0.002\n",
      "iteration: 471800 loss: 0.0010 lr: 0.002\n",
      "iteration: 471900 loss: 0.0009 lr: 0.002\n",
      "iteration: 472000 loss: 0.0010 lr: 0.002\n",
      "iteration: 472100 loss: 0.0009 lr: 0.002\n",
      "iteration: 472200 loss: 0.0009 lr: 0.002\n",
      "iteration: 472300 loss: 0.0010 lr: 0.002\n",
      "iteration: 472400 loss: 0.0009 lr: 0.002\n",
      "iteration: 472500 loss: 0.0008 lr: 0.002\n",
      "iteration: 472600 loss: 0.0009 lr: 0.002\n",
      "iteration: 472700 loss: 0.0009 lr: 0.002\n",
      "iteration: 472800 loss: 0.0009 lr: 0.002\n",
      "iteration: 472900 loss: 0.0009 lr: 0.002\n",
      "iteration: 473000 loss: 0.0009 lr: 0.002\n",
      "iteration: 473100 loss: 0.0008 lr: 0.002\n",
      "iteration: 473200 loss: 0.0009 lr: 0.002\n",
      "iteration: 473300 loss: 0.0009 lr: 0.002\n",
      "iteration: 473400 loss: 0.0009 lr: 0.002\n",
      "iteration: 473500 loss: 0.0009 lr: 0.002\n",
      "iteration: 473600 loss: 0.0009 lr: 0.002\n",
      "iteration: 473700 loss: 0.0009 lr: 0.002\n",
      "iteration: 473800 loss: 0.0010 lr: 0.002\n",
      "iteration: 473900 loss: 0.0009 lr: 0.002\n",
      "iteration: 474000 loss: 0.0009 lr: 0.002\n",
      "iteration: 474100 loss: 0.0008 lr: 0.002\n",
      "iteration: 474200 loss: 0.0009 lr: 0.002\n",
      "iteration: 474300 loss: 0.0009 lr: 0.002\n",
      "iteration: 474400 loss: 0.0009 lr: 0.002\n",
      "iteration: 474500 loss: 0.0009 lr: 0.002\n",
      "iteration: 474600 loss: 0.0009 lr: 0.002\n",
      "iteration: 474700 loss: 0.0010 lr: 0.002\n",
      "iteration: 474800 loss: 0.0009 lr: 0.002\n",
      "iteration: 474900 loss: 0.0009 lr: 0.002\n",
      "iteration: 475000 loss: 0.0009 lr: 0.002\n",
      "iteration: 475100 loss: 0.0010 lr: 0.002\n",
      "iteration: 475200 loss: 0.0010 lr: 0.002\n",
      "iteration: 475300 loss: 0.0008 lr: 0.002\n",
      "iteration: 475400 loss: 0.0008 lr: 0.002\n",
      "iteration: 475500 loss: 0.0008 lr: 0.002\n",
      "iteration: 475600 loss: 0.0010 lr: 0.002\n",
      "iteration: 475700 loss: 0.0008 lr: 0.002\n",
      "iteration: 475800 loss: 0.0009 lr: 0.002\n",
      "iteration: 475900 loss: 0.0008 lr: 0.002\n",
      "iteration: 476000 loss: 0.0010 lr: 0.002\n",
      "iteration: 476100 loss: 0.0009 lr: 0.002\n",
      "iteration: 476200 loss: 0.0009 lr: 0.002\n",
      "iteration: 476300 loss: 0.0010 lr: 0.002\n",
      "iteration: 476400 loss: 0.0010 lr: 0.002\n",
      "iteration: 476500 loss: 0.0009 lr: 0.002\n",
      "iteration: 476600 loss: 0.0009 lr: 0.002\n",
      "iteration: 476700 loss: 0.0009 lr: 0.002\n",
      "iteration: 476800 loss: 0.0008 lr: 0.002\n",
      "iteration: 476900 loss: 0.0010 lr: 0.002\n",
      "iteration: 477000 loss: 0.0009 lr: 0.002\n",
      "iteration: 477100 loss: 0.0009 lr: 0.002\n",
      "iteration: 477200 loss: 0.0008 lr: 0.002\n",
      "iteration: 477300 loss: 0.0009 lr: 0.002\n",
      "iteration: 477400 loss: 0.0009 lr: 0.002\n",
      "iteration: 477500 loss: 0.0008 lr: 0.002\n",
      "iteration: 477600 loss: 0.0011 lr: 0.002\n",
      "iteration: 477700 loss: 0.0010 lr: 0.002\n",
      "iteration: 477800 loss: 0.0009 lr: 0.002\n",
      "iteration: 477900 loss: 0.0010 lr: 0.002\n",
      "iteration: 478000 loss: 0.0009 lr: 0.002\n",
      "iteration: 478100 loss: 0.0008 lr: 0.002\n",
      "iteration: 478200 loss: 0.0010 lr: 0.002\n",
      "iteration: 478300 loss: 0.0011 lr: 0.002\n",
      "iteration: 478400 loss: 0.0010 lr: 0.002\n",
      "iteration: 478500 loss: 0.0009 lr: 0.002\n",
      "iteration: 478600 loss: 0.0009 lr: 0.002\n",
      "iteration: 478700 loss: 0.0009 lr: 0.002\n",
      "iteration: 478800 loss: 0.0008 lr: 0.002\n",
      "iteration: 478900 loss: 0.0009 lr: 0.002\n",
      "iteration: 479000 loss: 0.0009 lr: 0.002\n",
      "iteration: 479100 loss: 0.0009 lr: 0.002\n",
      "iteration: 479200 loss: 0.0009 lr: 0.002\n",
      "iteration: 479300 loss: 0.0009 lr: 0.002\n",
      "iteration: 479400 loss: 0.0010 lr: 0.002\n",
      "iteration: 479500 loss: 0.0008 lr: 0.002\n",
      "iteration: 479600 loss: 0.0009 lr: 0.002\n",
      "iteration: 479700 loss: 0.0010 lr: 0.002\n",
      "iteration: 479800 loss: 0.0009 lr: 0.002\n",
      "iteration: 479900 loss: 0.0008 lr: 0.002\n",
      "iteration: 480000 loss: 0.0011 lr: 0.002\n",
      "iteration: 480100 loss: 0.0009 lr: 0.002\n",
      "iteration: 480200 loss: 0.0010 lr: 0.002\n",
      "iteration: 480300 loss: 0.0009 lr: 0.002\n",
      "iteration: 480400 loss: 0.0009 lr: 0.002\n",
      "iteration: 480500 loss: 0.0008 lr: 0.002\n",
      "iteration: 480600 loss: 0.0011 lr: 0.002\n",
      "iteration: 480700 loss: 0.0010 lr: 0.002\n",
      "iteration: 480800 loss: 0.0009 lr: 0.002\n",
      "iteration: 480900 loss: 0.0009 lr: 0.002\n",
      "iteration: 481000 loss: 0.0008 lr: 0.002\n",
      "iteration: 481100 loss: 0.0010 lr: 0.002\n",
      "iteration: 481200 loss: 0.0010 lr: 0.002\n",
      "iteration: 481300 loss: 0.0009 lr: 0.002\n",
      "iteration: 481400 loss: 0.0009 lr: 0.002\n",
      "iteration: 481500 loss: 0.0009 lr: 0.002\n",
      "iteration: 481600 loss: 0.0010 lr: 0.002\n",
      "iteration: 481700 loss: 0.0008 lr: 0.002\n",
      "iteration: 481800 loss: 0.0008 lr: 0.002\n",
      "iteration: 481900 loss: 0.0010 lr: 0.002\n",
      "iteration: 482000 loss: 0.0009 lr: 0.002\n",
      "iteration: 482100 loss: 0.0009 lr: 0.002\n",
      "iteration: 482200 loss: 0.0010 lr: 0.002\n",
      "iteration: 482300 loss: 0.0010 lr: 0.002\n",
      "iteration: 482400 loss: 0.0010 lr: 0.002\n",
      "iteration: 482500 loss: 0.0009 lr: 0.002\n",
      "iteration: 482600 loss: 0.0009 lr: 0.002\n",
      "iteration: 482700 loss: 0.0009 lr: 0.002\n",
      "iteration: 482800 loss: 0.0010 lr: 0.002\n",
      "iteration: 482900 loss: 0.0009 lr: 0.002\n",
      "iteration: 483000 loss: 0.0009 lr: 0.002\n",
      "iteration: 483100 loss: 0.0010 lr: 0.002\n",
      "iteration: 483200 loss: 0.0008 lr: 0.002\n",
      "iteration: 483300 loss: 0.0008 lr: 0.002\n",
      "iteration: 483400 loss: 0.0009 lr: 0.002\n",
      "iteration: 483500 loss: 0.0010 lr: 0.002\n",
      "iteration: 483600 loss: 0.0010 lr: 0.002\n",
      "iteration: 483700 loss: 0.0009 lr: 0.002\n",
      "iteration: 483800 loss: 0.0009 lr: 0.002\n",
      "iteration: 483900 loss: 0.0009 lr: 0.002\n",
      "iteration: 484000 loss: 0.0010 lr: 0.002\n",
      "iteration: 484100 loss: 0.0009 lr: 0.002\n",
      "iteration: 484200 loss: 0.0008 lr: 0.002\n",
      "iteration: 484300 loss: 0.0009 lr: 0.002\n",
      "iteration: 484400 loss: 0.0010 lr: 0.002\n",
      "iteration: 484500 loss: 0.0010 lr: 0.002\n",
      "iteration: 484600 loss: 0.0009 lr: 0.002\n",
      "iteration: 484700 loss: 0.0009 lr: 0.002\n",
      "iteration: 484800 loss: 0.0009 lr: 0.002\n",
      "iteration: 484900 loss: 0.0009 lr: 0.002\n",
      "iteration: 485000 loss: 0.0009 lr: 0.002\n",
      "iteration: 485100 loss: 0.0010 lr: 0.002\n",
      "iteration: 485200 loss: 0.0009 lr: 0.002\n",
      "iteration: 485300 loss: 0.0009 lr: 0.002\n",
      "iteration: 485400 loss: 0.0009 lr: 0.002\n",
      "iteration: 485500 loss: 0.0009 lr: 0.002\n",
      "iteration: 485600 loss: 0.0009 lr: 0.002\n",
      "iteration: 485700 loss: 0.0008 lr: 0.002\n",
      "iteration: 485800 loss: 0.0008 lr: 0.002\n",
      "iteration: 485900 loss: 0.0008 lr: 0.002\n",
      "iteration: 486000 loss: 0.0009 lr: 0.002\n",
      "iteration: 486100 loss: 0.0010 lr: 0.002\n",
      "iteration: 486200 loss: 0.0009 lr: 0.002\n",
      "iteration: 486300 loss: 0.0010 lr: 0.002\n",
      "iteration: 486400 loss: 0.0010 lr: 0.002\n",
      "iteration: 486500 loss: 0.0008 lr: 0.002\n",
      "iteration: 486600 loss: 0.0010 lr: 0.002\n",
      "iteration: 486700 loss: 0.0010 lr: 0.002\n",
      "iteration: 486800 loss: 0.0009 lr: 0.002\n",
      "iteration: 486900 loss: 0.0009 lr: 0.002\n",
      "iteration: 487000 loss: 0.0009 lr: 0.002\n",
      "iteration: 487100 loss: 0.0009 lr: 0.002\n",
      "iteration: 487200 loss: 0.0009 lr: 0.002\n",
      "iteration: 487300 loss: 0.0010 lr: 0.002\n",
      "iteration: 487400 loss: 0.0008 lr: 0.002\n",
      "iteration: 487500 loss: 0.0010 lr: 0.002\n",
      "iteration: 487600 loss: 0.0009 lr: 0.002\n",
      "iteration: 487700 loss: 0.0009 lr: 0.002\n",
      "iteration: 487800 loss: 0.0009 lr: 0.002\n",
      "iteration: 487900 loss: 0.0009 lr: 0.002\n",
      "iteration: 488000 loss: 0.0009 lr: 0.002\n",
      "iteration: 488100 loss: 0.0009 lr: 0.002\n",
      "iteration: 488200 loss: 0.0009 lr: 0.002\n",
      "iteration: 488300 loss: 0.0009 lr: 0.002\n",
      "iteration: 488400 loss: 0.0009 lr: 0.002\n",
      "iteration: 488500 loss: 0.0009 lr: 0.002\n",
      "iteration: 488600 loss: 0.0010 lr: 0.002\n",
      "iteration: 488700 loss: 0.0008 lr: 0.002\n",
      "iteration: 488800 loss: 0.0010 lr: 0.002\n",
      "iteration: 488900 loss: 0.0009 lr: 0.002\n",
      "iteration: 489000 loss: 0.0009 lr: 0.002\n",
      "iteration: 489100 loss: 0.0010 lr: 0.002\n",
      "iteration: 489200 loss: 0.0009 lr: 0.002\n",
      "iteration: 489300 loss: 0.0008 lr: 0.002\n",
      "iteration: 489400 loss: 0.0008 lr: 0.002\n",
      "iteration: 489500 loss: 0.0008 lr: 0.002\n",
      "iteration: 489600 loss: 0.0009 lr: 0.002\n",
      "iteration: 489700 loss: 0.0010 lr: 0.002\n",
      "iteration: 489800 loss: 0.0008 lr: 0.002\n",
      "iteration: 489900 loss: 0.0010 lr: 0.002\n",
      "iteration: 490000 loss: 0.0010 lr: 0.002\n",
      "iteration: 490100 loss: 0.0010 lr: 0.002\n",
      "iteration: 490200 loss: 0.0010 lr: 0.002\n",
      "iteration: 490300 loss: 0.0009 lr: 0.002\n",
      "iteration: 490400 loss: 0.0010 lr: 0.002\n",
      "iteration: 490500 loss: 0.0009 lr: 0.002\n",
      "iteration: 490600 loss: 0.0010 lr: 0.002\n",
      "iteration: 490700 loss: 0.0009 lr: 0.002\n",
      "iteration: 490800 loss: 0.0009 lr: 0.002\n",
      "iteration: 490900 loss: 0.0009 lr: 0.002\n",
      "iteration: 491000 loss: 0.0010 lr: 0.002\n",
      "iteration: 491100 loss: 0.0010 lr: 0.002\n",
      "iteration: 491200 loss: 0.0009 lr: 0.002\n",
      "iteration: 491300 loss: 0.0009 lr: 0.002\n",
      "iteration: 491400 loss: 0.0008 lr: 0.002\n",
      "iteration: 491500 loss: 0.0009 lr: 0.002\n",
      "iteration: 491600 loss: 0.0009 lr: 0.002\n",
      "iteration: 491700 loss: 0.0009 lr: 0.002\n",
      "iteration: 491800 loss: 0.0010 lr: 0.002\n",
      "iteration: 491900 loss: 0.0009 lr: 0.002\n",
      "iteration: 492000 loss: 0.0009 lr: 0.002\n",
      "iteration: 492100 loss: 0.0009 lr: 0.002\n",
      "iteration: 492200 loss: 0.0008 lr: 0.002\n",
      "iteration: 492300 loss: 0.0008 lr: 0.002\n",
      "iteration: 492400 loss: 0.0008 lr: 0.002\n",
      "iteration: 492500 loss: 0.0009 lr: 0.002\n",
      "iteration: 492600 loss: 0.0010 lr: 0.002\n",
      "iteration: 492700 loss: 0.0010 lr: 0.002\n",
      "iteration: 492800 loss: 0.0009 lr: 0.002\n",
      "iteration: 492900 loss: 0.0009 lr: 0.002\n",
      "iteration: 493000 loss: 0.0009 lr: 0.002\n",
      "iteration: 493100 loss: 0.0010 lr: 0.002\n",
      "iteration: 493200 loss: 0.0009 lr: 0.002\n",
      "iteration: 493300 loss: 0.0009 lr: 0.002\n",
      "iteration: 493400 loss: 0.0009 lr: 0.002\n",
      "iteration: 493500 loss: 0.0010 lr: 0.002\n",
      "iteration: 493600 loss: 0.0010 lr: 0.002\n",
      "iteration: 493700 loss: 0.0008 lr: 0.002\n",
      "iteration: 493800 loss: 0.0009 lr: 0.002\n",
      "iteration: 493900 loss: 0.0010 lr: 0.002\n",
      "iteration: 494000 loss: 0.0009 lr: 0.002\n",
      "iteration: 494100 loss: 0.0010 lr: 0.002\n",
      "iteration: 494200 loss: 0.0009 lr: 0.002\n",
      "iteration: 494300 loss: 0.0011 lr: 0.002\n",
      "iteration: 494400 loss: 0.0009 lr: 0.002\n",
      "iteration: 494500 loss: 0.0009 lr: 0.002\n",
      "iteration: 494600 loss: 0.0009 lr: 0.002\n",
      "iteration: 494700 loss: 0.0008 lr: 0.002\n",
      "iteration: 494800 loss: 0.0009 lr: 0.002\n",
      "iteration: 494900 loss: 0.0009 lr: 0.002\n",
      "iteration: 495000 loss: 0.0009 lr: 0.002\n",
      "iteration: 495100 loss: 0.0009 lr: 0.002\n",
      "iteration: 495200 loss: 0.0009 lr: 0.002\n",
      "iteration: 495300 loss: 0.0009 lr: 0.002\n",
      "iteration: 495400 loss: 0.0009 lr: 0.002\n",
      "iteration: 495500 loss: 0.0009 lr: 0.002\n",
      "iteration: 495600 loss: 0.0010 lr: 0.002\n",
      "iteration: 495700 loss: 0.0008 lr: 0.002\n",
      "iteration: 495800 loss: 0.0009 lr: 0.002\n",
      "iteration: 495900 loss: 0.0009 lr: 0.002\n",
      "iteration: 496000 loss: 0.0009 lr: 0.002\n",
      "iteration: 496100 loss: 0.0009 lr: 0.002\n",
      "iteration: 496200 loss: 0.0009 lr: 0.002\n",
      "iteration: 496300 loss: 0.0010 lr: 0.002\n",
      "iteration: 496400 loss: 0.0009 lr: 0.002\n",
      "iteration: 496500 loss: 0.0009 lr: 0.002\n",
      "iteration: 496600 loss: 0.0008 lr: 0.002\n",
      "iteration: 496700 loss: 0.0008 lr: 0.002\n",
      "iteration: 496800 loss: 0.0009 lr: 0.002\n",
      "iteration: 496900 loss: 0.0010 lr: 0.002\n",
      "iteration: 497000 loss: 0.0009 lr: 0.002\n",
      "iteration: 497100 loss: 0.0009 lr: 0.002\n",
      "iteration: 497200 loss: 0.0009 lr: 0.002\n",
      "iteration: 497300 loss: 0.0009 lr: 0.002\n",
      "iteration: 497400 loss: 0.0009 lr: 0.002\n",
      "iteration: 497500 loss: 0.0009 lr: 0.002\n",
      "iteration: 497600 loss: 0.0010 lr: 0.002\n",
      "iteration: 497700 loss: 0.0009 lr: 0.002\n",
      "iteration: 497800 loss: 0.0009 lr: 0.002\n",
      "iteration: 497900 loss: 0.0010 lr: 0.002\n",
      "iteration: 498000 loss: 0.0009 lr: 0.002\n",
      "iteration: 498100 loss: 0.0009 lr: 0.002\n",
      "iteration: 498200 loss: 0.0009 lr: 0.002\n",
      "iteration: 498300 loss: 0.0009 lr: 0.002\n",
      "iteration: 498400 loss: 0.0008 lr: 0.002\n",
      "iteration: 498500 loss: 0.0009 lr: 0.002\n",
      "iteration: 498600 loss: 0.0008 lr: 0.002\n",
      "iteration: 498700 loss: 0.0009 lr: 0.002\n",
      "iteration: 498800 loss: 0.0010 lr: 0.002\n",
      "iteration: 498900 loss: 0.0010 lr: 0.002\n",
      "iteration: 499000 loss: 0.0008 lr: 0.002\n",
      "iteration: 499100 loss: 0.0009 lr: 0.002\n",
      "iteration: 499200 loss: 0.0009 lr: 0.002\n",
      "iteration: 499300 loss: 0.0008 lr: 0.002\n",
      "iteration: 499400 loss: 0.0009 lr: 0.002\n",
      "iteration: 499500 loss: 0.0009 lr: 0.002\n",
      "iteration: 499600 loss: 0.0009 lr: 0.002\n",
      "iteration: 499700 loss: 0.0010 lr: 0.002\n",
      "iteration: 499800 loss: 0.0010 lr: 0.002\n",
      "iteration: 499900 loss: 0.0010 lr: 0.002\n",
      "iteration: 500000 loss: 0.0010 lr: 0.002\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1378, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _run_fn\n",
      "    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1454, in _call_tf_sessionrun\n",
      "    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 85, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1191, in _run\n",
      "    results = self._do_run(handle, final_targets, final_fetches,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1371, in _do_run\n",
      "    return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1397, in _do_call\n",
      "    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Graph execution error:\n",
      "\n",
      "Detected at node 'fifo_queue_enqueue' defined at (most recent call last):\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "      app.start()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "      self.io_loop.start()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "      await result\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "      result = runner(coro)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\insan\\AppData\\Local\\Temp\\ipykernel_14120\\194897986.py\", line 1, in <module>\n",
      "      deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=500000, allow_growth=True)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n",
      "      train(\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 171, in train\n",
      "      batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "    File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 71, in setup_preloading\n",
      "      enqueue_op = q.enqueue(placeholders_list)\n",
      "Node: 'fifo_queue_enqueue'\n",
      "Enqueue operation was cancelled\n",
      "\t [[{{node fifo_queue_enqueue}}]]\n",
      "\n",
      "Original stack trace for 'fifo_queue_enqueue':\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\asyncio\\events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\insan\\AppData\\Local\\Temp\\ipykernel_14120\\194897986.py\", line 1, in <module>\n",
      "    deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=500000, allow_growth=True)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\", line 212, in train_network\n",
      "    train(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 171, in train\n",
      "    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 71, in setup_preloading\n",
      "    enqueue_op = q.enqueue(placeholders_list)\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py\", line 346, in enqueue\n",
      "    return gen_data_flow_ops.queue_enqueue_v2(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 4062, in queue_enqueue_v2\n",
      "    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n",
      "    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "  File \"C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n",
      "    ret = Operation(\n",
      "\n",
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n",
      "Running  DLC_resnet50_Training_projectMar3shuffle1_500000  with # of training iterations: 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:02, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-500000\n",
      "Results for 500000  training iterations: 95 1 train error: 1.65 pixels. Test error: 4.24  pixels.\n",
      "With pcutoff of 0.6  train error: 1.65 pixels. Test error: 4.24 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:12<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, autotune=False, displayiters=100, saveiters=10000, maxiters=500000, allow_growth=True)\n",
    "deeplabcut.evaluate_network(config_path,plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dcceff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['Ear', 'Mouth', 'Whisker', 'Hand', 'Nose'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_Training_projectMar3\\\\Training_project_TH95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\insan\\\\anaconda3\\\\envs\\\\DEEPLABCUT\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\insan\\\\Desktop\\\\DLC_Projects\\\\Training_nonmulti\\\\Training_project-TH-2023-03-03\\\\dlc-models\\\\iteration-0\\\\Training_projectMar3-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "C:\\Users\\insan\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-500000 for model C:\\Users\\insan\\Desktop\\DLC_Projects\\Training_nonmulti\\Training_project-TH-2023-03-03\\dlc-models\\iteration-0\\Training_projectMar3-trainset95shuffle1\n",
      "Starting to analyze %  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_1\\CogRig_AE_231_2023-02-14_1.mp4\n",
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_1\\CogRig_AE_231_2023-02-14_1.mp4\n",
      "Duration of video [s]:  525.29 , recorded with  100.0 fps!\n",
      "Overall # of frames:  52529  found with (before cropping) frame dimensions:  640 480\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52529/52529 [18:42<00:00, 46.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_1...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "video_file3 = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 9\\\\AE_231_1\\\\CogRig_AE_231_2023-02-14_1.mp4'\n",
    "video3 = deeplabcut.analyze_videos(config_path, [video_file3],videotype='.mp4',auto_track=True,save_as_csv=True)\n",
    "deeplabcut.create_labeled_video(config_path, [video_file3], save_frames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e4de90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering with median model C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_1\\CogRig_AE_231_2023-02-14_1.mp4\n",
      "Saving filtered csv poses!\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.filterpredictions(config_path,[video_file3], videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67349a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  C:\\Users\\insan\\Desktop\\IR Camera Videos\\Day 9\\AE_231_1\\CogRig_AE_231_2023-02-14_1.mp4 and data.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array of sample points is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_file3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\plotting.py:316\u001b[0m, in \u001b[0;36mplot_trajectories\u001b[1;34m(config, videos, videotype, shuffle, trainingsetindex, filtered, displayedbodyparts, displayedindividuals, showfigures, destfolder, modelprefix, imagetype, resolution, linewidth, track_method)\u001b[0m\n\u001b[0;32m    314\u001b[0m         animals \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m animal \u001b[38;5;129;01min\u001b[39;00m animals\u001b[38;5;241m.\u001b[39mintersection(individuals) \u001b[38;5;129;01mor\u001b[39;00m animals:\n\u001b[1;32m--> 316\u001b[0m         \u001b[43mPlottingResults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtmpfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabeled_bpts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[43manimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshowfigures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43manimal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimagetype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    328\u001b[0m     failed\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\plotting.py:108\u001b[0m, in \u001b[0;36mPlottingResults\u001b[1;34m(tmpfolder, Dataframe, cfg, bodyparts2plot, individuals2plot, showfigures, suffix, resolution, linewidth)\u001b[0m\n\u001b[0;32m    100\u001b[0m temp_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    101\u001b[0m     Dataframe\u001b[38;5;241m.\u001b[39mxs((bp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m), level\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m    102\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m temp_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    105\u001b[0m     Dataframe\u001b[38;5;241m.\u001b[39mxs((bp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m), level\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[0;32m    106\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    107\u001b[0m )\n\u001b[1;32m--> 108\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbpindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphavalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m ax2\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m    111\u001b[0m     temp_x,\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     alpha\u001b[38;5;241m=\u001b[39malphavalue,\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m ax2\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m    118\u001b[0m     temp_y,\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m     alpha\u001b[38;5;241m=\u001b[39malphavalue,\n\u001b[0;32m    123\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1690\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m-> 1690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scalex:\n\u001b[0;32m   1692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\axes\\_base.py:2304\u001b[0m, in \u001b[0;36m_AxesBase.add_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2302\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_clip_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch)\n\u001b[1;32m-> 2304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_line_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_label():\n\u001b[0;32m   2306\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_label(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_child\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\axes\\_base.py:2327\u001b[0m, in \u001b[0;36m_AxesBase._update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_line_limits\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;124;03m    Figures out the data limit of the given line, updating self.dataLim.\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2327\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\lines.py:1029\u001b[0m, in \u001b[0;36mLine2D.get_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidx:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\lines.py:681\u001b[0m, in \u001b[0;36mLine2D.recache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    680\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m--> 681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_filled[nanmask] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnanmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnanmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnanmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minterp\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\numpy\\lib\\function_base.py:1594\u001b[0m, in \u001b[0;36minterp\u001b[1;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     xp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((xp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mperiod, xp, xp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mperiod))\n\u001b[0;32m   1592\u001b[0m     fp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], fp, fp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m-> 1594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: array of sample points is empty"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.plot_trajectories(config_path,[video_file3], videotype='.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796fe634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656dd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file2 = 'C:\\\\Users\\insan\\\\Desktop\\\\IR Camera Videos\\\\Day 7\\\\Cut\\\\AE_231_Cut.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cb42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0cd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
